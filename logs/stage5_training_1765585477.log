2025-12-12 19:24:37 [INFO] [__main__:310] ================================================================================
2025-12-12 19:24:37 [INFO] [__main__:311] STAGE 5: MODEL TRAINING
2025-12-12 19:24:37 [INFO] [__main__:312] ================================================================================
2025-12-12 19:24:37 [INFO] [__main__:313] Project root: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc
2025-12-12 19:24:37 [INFO] [__main__:314] Scaled metadata: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/scaled_videos/scaled_metadata.arrow
2025-12-12 19:24:37 [INFO] [__main__:315] Features Stage 2: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/features_stage2/features_metadata.arrow
2025-12-12 19:24:37 [INFO] [__main__:316] Features Stage 4: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/features_stage4/features_scaled_metadata.arrow
2025-12-12 19:24:37 [INFO] [__main__:317] Model types: ['x3d']
2025-12-12 19:24:37 [INFO] [__main__:318] K-fold splits: 5
2025-12-12 19:24:37 [INFO] [__main__:319] Number of frames: 500
2025-12-12 19:24:37 [INFO] [__main__:320] Output directory: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/stage5
2025-12-12 19:24:37 [INFO] [__main__:321] Experiment tracking: Enabled
2025-12-12 19:24:37 [INFO] [__main__:324] Delete existing: False
2025-12-12 19:24:37 [INFO] [__main__:325] Resume mode: True
2025-12-12 19:24:37 [INFO] [__main__:326] Log file: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/logs/stage5_training_1765585477.log
2025-12-12 19:24:37 [INFO] [__main__:333] ================================================================================
2025-12-12 19:24:37 [INFO] [__main__:334] Checking prerequisites...
2025-12-12 19:24:37 [INFO] [__main__:335] ================================================================================
2025-12-12 19:24:37 [INFO] [__main__:342] ✓ Scaled metadata file found: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/scaled_videos/scaled_metadata.arrow
2025-12-12 19:24:37 [INFO] [__main__:352] ✓ All model types are valid
2025-12-12 19:24:37 [INFO] [__main__:358] ================================================================================
2025-12-12 19:24:37 [INFO] [__main__:359] Initial memory statistics:
2025-12-12 19:24:37 [INFO] [__main__:360] ================================================================================
2025-12-12 19:24:37 [INFO] [lib.utils.memory:89] Memory stats (Stage 5: before training): {'cpu_memory_mb': 735.8984375, 'cpu_memory_gb': 0.7186508178710938, 'cpu_vms_mb': 10217.70703125, 'gpu_allocated_gb': 0.0, 'gpu_reserved_gb': 0.0, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 16.928342016}
2025-12-12 19:24:37 [INFO] [__main__:364] ================================================================================
2025-12-12 19:24:37 [INFO] [__main__:365] Starting Stage 5: Model Training
2025-12-12 19:24:37 [INFO] [__main__:366] ================================================================================
2025-12-12 19:24:37 [INFO] [__main__:367] Training 1 model(s) with 5-fold cross-validation
2025-12-12 19:24:37 [INFO] [__main__:368] This may take a while depending on dataset size and model complexity...
2025-12-12 19:24:37 [INFO] [__main__:369] Progress will be logged in real-time
2025-12-12 19:24:37 [INFO] [__main__:370] ================================================================================
2025-12-12 19:24:37 [INFO] [__main__:375] Calling Stage 5 training pipeline...
2025-12-12 19:24:37 [INFO] [__main__:376] This may take a while - progress will be logged in real-time
2025-12-12 19:24:37 [INFO] [lib.training.pipeline:490] ================================================================================
2025-12-12 19:24:37 [INFO] [lib.training.pipeline:491] Stage 5: Model Training Pipeline Started
2025-12-12 19:24:37 [INFO] [lib.training.pipeline:492] ================================================================================
2025-12-12 19:24:37 [INFO] [lib.training.pipeline:493] Model types: ['x3d']
2025-12-12 19:24:37 [INFO] [lib.training.pipeline:494] K-fold splits: 5
2025-12-12 19:24:37 [INFO] [lib.training.pipeline:495] Frames per video: 500
2025-12-12 19:24:37 [INFO] [lib.training.pipeline:496] Output directory: data/stage5
2025-12-12 19:24:37 [INFO] [lib.training.pipeline:497] Initializing pipeline...
2025-12-12 19:24:37 [INFO] [lib.training.pipeline:265] ================================================================================
2025-12-12 19:24:37 [INFO] [lib.training.pipeline:266] STAGE 5 PREREQUISITE VALIDATION
2025-12-12 19:24:37 [INFO] [lib.training.pipeline:267] ================================================================================
2025-12-12 19:24:37 [INFO] [lib.training.pipeline:270] 
[1/3] Checking Stage 3 (scaled videos) - REQUIRED for all models...
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:279] ✓ Stage 3 metadata found: 3278 scaled videos
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:280]   Path: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/scaled_videos/scaled_metadata.arrow
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:283] 
[2/3] Checking Stage 2 (features) - REQUIRED for *_stage2 models...
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:291] ✓ Stage 2 metadata found: 3278 feature rows
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:292]   Path: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/features_stage2/features_metadata.arrow
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:295] 
[3/3] Checking Stage 4 (scaled features) - REQUIRED for *_stage2_stage4 models...
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:303] ✓ Stage 4 metadata found: 3277 feature rows
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:304]   Path: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/features_stage4/features_scaled_metadata.arrow
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:307] 
================================================================================
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:308] MODEL REQUIREMENTS CHECK
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:309] ================================================================================
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:333] ✓ x3d: CAN RUN
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:336] 
================================================================================
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:337] VALIDATION SUMMARY
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:338] ================================================================================
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:339] Stage 3 (scaled videos): ✓ Available
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:340]   Count: 3278 videos
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:341] Stage 2 (features): ✓ Available
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:342]   Count: 3278 feature rows
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:343] Stage 4 (scaled features): ✓ Available
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:344]   Count: 3277 feature rows
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:345] 
Runnable models: 1/1
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:346]   ['x3d']
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:353] ================================================================================
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:583] 
Stage 5: Loading metadata...
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:609] Loaded metadata: 3278 rows
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:619] ✓ Data validation passed: 3278 rows (> 3000 required)
2025-12-12 19:24:39 [WARNING] [lib.utils.guardrails:171] Disk usage high: free=833783.25GB, used=1666056.75GB (66.6%)
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:651] Stage 5: Found 3278 scaled videos
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:675] Enabling adaptive chunked frame loading for x3d: initial_chunk_size=200, num_frames=500. Chunk size will adapt automatically based on OOM events (AIMD algorithm).
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:747] 
================================================================================
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:748] Stage 5: Training model: x3d
2025-12-12 19:24:39 [INFO] [lib.training.pipeline:749] ================================================================================
2025-12-12 19:24:41 [INFO] [lib.training.pipeline:773] Grid search: 16 hyperparameter combinations to try
2025-12-12 19:24:41 [INFO] [lib.training.pipeline:781] ================================================================================
2025-12-12 19:24:41 [INFO] [lib.training.pipeline:782] HYPERPARAMETER SEARCH: Using 20% stratified sample for efficiency
2025-12-12 19:24:41 [INFO] [lib.training.pipeline:783] ================================================================================
2025-12-12 19:24:42 [INFO] [lib.training.pipeline:792] Hyperparameter search sample: 655 rows (20.0% of 3278 total)
2025-12-12 19:24:44 [INFO] [lib.training.pipeline:805] Using 5-fold stratified cross-validation on 20% sample for hyperparameter search
2025-12-12 19:24:44 [INFO] [lib.training.pipeline:818] 
================================================================================
2025-12-12 19:24:44 [INFO] [lib.training.pipeline:819] Grid Search: Hyperparameter combination 1/16
2025-12-12 19:24:44 [INFO] [lib.training.pipeline:820] Parameters: {'learning_rate': 5e-05, 'backbone_lr': 1e-06, 'head_lr': 0.0001, 'weight_decay': 0.0001}
2025-12-12 19:24:44 [INFO] [lib.training.pipeline:821] ================================================================================
2025-12-12 19:24:44 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 1/5 (20% sample)
2025-12-12 19:24:44 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:24:44 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:24:45 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:24:45 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:24:46 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:24:51 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: d84970fa9c7c470dab340bc9fdcaae6b (experiment: x3d)
2025-12-12 19:24:51 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 1...
2025-12-12 19:25:04 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:25:04 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:25:10 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.30 GiB is free. Including non-PyTorch memory, this process has 2.46 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 32.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:25:11 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 1657.62109375, 'cpu_memory_gb': 1.6187705993652344, 'cpu_vms_mb': 48707.1796875, 'gpu_allocated_gb': 1.631732736, 'gpu_reserved_gb': 1.660944384, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.267397632}
2025-12-12 19:25:11 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.30 GiB is free. Including non-PyTorch memory, this process has 2.46 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 32.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:25:11 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:25:12 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:25:14 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.26 GiB is free. Including non-PyTorch memory, this process has 2.50 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 45.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:25:14 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 1697.65625, 'cpu_memory_gb': 1.657867431640625, 'cpu_vms_mb': 48747.0859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.702887424, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.225454592}
2025-12-12 19:25:14 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.26 GiB is free. Including non-PyTorch memory, this process has 2.50 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 45.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:25:14 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:25:20 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.26 GiB is free. Including non-PyTorch memory, this process has 2.50 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 45.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:25:20 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 2583.6015625, 'cpu_memory_gb': 2.5230484008789062, 'cpu_vms_mb': 49633.390625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.702887424, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.225454592}
2025-12-12 19:25:20 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.26 GiB is free. Including non-PyTorch memory, this process has 2.50 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 45.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:25:20 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:25:23 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.26 GiB is free. Including non-PyTorch memory, this process has 2.50 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 45.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:25:23 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 2583.6015625, 'cpu_memory_gb': 2.5230484008789062, 'cpu_vms_mb': 49633.390625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.702887424, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.225454592}
2025-12-12 19:25:23 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.26 GiB is free. Including non-PyTorch memory, this process has 2.50 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 45.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Final batch size: 1
2025-12-12 19:25:23 [ERROR] [lib.training.pipeline:1650] Error training fold 1: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.26 GiB is free. Including non-PyTorch memory, this process has 2.50 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 45.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.26 GiB is free. Including non-PyTorch memory, this process has 2.50 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 45.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:25:23 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: d84970fa9c7c470dab340bc9fdcaae6b
2025-12-12 19:25:23 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 2/5 (20% sample)
2025-12-12 19:25:23 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:25:23 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:25:24 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:25:24 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:25:25 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:25:25 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 73189edde5fb4de6997c8a6b77ae78d6 (experiment: x3d)
2025-12-12 19:25:25 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 2...
2025-12-12 19:25:26 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:25:26 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:25:27 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:25:31 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.28 GiB is free. Including non-PyTorch memory, this process has 2.48 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 27.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:25:32 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 2502.2734375, 'cpu_memory_gb': 2.4436264038085938, 'cpu_vms_mb': 49552.453125, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.684013056, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.24432896}
2025-12-12 19:25:32 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.28 GiB is free. Including non-PyTorch memory, this process has 2.48 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 27.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:25:32 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:25:34 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.30 GiB is free. Including non-PyTorch memory, this process has 2.46 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 34.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:25:35 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 2502.28515625, 'cpu_memory_gb': 2.4436378479003906, 'cpu_vms_mb': 49552.453125, 'gpu_allocated_gb': 1.631732736, 'gpu_reserved_gb': 1.684013056, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.24432896}
2025-12-12 19:25:35 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.30 GiB is free. Including non-PyTorch memory, this process has 2.46 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 34.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:25:35 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:25:37 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:25:38 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.28 GiB is free. Including non-PyTorch memory, this process has 2.48 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 27.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:25:38 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 2502.28515625, 'cpu_memory_gb': 2.4436378479003906, 'cpu_vms_mb': 49552.453125, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.684013056, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.24432896}
2025-12-12 19:25:38 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.28 GiB is free. Including non-PyTorch memory, this process has 2.48 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 27.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:25:38 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:25:43 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.28 GiB is free. Including non-PyTorch memory, this process has 2.48 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 27.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:25:43 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 2502.90625, 'cpu_memory_gb': 2.444244384765625, 'cpu_vms_mb': 49552.453125, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.684013056, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.24432896}
2025-12-12 19:25:43 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.28 GiB is free. Including non-PyTorch memory, this process has 2.48 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 27.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Final batch size: 1
2025-12-12 19:25:43 [ERROR] [lib.training.pipeline:1650] Error training fold 2: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.28 GiB is free. Including non-PyTorch memory, this process has 2.48 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 27.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.28 GiB is free. Including non-PyTorch memory, this process has 2.48 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 27.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:25:43 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 73189edde5fb4de6997c8a6b77ae78d6
2025-12-12 19:25:43 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 3/5 (20% sample)
2025-12-12 19:25:43 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:25:43 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:25:43 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:25:43 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:25:44 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:25:44 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: ec269c0907b741e6b11fb30e971ef33d (experiment: x3d)
2025-12-12 19:25:44 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 3...
2025-12-12 19:25:46 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:25:46 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:25:48 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:25:49 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.26 GiB is free. Including non-PyTorch memory, this process has 2.50 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 74.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:25:49 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 2447.9453125, 'cpu_memory_gb': 2.3905715942382812, 'cpu_vms_mb': 49497.359375, 'gpu_allocated_gb': 1.631732736, 'gpu_reserved_gb': 1.725956096, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.20238592}
2025-12-12 19:25:49 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.26 GiB is free. Including non-PyTorch memory, this process has 2.50 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 74.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:25:49 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:25:53 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.58 GiB is free. Including non-PyTorch memory, this process has 3.19 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 79.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:25:53 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 2959.9453125, 'cpu_memory_gb': 2.8905715942382812, 'cpu_vms_mb': 86425.359375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.250244096, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.67809792}
2025-12-12 19:25:53 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.58 GiB is free. Including non-PyTorch memory, this process has 3.19 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 79.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:25:53 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:25:56 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.24 GiB is free. Including non-PyTorch memory, this process has 2.52 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 67.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:25:56 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 2959.9453125, 'cpu_memory_gb': 2.8905715942382812, 'cpu_vms_mb': 86425.359375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.725956096, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.20238592}
2025-12-12 19:25:56 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.24 GiB is free. Including non-PyTorch memory, this process has 2.52 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 67.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:25:56 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:25:57 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:25:59 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.24 GiB is free. Including non-PyTorch memory, this process has 2.52 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 67.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:25:59 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 2959.9453125, 'cpu_memory_gb': 2.8905715942382812, 'cpu_vms_mb': 86425.359375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.725956096, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.20238592}
2025-12-12 19:25:59 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.24 GiB is free. Including non-PyTorch memory, this process has 2.52 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 67.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Final batch size: 1
2025-12-12 19:25:59 [ERROR] [lib.training.pipeline:1650] Error training fold 3: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.24 GiB is free. Including non-PyTorch memory, this process has 2.52 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 67.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.24 GiB is free. Including non-PyTorch memory, this process has 2.52 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 67.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:25:59 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: ec269c0907b741e6b11fb30e971ef33d
2025-12-12 19:25:59 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 4/5 (20% sample)
2025-12-12 19:25:59 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:25:59 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:26:00 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:26:00 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:26:00 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:26:00 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 4f529abc0793444f8adc58f4017443f3 (experiment: x3d)
2025-12-12 19:26:00 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 4...
2025-12-12 19:26:02 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:26:03 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:26:06 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.24 GiB is free. Including non-PyTorch memory, this process has 2.52 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 69.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:26:06 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 2959.9453125, 'cpu_memory_gb': 2.8905715942382812, 'cpu_vms_mb': 86425.359375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.728053248, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.200288768}
2025-12-12 19:26:06 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.24 GiB is free. Including non-PyTorch memory, this process has 2.52 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 69.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:26:06 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:26:07 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:26:09 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.24 GiB is free. Including non-PyTorch memory, this process has 2.52 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 69.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:26:09 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 2959.9453125, 'cpu_memory_gb': 2.8905715942382812, 'cpu_vms_mb': 86425.359375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.728053248, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.200288768}
2025-12-12 19:26:09 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.24 GiB is free. Including non-PyTorch memory, this process has 2.52 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 69.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:26:09 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:26:14 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.24 GiB is free. Including non-PyTorch memory, this process has 2.52 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 69.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:26:14 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3225.62890625, 'cpu_memory_gb': 3.1500282287597656, 'cpu_vms_mb': 86691.0625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.728053248, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.200288768}
2025-12-12 19:26:14 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.24 GiB is free. Including non-PyTorch memory, this process has 2.52 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 69.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:26:14 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:26:18 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.24 GiB is free. Including non-PyTorch memory, this process has 2.52 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 69.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:26:18 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 2960.265625, 'cpu_memory_gb': 2.8908843994140625, 'cpu_vms_mb': 86426.55078125, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.728053248, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.200288768}
2025-12-12 19:26:18 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.24 GiB is free. Including non-PyTorch memory, this process has 2.52 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 69.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Final batch size: 1
2025-12-12 19:26:18 [ERROR] [lib.training.pipeline:1650] Error training fold 4: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.24 GiB is free. Including non-PyTorch memory, this process has 2.52 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 69.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.24 GiB is free. Including non-PyTorch memory, this process has 2.52 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 69.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:26:18 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 4f529abc0793444f8adc58f4017443f3
2025-12-12 19:26:19 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 5/5 (20% sample)
2025-12-12 19:26:19 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:26:19 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:26:19 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:26:19 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:26:20 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:26:20 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 24e08016e84c4e1eaa05c296043b2ffb (experiment: x3d)
2025-12-12 19:26:20 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 5...
2025-12-12 19:26:21 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:26:22 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:26:22 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:26:25 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:26:26 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 2960.27734375, 'cpu_memory_gb': 2.8908958435058594, 'cpu_vms_mb': 86426.55078125, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.749024768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.179317248}
2025-12-12 19:26:26 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:26:26 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:26:28 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:26:28 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 2960.27734375, 'cpu_memory_gb': 2.8908958435058594, 'cpu_vms_mb': 86426.55078125, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.749024768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.179317248}
2025-12-12 19:26:28 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:26:28 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:26:30 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:26:31 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:26:31 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 2960.27734375, 'cpu_memory_gb': 2.8908958435058594, 'cpu_vms_mb': 86426.55078125, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.749024768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.179317248}
2025-12-12 19:26:31 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:26:31 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:26:35 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.55 GiB is free. Including non-PyTorch memory, this process has 3.21 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 101.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:26:35 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 2960.27734375, 'cpu_memory_gb': 2.8908958435058594, 'cpu_vms_mb': 122842.55078125, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.273312768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.655029248}
2025-12-12 19:26:35 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.55 GiB is free. Including non-PyTorch memory, this process has 3.21 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 101.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Final batch size: 1
2025-12-12 19:26:35 [ERROR] [lib.training.pipeline:1650] Error training fold 5: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.55 GiB is free. Including non-PyTorch memory, this process has 3.21 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 101.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.55 GiB is free. Including non-PyTorch memory, this process has 3.21 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 101.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:26:35 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 24e08016e84c4e1eaa05c296043b2ffb
2025-12-12 19:26:36 [INFO] [lib.training.pipeline:818] 
================================================================================
2025-12-12 19:26:36 [INFO] [lib.training.pipeline:819] Grid Search: Hyperparameter combination 2/16
2025-12-12 19:26:36 [INFO] [lib.training.pipeline:820] Parameters: {'learning_rate': 5e-05, 'backbone_lr': 1e-06, 'head_lr': 0.0001, 'weight_decay': 0.001}
2025-12-12 19:26:36 [INFO] [lib.training.pipeline:821] ================================================================================
2025-12-12 19:26:36 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 1/5 (20% sample)
2025-12-12 19:26:36 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:26:36 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:26:36 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:26:36 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:26:36 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:26:36 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 4afeb1ed53af44dfbf75c8a7a9629c7b (experiment: x3d)
2025-12-12 19:26:36 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 1...
2025-12-12 19:26:38 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:26:39 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:26:41 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:26:44 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.70 GiB is free. Including non-PyTorch memory, this process has 4.07 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 90.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:26:44 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 2960.27734375, 'cpu_memory_gb': 2.8908958435058594, 'cpu_vms_mb': 159258.55078125, 'gpu_allocated_gb': 2.828276736, 'gpu_reserved_gb': 2.923429888, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.004912128}
2025-12-12 19:26:44 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.70 GiB is free. Including non-PyTorch memory, this process has 4.07 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 90.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:26:44 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:26:47 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.24 GiB is free. Including non-PyTorch memory, this process has 2.52 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 69.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:26:48 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 2960.27734375, 'cpu_memory_gb': 2.8908958435058594, 'cpu_vms_mb': 159258.55078125, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.728053248, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.200288768}
2025-12-12 19:26:48 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.24 GiB is free. Including non-PyTorch memory, this process has 2.52 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 69.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:26:48 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:26:51 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.57 GiB is free. Including non-PyTorch memory, this process has 3.19 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 81.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:26:51 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 2960.27734375, 'cpu_memory_gb': 2.8908958435058594, 'cpu_vms_mb': 195674.55078125, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.252341248, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.676000768}
2025-12-12 19:26:51 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.57 GiB is free. Including non-PyTorch memory, this process has 3.19 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 81.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:26:51 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:26:55 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:26:58 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.70 GiB is free. Including non-PyTorch memory, this process has 4.07 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 90.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:26:58 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3462.59375, 'cpu_memory_gb': 3.381439208984375, 'cpu_vms_mb': 232592.88671875, 'gpu_allocated_gb': 2.828276736, 'gpu_reserved_gb': 2.923429888, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.004912128}
2025-12-12 19:26:58 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.70 GiB is free. Including non-PyTorch memory, this process has 4.07 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 90.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Final batch size: 1
2025-12-12 19:26:58 [ERROR] [lib.training.pipeline:1650] Error training fold 1: CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.70 GiB is free. Including non-PyTorch memory, this process has 4.07 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 90.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.70 GiB is free. Including non-PyTorch memory, this process has 4.07 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 90.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:26:58 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 4afeb1ed53af44dfbf75c8a7a9629c7b
2025-12-12 19:26:58 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 2/5 (20% sample)
2025-12-12 19:26:58 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:26:58 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:26:59 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:26:59 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:26:59 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:26:59 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: bea155bd6b194beebe7cb45472a2325f (experiment: x3d)
2025-12-12 19:26:59 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 2...
2025-12-12 19:27:01 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:27:01 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:27:06 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:27:07 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3572.859375, 'cpu_memory_gb': 3.4891204833984375, 'cpu_vms_mb': 232702.90625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:27:07 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:27:07 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:27:08 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:27:09 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:27:09 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3572.87109375, 'cpu_memory_gb': 3.4891319274902344, 'cpu_vms_mb': 232702.90625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:27:09 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:27:09 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:27:12 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:27:12 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3463.3515625, 'cpu_memory_gb': 3.3821792602539062, 'cpu_vms_mb': 232593.31640625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:27:12 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:27:12 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:27:15 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:27:15 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3463.3515625, 'cpu_memory_gb': 3.3821792602539062, 'cpu_vms_mb': 232593.31640625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:27:15 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Final batch size: 1
2025-12-12 19:27:15 [ERROR] [lib.training.pipeline:1650] Error training fold 2: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:27:15 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: bea155bd6b194beebe7cb45472a2325f
2025-12-12 19:27:15 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 3/5 (20% sample)
2025-12-12 19:27:15 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:27:15 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:27:15 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:27:15 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:27:16 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:27:16 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 7025454dc65f479eb593f1b39aaff9a0 (experiment: x3d)
2025-12-12 19:27:16 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 3...
2025-12-12 19:27:17 [INFO] [lib.models.video:417] Chunk size increased from 210 to 220 (after 3 successes, increment: 10)
2025-12-12 19:27:18 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:27:18 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:27:21 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:27:21 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3463.3515625, 'cpu_memory_gb': 3.3821792602539062, 'cpu_vms_mb': 232593.31640625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:27:21 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:27:21 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:27:27 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.66 GiB is free. Including non-PyTorch memory, this process has 4.11 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 130.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:27:27 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3463.3515625, 'cpu_memory_gb': 3.3821792602539062, 'cpu_vms_mb': 269009.31640625, 'gpu_allocated_gb': 2.828276736, 'gpu_reserved_gb': 2.965372928, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 13.962969088}
2025-12-12 19:27:27 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.66 GiB is free. Including non-PyTorch memory, this process has 4.11 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 130.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:27:27 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:27:31 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:27:32 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:27:32 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3463.3671875, 'cpu_memory_gb': 3.3821945190429688, 'cpu_vms_mb': 269009.31640625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:27:32 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:27:32 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:27:35 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 21.42 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.32 GiB is free. Including non-PyTorch memory, this process has 3.44 GiB memory in use. Of the allocated memory 2.96 GiB is allocated by PyTorch, and 118.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:27:36 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3463.3671875, 'cpu_memory_gb': 3.3821945190429688, 'cpu_vms_mb': 305425.31640625, 'gpu_allocated_gb': 2.324468736, 'gpu_reserved_gb': 2.441084928, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.487257088}
2025-12-12 19:27:36 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 21.42 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.32 GiB is free. Including non-PyTorch memory, this process has 3.44 GiB memory in use. Of the allocated memory 2.96 GiB is allocated by PyTorch, and 118.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Final batch size: 1
2025-12-12 19:27:36 [ERROR] [lib.training.pipeline:1650] Error training fold 3: CUDA out of memory. Tried to allocate 21.42 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.32 GiB is free. Including non-PyTorch memory, this process has 3.44 GiB memory in use. Of the allocated memory 2.96 GiB is allocated by PyTorch, and 118.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 21.42 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.32 GiB is free. Including non-PyTorch memory, this process has 3.44 GiB memory in use. Of the allocated memory 2.96 GiB is allocated by PyTorch, and 118.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:27:36 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 7025454dc65f479eb593f1b39aaff9a0
2025-12-12 19:27:36 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 4/5 (20% sample)
2025-12-12 19:27:36 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:27:36 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:27:36 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:27:36 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:27:36 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:27:37 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 186ad6f47138445886aa4b179a0afe3a (experiment: x3d)
2025-12-12 19:27:37 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 4...
2025-12-12 19:27:38 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:27:39 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:27:41 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:27:42 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:27:42 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3463.3671875, 'cpu_memory_gb': 3.3821945190429688, 'cpu_vms_mb': 305425.31640625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.749024768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.179317248}
2025-12-12 19:27:42 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:27:42 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:27:45 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:27:45 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3463.3671875, 'cpu_memory_gb': 3.3821945190429688, 'cpu_vms_mb': 305425.31640625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.749024768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.179317248}
2025-12-12 19:27:45 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:27:45 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:27:51 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:27:51 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3633.59765625, 'cpu_memory_gb': 3.5484352111816406, 'cpu_vms_mb': 305595.28125, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.749024768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.179317248}
2025-12-12 19:27:51 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:27:51 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:27:52 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:27:54 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:27:54 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3462.82421875, 'cpu_memory_gb': 3.381664276123047, 'cpu_vms_mb': 305424.25390625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.749024768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.179317248}
2025-12-12 19:27:54 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Final batch size: 1
2025-12-12 19:27:54 [ERROR] [lib.training.pipeline:1650] Error training fold 4: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:27:54 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 186ad6f47138445886aa4b179a0afe3a
2025-12-12 19:27:54 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 5/5 (20% sample)
2025-12-12 19:27:54 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:27:54 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:27:54 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:27:54 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:27:55 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:27:55 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: ff3378243396442d8d86b8c49a2ab298 (experiment: x3d)
2025-12-12 19:27:55 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 5...
2025-12-12 19:27:57 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:27:57 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:28:00 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.42 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.26 GiB is free. Including non-PyTorch memory, this process has 2.50 GiB memory in use. Of the allocated memory 2.03 GiB is allocated by PyTorch, and 104.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:28:00 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3462.82421875, 'cpu_memory_gb': 3.381664276123047, 'cpu_vms_mb': 305424.25390625, 'gpu_allocated_gb': 1.610740736, 'gpu_reserved_gb': 1.728053248, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.200288768}
2025-12-12 19:28:00 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.42 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.26 GiB is free. Including non-PyTorch memory, this process has 2.50 GiB memory in use. Of the allocated memory 2.03 GiB is allocated by PyTorch, and 104.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:28:00 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:28:02 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:28:03 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:28:03 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3462.82421875, 'cpu_memory_gb': 3.381664276123047, 'cpu_vms_mb': 305424.25390625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.749024768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.179317248}
2025-12-12 19:28:03 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:28:03 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:28:06 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:28:06 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3462.82421875, 'cpu_memory_gb': 3.381664276123047, 'cpu_vms_mb': 305424.25390625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.749024768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.179317248}
2025-12-12 19:28:06 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:28:06 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:28:10 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.55 GiB is free. Including non-PyTorch memory, this process has 3.21 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 101.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:28:10 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3462.82421875, 'cpu_memory_gb': 3.381664276123047, 'cpu_vms_mb': 341840.25390625, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.273312768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.655029248}
2025-12-12 19:28:10 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.55 GiB is free. Including non-PyTorch memory, this process has 3.21 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 101.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Final batch size: 1
2025-12-12 19:28:10 [ERROR] [lib.training.pipeline:1650] Error training fold 5: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.55 GiB is free. Including non-PyTorch memory, this process has 3.21 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 101.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.55 GiB is free. Including non-PyTorch memory, this process has 3.21 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 101.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:28:10 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: ff3378243396442d8d86b8c49a2ab298
2025-12-12 19:28:10 [INFO] [lib.training.pipeline:818] 
================================================================================
2025-12-12 19:28:10 [INFO] [lib.training.pipeline:819] Grid Search: Hyperparameter combination 3/16
2025-12-12 19:28:10 [INFO] [lib.training.pipeline:820] Parameters: {'learning_rate': 5e-05, 'backbone_lr': 1e-06, 'head_lr': 0.0005, 'weight_decay': 0.0001}
2025-12-12 19:28:10 [INFO] [lib.training.pipeline:821] ================================================================================
2025-12-12 19:28:10 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 1/5 (20% sample)
2025-12-12 19:28:10 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:28:10 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:28:10 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:28:10 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:28:11 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:28:11 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: b46a15d2856948e0a600dba7ea24287f (experiment: x3d)
2025-12-12 19:28:11 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 1...
2025-12-12 19:28:12 [INFO] [lib.models.video:417] Chunk size increased from 220 to 230 (after 3 successes, increment: 10)
2025-12-12 19:28:13 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:28:14 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:28:17 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 23.07 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.09 GiB is free. Including non-PyTorch memory, this process has 3.68 GiB memory in use. Of the allocated memory 3.18 GiB is allocated by PyTorch, and 135.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:28:17 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3462.82421875, 'cpu_memory_gb': 3.381664276123047, 'cpu_vms_mb': 378256.25390625, 'gpu_allocated_gb': 2.492404736, 'gpu_reserved_gb': 2.629828608, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.298513408}
2025-12-12 19:28:17 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 23.07 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.09 GiB is free. Including non-PyTorch memory, this process has 3.68 GiB memory in use. Of the allocated memory 3.18 GiB is allocated by PyTorch, and 135.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:28:17 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:28:21 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.54 GiB is free. Including non-PyTorch memory, this process has 3.23 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 121.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:28:21 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3462.82421875, 'cpu_memory_gb': 3.381664276123047, 'cpu_vms_mb': 414672.25390625, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.294284288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.634057728}
2025-12-12 19:28:21 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.54 GiB is free. Including non-PyTorch memory, this process has 3.23 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 121.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:28:21 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:28:22 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:28:23 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:28:24 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3462.82421875, 'cpu_memory_gb': 3.381664276123047, 'cpu_vms_mb': 414672.25390625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:28:24 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:28:24 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:28:29 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.66 GiB is free. Including non-PyTorch memory, this process has 4.11 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 130.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:28:29 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3462.82421875, 'cpu_memory_gb': 3.381664276123047, 'cpu_vms_mb': 451088.25390625, 'gpu_allocated_gb': 2.828276736, 'gpu_reserved_gb': 2.965372928, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 13.962969088}
2025-12-12 19:28:29 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.66 GiB is free. Including non-PyTorch memory, this process has 4.11 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 130.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Final batch size: 1
2025-12-12 19:28:29 [ERROR] [lib.training.pipeline:1650] Error training fold 1: CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.66 GiB is free. Including non-PyTorch memory, this process has 4.11 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 130.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.66 GiB is free. Including non-PyTorch memory, this process has 4.11 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 130.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:28:29 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: b46a15d2856948e0a600dba7ea24287f
2025-12-12 19:28:29 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 2/5 (20% sample)
2025-12-12 19:28:29 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:28:29 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:28:29 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:28:29 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:28:30 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:28:30 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 58185512652b4457b7bb678d0ad8edb1 (experiment: x3d)
2025-12-12 19:28:30 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 2...
2025-12-12 19:28:31 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:28:32 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:28:33 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:28:35 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:28:35 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3462.82421875, 'cpu_memory_gb': 3.381664276123047, 'cpu_vms_mb': 451088.25390625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:28:35 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:28:35 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:28:39 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:28:39 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3462.82421875, 'cpu_memory_gb': 3.381664276123047, 'cpu_vms_mb': 451088.25390625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:28:39 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:28:39 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:28:42 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:28:42 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3462.82421875, 'cpu_memory_gb': 3.381664276123047, 'cpu_vms_mb': 451088.25390625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:28:42 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:28:42 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:28:44 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:28:45 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:28:45 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3462.82421875, 'cpu_memory_gb': 3.381664276123047, 'cpu_vms_mb': 451088.25390625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:28:45 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Final batch size: 1
2025-12-12 19:28:45 [ERROR] [lib.training.pipeline:1650] Error training fold 2: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:28:45 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 58185512652b4457b7bb678d0ad8edb1
2025-12-12 19:28:46 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 3/5 (20% sample)
2025-12-12 19:28:46 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:28:46 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:28:46 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:28:46 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:28:46 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:28:46 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: bd58ee1791874ecd8eb417f5b661115f (experiment: x3d)
2025-12-12 19:28:46 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 3...
2025-12-12 19:28:48 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:28:49 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:28:56 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.52 GiB is free. Including non-PyTorch memory, this process has 3.25 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 141.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:28:56 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3735.82421875, 'cpu_memory_gb': 3.648265838623047, 'cpu_vms_mb': 487777.6328125, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.315255808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.613086208}
2025-12-12 19:28:56 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.52 GiB is free. Including non-PyTorch memory, this process has 3.25 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 141.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:28:56 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:28:59 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:29:00 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:29:00 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3735.82421875, 'cpu_memory_gb': 3.648265838623047, 'cpu_vms_mb': 487777.6328125, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:29:00 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:29:00 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:29:04 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.16 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.59 GiB is free. Including non-PyTorch memory, this process has 3.17 GiB memory in use. Of the allocated memory 2.66 GiB is allocated by PyTorch, and 144.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:29:04 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3735.82421875, 'cpu_memory_gb': 3.648265838623047, 'cpu_vms_mb': 524193.6328125, 'gpu_allocated_gb': 2.093556736, 'gpu_reserved_gb': 2.231369728, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.696972288}
2025-12-12 19:29:04 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 19.16 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.59 GiB is free. Including non-PyTorch memory, this process has 3.17 GiB memory in use. Of the allocated memory 2.66 GiB is allocated by PyTorch, and 144.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:29:04 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:29:07 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 16.48 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.95 GiB is free. Including non-PyTorch memory, this process has 2.82 GiB memory in use. Of the allocated memory 2.31 GiB is allocated by PyTorch, and 146.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:29:07 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3463.5234375, 'cpu_memory_gb': 3.3823471069335938, 'cpu_vms_mb': 523921.25390625, 'gpu_allocated_gb': 1.820660736, 'gpu_reserved_gb': 1.958739968, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.969602048}
2025-12-12 19:29:07 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 16.48 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.95 GiB is free. Including non-PyTorch memory, this process has 2.82 GiB memory in use. Of the allocated memory 2.31 GiB is allocated by PyTorch, and 146.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Final batch size: 1
2025-12-12 19:29:07 [ERROR] [lib.training.pipeline:1650] Error training fold 3: CUDA out of memory. Tried to allocate 16.48 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.95 GiB is free. Including non-PyTorch memory, this process has 2.82 GiB memory in use. Of the allocated memory 2.31 GiB is allocated by PyTorch, and 146.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.48 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.95 GiB is free. Including non-PyTorch memory, this process has 2.82 GiB memory in use. Of the allocated memory 2.31 GiB is allocated by PyTorch, and 146.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:29:07 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: bd58ee1791874ecd8eb417f5b661115f
2025-12-12 19:29:07 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 4/5 (20% sample)
2025-12-12 19:29:07 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:29:07 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:29:07 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:29:07 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:29:08 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:29:08 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 97d25adc461a46159358545f8c15dd94 (experiment: x3d)
2025-12-12 19:29:08 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 4...
2025-12-12 19:29:09 [INFO] [lib.models.video:417] Chunk size increased from 230 to 240 (after 3 successes, increment: 10)
2025-12-12 19:29:10 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:29:11 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:29:14 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.48 GiB is free. Including non-PyTorch memory, this process has 3.29 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 181.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:29:14 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3463.5234375, 'cpu_memory_gb': 3.3823471069335938, 'cpu_vms_mb': 560337.25390625, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.357198848, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.571143168}
2025-12-12 19:29:14 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.48 GiB is free. Including non-PyTorch memory, this process has 3.29 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 181.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:29:14 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:29:18 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.48 GiB is free. Including non-PyTorch memory, this process has 3.29 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 181.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:29:18 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3463.5234375, 'cpu_memory_gb': 3.3823471069335938, 'cpu_vms_mb': 596753.25390625, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.357198848, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.571143168}
2025-12-12 19:29:18 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.48 GiB is free. Including non-PyTorch memory, this process has 3.29 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 181.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:29:18 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:29:20 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:29:22 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:29:22 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3463.54296875, 'cpu_memory_gb': 3.382366180419922, 'cpu_vms_mb': 596753.25390625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.832910848, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.095431168}
2025-12-12 19:29:22 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:29:22 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:29:26 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:29:26 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3463.54296875, 'cpu_memory_gb': 3.382366180419922, 'cpu_vms_mb': 596753.25390625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.832910848, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.095431168}
2025-12-12 19:29:26 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Final batch size: 1
2025-12-12 19:29:26 [ERROR] [lib.training.pipeline:1650] Error training fold 4: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:29:26 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 97d25adc461a46159358545f8c15dd94
2025-12-12 19:29:27 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 5/5 (20% sample)
2025-12-12 19:29:27 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:29:27 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:29:27 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:29:27 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:29:27 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:29:27 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 65ae44c0731b45fb8b3603f95bf24cca (experiment: x3d)
2025-12-12 19:29:27 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 5...
2025-12-12 19:29:29 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:29:30 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:29:31 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:29:33 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:29:33 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3463.54296875, 'cpu_memory_gb': 3.382366180419922, 'cpu_vms_mb': 596753.25390625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:29:33 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:29:33 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:29:38 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.54 GiB is free. Including non-PyTorch memory, this process has 3.23 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 121.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:29:38 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3463.54296875, 'cpu_memory_gb': 3.382366180419922, 'cpu_vms_mb': 633169.25390625, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.294284288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.634057728}
2025-12-12 19:29:38 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.54 GiB is free. Including non-PyTorch memory, this process has 3.23 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 121.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:29:38 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:29:41 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:29:41 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3463.54296875, 'cpu_memory_gb': 3.382366180419922, 'cpu_vms_mb': 633169.25390625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:29:41 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:29:41 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:29:55 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:29:56 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:29:56 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 7328.29296875, 'cpu_memory_gb': 7.156536102294922, 'cpu_vms_mb': 637034.1953125, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:29:57 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Final batch size: 1
2025-12-12 19:29:57 [ERROR] [lib.training.pipeline:1650] Error training fold 5: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:29:57 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 65ae44c0731b45fb8b3603f95bf24cca
2025-12-12 19:29:57 [INFO] [lib.training.pipeline:818] 
================================================================================
2025-12-12 19:29:57 [INFO] [lib.training.pipeline:819] Grid Search: Hyperparameter combination 4/16
2025-12-12 19:29:57 [INFO] [lib.training.pipeline:820] Parameters: {'learning_rate': 5e-05, 'backbone_lr': 1e-06, 'head_lr': 0.0005, 'weight_decay': 0.001}
2025-12-12 19:29:57 [INFO] [lib.training.pipeline:821] ================================================================================
2025-12-12 19:29:57 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 1/5 (20% sample)
2025-12-12 19:29:57 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:29:57 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:29:57 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:29:57 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:29:57 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:29:57 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: cb9f9d7eea7145808f76bd1487b2a65c (experiment: x3d)
2025-12-12 19:29:57 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 1...
2025-12-12 19:29:59 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:30:00 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:30:02 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 156.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:30:03 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 7328.29296875, 'cpu_memory_gb': 7.156536102294922, 'cpu_vms_mb': 637034.1953125, 'gpu_allocated_gb': 1.631732736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:30:03 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 156.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:30:03 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:30:04 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:30:05 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:30:05 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 7328.29296875, 'cpu_memory_gb': 7.156536102294922, 'cpu_vms_mb': 637034.1953125, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:30:05 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:30:05 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:30:08 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:30:08 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 7328.29296875, 'cpu_memory_gb': 7.156536102294922, 'cpu_vms_mb': 637034.1953125, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:30:08 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:30:08 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:30:11 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:30:11 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 7328.29296875, 'cpu_memory_gb': 7.156536102294922, 'cpu_vms_mb': 637034.1953125, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:30:11 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Final batch size: 1
2025-12-12 19:30:11 [ERROR] [lib.training.pipeline:1650] Error training fold 1: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:30:11 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: cb9f9d7eea7145808f76bd1487b2a65c
2025-12-12 19:30:11 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 2/5 (20% sample)
2025-12-12 19:30:11 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:30:11 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:30:12 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:30:12 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:30:12 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:30:12 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 328c31b07acf44dfbf88bb9973b48248 (experiment: x3d)
2025-12-12 19:30:12 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 2...
2025-12-12 19:30:13 [INFO] [lib.models.video:417] Chunk size increased from 210 to 220 (after 3 successes, increment: 10)
2025-12-12 19:30:13 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:30:14 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:30:18 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 23.07 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.07 GiB is free. Including non-PyTorch memory, this process has 3.70 GiB memory in use. Of the allocated memory 3.18 GiB is allocated by PyTorch, and 155.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:30:18 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3465.484375, 'cpu_memory_gb': 3.3842620849609375, 'cpu_vms_mb': 669587.25390625, 'gpu_allocated_gb': 2.492404736, 'gpu_reserved_gb': 2.650800128, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.277541888}
2025-12-12 19:30:18 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 23.07 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.07 GiB is free. Including non-PyTorch memory, this process has 3.70 GiB memory in use. Of the allocated memory 3.18 GiB is allocated by PyTorch, and 155.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:30:18 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:30:22 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.52 GiB is free. Including non-PyTorch memory, this process has 3.25 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 141.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:30:22 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3465.48828125, 'cpu_memory_gb': 3.384265899658203, 'cpu_vms_mb': 706003.25390625, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.315255808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.613086208}
2025-12-12 19:30:22 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.52 GiB is free. Including non-PyTorch memory, this process has 3.25 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 141.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:30:22 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:30:25 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:30:27 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:30:27 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3465.49609375, 'cpu_memory_gb': 3.3842735290527344, 'cpu_vms_mb': 706003.25390625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:30:27 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:30:27 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:30:30 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:30:30 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3465.49609375, 'cpu_memory_gb': 3.3842735290527344, 'cpu_vms_mb': 706003.25390625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:30:30 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Final batch size: 1
2025-12-12 19:30:30 [ERROR] [lib.training.pipeline:1650] Error training fold 2: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:30:30 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 328c31b07acf44dfbf88bb9973b48248
2025-12-12 19:30:30 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 3/5 (20% sample)
2025-12-12 19:30:30 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:30:30 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:30:30 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:30:30 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:30:31 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:30:31 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: db03ada160df4e5ea8e5d9469f23912b (experiment: x3d)
2025-12-12 19:30:31 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 3...
2025-12-12 19:30:33 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:30:34 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:30:35 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:30:36 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:30:36 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3465.49609375, 'cpu_memory_gb': 3.3842735290527344, 'cpu_vms_mb': 706003.25390625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.832910848, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.095431168}
2025-12-12 19:30:36 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:30:36 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:30:39 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:30:39 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3465.49609375, 'cpu_memory_gb': 3.3842735290527344, 'cpu_vms_mb': 706003.25390625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.832910848, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.095431168}
2025-12-12 19:30:39 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:30:39 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:30:42 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:30:42 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3465.49609375, 'cpu_memory_gb': 3.3842735290527344, 'cpu_vms_mb': 706003.25390625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.832910848, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.095431168}
2025-12-12 19:30:42 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:30:42 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:30:43 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:30:45 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 21.42 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.26 GiB is free. Including non-PyTorch memory, this process has 3.50 GiB memory in use. Of the allocated memory 2.96 GiB is allocated by PyTorch, and 178.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:30:46 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3465.49609375, 'cpu_memory_gb': 3.3842735290527344, 'cpu_vms_mb': 742419.25390625, 'gpu_allocated_gb': 2.324468736, 'gpu_reserved_gb': 2.503999488, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.424342528}
2025-12-12 19:30:46 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 21.42 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.26 GiB is free. Including non-PyTorch memory, this process has 3.50 GiB memory in use. Of the allocated memory 2.96 GiB is allocated by PyTorch, and 178.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Final batch size: 1
2025-12-12 19:30:46 [ERROR] [lib.training.pipeline:1650] Error training fold 3: CUDA out of memory. Tried to allocate 21.42 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.26 GiB is free. Including non-PyTorch memory, this process has 3.50 GiB memory in use. Of the allocated memory 2.96 GiB is allocated by PyTorch, and 178.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 21.42 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.26 GiB is free. Including non-PyTorch memory, this process has 3.50 GiB memory in use. Of the allocated memory 2.96 GiB is allocated by PyTorch, and 178.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:30:46 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: db03ada160df4e5ea8e5d9469f23912b
2025-12-12 19:30:46 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 4/5 (20% sample)
2025-12-12 19:30:46 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:30:46 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:30:46 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:30:46 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:30:47 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:30:47 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: d6bba86f995a4043b3a9f0b6547810c6 (experiment: x3d)
2025-12-12 19:30:47 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 4...
2025-12-12 19:30:49 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:30:49 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:30:52 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:30:52 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3465.49609375, 'cpu_memory_gb': 3.3842735290527344, 'cpu_vms_mb': 742419.25390625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:30:52 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:30:52 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:31:00 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:31:01 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 116.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:31:02 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5072.40625, 'cpu_memory_gb': 4.953521728515625, 'cpu_vms_mb': 744026.19921875, 'gpu_allocated_gb': 1.631732736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:31:02 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 116.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:31:02 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:31:06 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 24.93 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.85 GiB is free. Including non-PyTorch memory, this process has 3.91 GiB memory in use. Of the allocated memory 3.42 GiB is allocated by PyTorch, and 125.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:31:06 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3465.49609375, 'cpu_memory_gb': 3.3842735290527344, 'cpu_vms_mb': 778835.25390625, 'gpu_allocated_gb': 2.681332736, 'gpu_reserved_gb': 2.797600768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.130741248}
2025-12-12 19:31:06 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 24.93 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.85 GiB is free. Including non-PyTorch memory, this process has 3.91 GiB memory in use. Of the allocated memory 3.42 GiB is allocated by PyTorch, and 125.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:31:06 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:31:10 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 21.42 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.32 GiB is free. Including non-PyTorch memory, this process has 3.44 GiB memory in use. Of the allocated memory 2.96 GiB is allocated by PyTorch, and 118.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:31:10 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3465.49609375, 'cpu_memory_gb': 3.3842735290527344, 'cpu_vms_mb': 815251.25390625, 'gpu_allocated_gb': 2.324468736, 'gpu_reserved_gb': 2.441084928, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.487257088}
2025-12-12 19:31:10 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 21.42 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.32 GiB is free. Including non-PyTorch memory, this process has 3.44 GiB memory in use. Of the allocated memory 2.96 GiB is allocated by PyTorch, and 118.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Final batch size: 1
2025-12-12 19:31:10 [ERROR] [lib.training.pipeline:1650] Error training fold 4: CUDA out of memory. Tried to allocate 21.42 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.32 GiB is free. Including non-PyTorch memory, this process has 3.44 GiB memory in use. Of the allocated memory 2.96 GiB is allocated by PyTorch, and 118.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 21.42 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.32 GiB is free. Including non-PyTorch memory, this process has 3.44 GiB memory in use. Of the allocated memory 2.96 GiB is allocated by PyTorch, and 118.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:31:10 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: d6bba86f995a4043b3a9f0b6547810c6
2025-12-12 19:31:10 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 5/5 (20% sample)
2025-12-12 19:31:10 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:31:10 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:31:10 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:31:10 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:31:11 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:31:11 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 3431bcc4d37a46e093e5639a2ab1c273 (experiment: x3d)
2025-12-12 19:31:11 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 5...
2025-12-12 19:31:12 [INFO] [lib.models.video:417] Chunk size increased from 240 to 250 (after 3 successes, increment: 10)
2025-12-12 19:31:13 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:31:14 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:31:18 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.60 GiB is free. Including non-PyTorch memory, this process has 4.16 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 190.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:31:18 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3465.49609375, 'cpu_memory_gb': 3.3842735290527344, 'cpu_vms_mb': 851667.25390625, 'gpu_allocated_gb': 2.828276736, 'gpu_reserved_gb': 3.028287488, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 13.900054528}
2025-12-12 19:31:18 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.60 GiB is free. Including non-PyTorch memory, this process has 4.16 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 190.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:31:18 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:31:22 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:31:22 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3465.49609375, 'cpu_memory_gb': 3.3842735290527344, 'cpu_vms_mb': 851667.25390625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.832910848, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.095431168}
2025-12-12 19:31:22 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:31:22 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:31:25 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:31:27 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:31:27 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3465.49609375, 'cpu_memory_gb': 3.3842735290527344, 'cpu_vms_mb': 851667.25390625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.832910848, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.095431168}
2025-12-12 19:31:27 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:31:27 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:31:31 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.48 GiB is free. Including non-PyTorch memory, this process has 3.29 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 181.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:31:31 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3465.49609375, 'cpu_memory_gb': 3.3842735290527344, 'cpu_vms_mb': 888083.25390625, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.357198848, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.571143168}
2025-12-12 19:31:31 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.48 GiB is free. Including non-PyTorch memory, this process has 3.29 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 181.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Final batch size: 1
2025-12-12 19:31:31 [ERROR] [lib.training.pipeline:1650] Error training fold 5: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.48 GiB is free. Including non-PyTorch memory, this process has 3.29 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 181.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.48 GiB is free. Including non-PyTorch memory, this process has 3.29 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 181.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:31:31 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 3431bcc4d37a46e093e5639a2ab1c273
2025-12-12 19:31:31 [INFO] [lib.training.pipeline:818] 
================================================================================
2025-12-12 19:31:31 [INFO] [lib.training.pipeline:819] Grid Search: Hyperparameter combination 5/16
2025-12-12 19:31:31 [INFO] [lib.training.pipeline:820] Parameters: {'learning_rate': 5e-05, 'backbone_lr': 5e-06, 'head_lr': 0.0001, 'weight_decay': 0.0001}
2025-12-12 19:31:31 [INFO] [lib.training.pipeline:821] ================================================================================
2025-12-12 19:31:31 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 1/5 (20% sample)
2025-12-12 19:31:31 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:31:31 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:31:32 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:31:32 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:31:32 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:31:32 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 6a912f0ec45d4eee8dc4a1432241e00c (experiment: x3d)
2025-12-12 19:31:32 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 1...
2025-12-12 19:31:34 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:31:35 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:31:36 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:31:37 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.42 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.03 GiB is allocated by PyTorch, and 164.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:31:37 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3465.49609375, 'cpu_memory_gb': 3.3842735290527344, 'cpu_vms_mb': 888083.25390625, 'gpu_allocated_gb': 1.610740736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:31:37 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.42 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.03 GiB is allocated by PyTorch, and 164.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:31:37 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:31:41 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:31:41 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3465.49609375, 'cpu_memory_gb': 3.3842735290527344, 'cpu_vms_mb': 888083.25390625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:31:41 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:31:41 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:31:44 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 21.01 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.32 GiB is free. Including non-PyTorch memory, this process has 3.44 GiB memory in use. Of the allocated memory 2.90 GiB is allocated by PyTorch, and 174.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:31:44 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3465.49609375, 'cpu_memory_gb': 3.3842735290527344, 'cpu_vms_mb': 924499.25390625, 'gpu_allocated_gb': 2.282484736, 'gpu_reserved_gb': 2.462056448, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.466285568}
2025-12-12 19:31:44 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 21.01 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.32 GiB is free. Including non-PyTorch memory, this process has 3.44 GiB memory in use. Of the allocated memory 2.90 GiB is allocated by PyTorch, and 174.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:31:44 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:31:45 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:31:47 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:31:47 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3465.49609375, 'cpu_memory_gb': 3.3842735290527344, 'cpu_vms_mb': 924499.25390625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:31:47 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Final batch size: 1
2025-12-12 19:31:47 [ERROR] [lib.training.pipeline:1650] Error training fold 1: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:31:47 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 6a912f0ec45d4eee8dc4a1432241e00c
2025-12-12 19:31:47 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 2/5 (20% sample)
2025-12-12 19:31:47 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:31:47 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:31:48 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:31:48 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:31:48 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:31:48 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: d9b59659635b43f8963c2542694727ef (experiment: x3d)
2025-12-12 19:31:48 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 2...
2025-12-12 19:31:50 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:31:50 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:31:54 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 21.22 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.36 GiB is free. Including non-PyTorch memory, this process has 3.40 GiB memory in use. Of the allocated memory 2.93 GiB is allocated by PyTorch, and 106.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:31:54 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3465.49609375, 'cpu_memory_gb': 3.3842735290527344, 'cpu_vms_mb': 960915.25390625, 'gpu_allocated_gb': 2.303476736, 'gpu_reserved_gb': 2.420113408, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.508228608}
2025-12-12 19:31:54 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 21.22 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.36 GiB is free. Including non-PyTorch memory, this process has 3.40 GiB memory in use. Of the allocated memory 2.93 GiB is allocated by PyTorch, and 106.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:31:54 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:32:00 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:32:02 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.54 GiB is free. Including non-PyTorch memory, this process has 3.23 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 121.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:32:02 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3741.671875, 'cpu_memory_gb': 3.6539764404296875, 'cpu_vms_mb': 997607.5078125, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.294284288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.634057728}
2025-12-12 19:32:02 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.54 GiB is free. Including non-PyTorch memory, this process has 3.23 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 121.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:32:02 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:32:05 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:32:05 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3741.671875, 'cpu_memory_gb': 3.6539764404296875, 'cpu_vms_mb': 997607.5078125, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:32:05 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:32:05 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:32:08 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:32:09 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3741.68359375, 'cpu_memory_gb': 3.6539878845214844, 'cpu_vms_mb': 997607.5078125, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:32:09 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Final batch size: 1
2025-12-12 19:32:09 [ERROR] [lib.training.pipeline:1650] Error training fold 2: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:32:09 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: d9b59659635b43f8963c2542694727ef
2025-12-12 19:32:09 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 3/5 (20% sample)
2025-12-12 19:32:09 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:32:09 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:32:09 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:32:09 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:32:09 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:32:09 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 6baeffc5eb6a453c8465a90a329e897e (experiment: x3d)
2025-12-12 19:32:09 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 3...
2025-12-12 19:32:11 [INFO] [lib.models.video:417] Chunk size increased from 250 to 260 (after 3 successes, increment: 10)
2025-12-12 19:32:11 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:32:12 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:32:16 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:32:16 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3741.68359375, 'cpu_memory_gb': 3.6539878845214844, 'cpu_vms_mb': 997607.5078125, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.832910848, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.095431168}
2025-12-12 19:32:16 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:32:16 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:32:20 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:32:20 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3741.68359375, 'cpu_memory_gb': 3.6539878845214844, 'cpu_vms_mb': 997607.5078125, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.832910848, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.095431168}
2025-12-12 19:32:20 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:32:20 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:32:22 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:32:24 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.50 GiB is free. Including non-PyTorch memory, this process has 3.27 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 161.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:32:24 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3741.68359375, 'cpu_memory_gb': 3.6539878845214844, 'cpu_vms_mb': 1034023.5078125, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.336227328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.592114688}
2025-12-12 19:32:24 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.50 GiB is free. Including non-PyTorch memory, this process has 3.27 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 161.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:32:24 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:32:26 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:32:26 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3741.68359375, 'cpu_memory_gb': 3.6539878845214844, 'cpu_vms_mb': 1034023.5078125, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.832910848, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.095431168}
2025-12-12 19:32:26 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Final batch size: 1
2025-12-12 19:32:26 [ERROR] [lib.training.pipeline:1650] Error training fold 3: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:32:26 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 6baeffc5eb6a453c8465a90a329e897e
2025-12-12 19:32:27 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 4/5 (20% sample)
2025-12-12 19:32:27 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:32:27 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:32:27 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:32:27 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:32:27 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:32:27 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 0217f3a00f3744bd8624d9eb30998bc0 (experiment: x3d)
2025-12-12 19:32:27 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 4...
2025-12-12 19:32:29 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:32:30 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:32:31 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:32:32 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:32:32 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3741.68359375, 'cpu_memory_gb': 3.6539878845214844, 'cpu_vms_mb': 1034023.5078125, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:32:32 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:32:32 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:32:36 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:32:36 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3741.68359375, 'cpu_memory_gb': 3.6539878845214844, 'cpu_vms_mb': 1034023.5078125, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:32:36 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:32:36 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:32:40 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 21.01 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.34 GiB is free. Including non-PyTorch memory, this process has 3.42 GiB memory in use. Of the allocated memory 2.90 GiB is allocated by PyTorch, and 154.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:32:40 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3741.68359375, 'cpu_memory_gb': 3.6539878845214844, 'cpu_vms_mb': 1070439.5078125, 'gpu_allocated_gb': 2.282484736, 'gpu_reserved_gb': 2.441084928, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.487257088}
2025-12-12 19:32:40 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 21.01 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.34 GiB is free. Including non-PyTorch memory, this process has 3.42 GiB memory in use. Of the allocated memory 2.90 GiB is allocated by PyTorch, and 154.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:32:40 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:32:41 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:32:43 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 16.48 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.95 GiB is free. Including non-PyTorch memory, this process has 2.82 GiB memory in use. Of the allocated memory 2.31 GiB is allocated by PyTorch, and 146.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:32:43 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3741.68359375, 'cpu_memory_gb': 3.6539878845214844, 'cpu_vms_mb': 1070439.5078125, 'gpu_allocated_gb': 1.820660736, 'gpu_reserved_gb': 1.958739968, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.969602048}
2025-12-12 19:32:43 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 16.48 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.95 GiB is free. Including non-PyTorch memory, this process has 2.82 GiB memory in use. Of the allocated memory 2.31 GiB is allocated by PyTorch, and 146.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Final batch size: 1
2025-12-12 19:32:43 [ERROR] [lib.training.pipeline:1650] Error training fold 4: CUDA out of memory. Tried to allocate 16.48 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.95 GiB is free. Including non-PyTorch memory, this process has 2.82 GiB memory in use. Of the allocated memory 2.31 GiB is allocated by PyTorch, and 146.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.48 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.95 GiB is free. Including non-PyTorch memory, this process has 2.82 GiB memory in use. Of the allocated memory 2.31 GiB is allocated by PyTorch, and 146.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:32:43 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 0217f3a00f3744bd8624d9eb30998bc0
2025-12-12 19:32:44 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 5/5 (20% sample)
2025-12-12 19:32:44 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:32:44 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:32:44 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:32:44 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:32:44 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:32:44 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: dad99ac4672f427890942039f9864c7d (experiment: x3d)
2025-12-12 19:32:44 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 5...
2025-12-12 19:32:46 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:32:47 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:32:50 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:32:50 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3741.68359375, 'cpu_memory_gb': 3.6539878845214844, 'cpu_vms_mb': 1070439.5078125, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:32:50 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:32:50 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:32:51 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:32:52 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:32:52 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 3741.68359375, 'cpu_memory_gb': 3.6539878845214844, 'cpu_vms_mb': 1070439.5078125, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:32:52 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:32:52 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:32:59 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:32:59 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 4468.71875, 'cpu_memory_gb': 4.363983154296875, 'cpu_vms_mb': 1071166.75, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:32:59 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:32:59 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:33:02 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:33:02 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 4468.72265625, 'cpu_memory_gb': 4.363986968994141, 'cpu_vms_mb': 1071166.75, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:33:02 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Final batch size: 1
2025-12-12 19:33:02 [ERROR] [lib.training.pipeline:1650] Error training fold 5: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:33:02 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: dad99ac4672f427890942039f9864c7d
2025-12-12 19:33:02 [INFO] [lib.training.pipeline:818] 
================================================================================
2025-12-12 19:33:02 [INFO] [lib.training.pipeline:819] Grid Search: Hyperparameter combination 6/16
2025-12-12 19:33:02 [INFO] [lib.training.pipeline:820] Parameters: {'learning_rate': 5e-05, 'backbone_lr': 5e-06, 'head_lr': 0.0001, 'weight_decay': 0.001}
2025-12-12 19:33:02 [INFO] [lib.training.pipeline:821] ================================================================================
2025-12-12 19:33:02 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 1/5 (20% sample)
2025-12-12 19:33:02 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:33:02 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:33:02 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:33:02 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:33:03 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:33:03 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 6f571b133d154db2a268ae52fb8584cc (experiment: x3d)
2025-12-12 19:33:03 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 1...
2025-12-12 19:33:04 [INFO] [lib.models.video:417] Chunk size increased from 260 to 270 (after 3 successes, increment: 10)
2025-12-12 19:33:05 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:33:05 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:33:13 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 18.33 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.71 GiB is free. Including non-PyTorch memory, this process has 3.05 GiB memory in use. Of the allocated memory 2.55 GiB is allocated by PyTorch, and 135.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:33:14 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 4445.0859375, 'cpu_memory_gb': 4.340904235839844, 'cpu_vms_mb': 1107559.1796875, 'gpu_allocated_gb': 2.009588736, 'gpu_reserved_gb': 2.147483648, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.780858368}
2025-12-12 19:33:14 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 18.33 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.71 GiB is free. Including non-PyTorch memory, this process has 3.05 GiB memory in use. Of the allocated memory 2.55 GiB is allocated by PyTorch, and 135.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:33:14 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:33:16 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:33:16 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 4445.08984375, 'cpu_memory_gb': 4.340908050537109, 'cpu_vms_mb': 1107559.1796875, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:33:16 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:33:16 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:33:18 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:33:19 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:33:19 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 4445.08984375, 'cpu_memory_gb': 4.340908050537109, 'cpu_vms_mb': 1107559.1796875, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:33:19 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:33:19 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:33:22 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:33:22 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 4445.08984375, 'cpu_memory_gb': 4.340908050537109, 'cpu_vms_mb': 1107559.1796875, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:33:22 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Final batch size: 1
2025-12-12 19:33:22 [ERROR] [lib.training.pipeline:1650] Error training fold 1: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:33:22 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 6f571b133d154db2a268ae52fb8584cc
2025-12-12 19:33:22 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 2/5 (20% sample)
2025-12-12 19:33:22 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:33:22 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:33:22 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:33:22 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:33:23 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:33:23 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 0d93b3dadfb84ee5a0be27e3b23c7e69 (experiment: x3d)
2025-12-12 19:33:23 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 2...
2025-12-12 19:33:24 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:33:25 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:33:26 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:33:28 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 21.01 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.38 GiB is free. Including non-PyTorch memory, this process has 3.38 GiB memory in use. Of the allocated memory 2.90 GiB is allocated by PyTorch, and 114.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:33:28 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 4445.08984375, 'cpu_memory_gb': 4.340908050537109, 'cpu_vms_mb': 1143975.1796875, 'gpu_allocated_gb': 2.282484736, 'gpu_reserved_gb': 2.399141888, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.529200128}
2025-12-12 19:33:28 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 21.01 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.38 GiB is free. Including non-PyTorch memory, this process has 3.38 GiB memory in use. Of the allocated memory 2.90 GiB is allocated by PyTorch, and 114.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:33:28 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:33:31 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:33:31 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 4445.08984375, 'cpu_memory_gb': 4.340908050537109, 'cpu_vms_mb': 1143975.1796875, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:33:31 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:33:31 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:33:34 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.54 GiB is free. Including non-PyTorch memory, this process has 3.23 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 121.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:33:34 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 4445.08984375, 'cpu_memory_gb': 4.340908050537109, 'cpu_vms_mb': 1180391.1796875, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.294284288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.634057728}
2025-12-12 19:33:34 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.54 GiB is free. Including non-PyTorch memory, this process has 3.23 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 121.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:33:34 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:33:36 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:33:37 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:33:37 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 4445.08984375, 'cpu_memory_gb': 4.340908050537109, 'cpu_vms_mb': 1180391.1796875, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:33:37 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Final batch size: 1
2025-12-12 19:33:37 [ERROR] [lib.training.pipeline:1650] Error training fold 2: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:33:37 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 0d93b3dadfb84ee5a0be27e3b23c7e69
2025-12-12 19:33:38 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 3/5 (20% sample)
2025-12-12 19:33:38 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:33:38 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:33:38 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:33:38 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:33:38 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:33:38 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: eb153b46630c4b81b4d5092694aa7cfd (experiment: x3d)
2025-12-12 19:33:38 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 3...
2025-12-12 19:33:40 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:33:41 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:33:43 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:33:44 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 4445.08984375, 'cpu_memory_gb': 4.340908050537109, 'cpu_vms_mb': 1180391.1796875, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:33:44 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:33:44 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:33:45 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:33:46 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:33:46 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 4445.08984375, 'cpu_memory_gb': 4.340908050537109, 'cpu_vms_mb': 1180391.1796875, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:33:46 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:33:46 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:33:49 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:33:49 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 4445.08984375, 'cpu_memory_gb': 4.340908050537109, 'cpu_vms_mb': 1180391.1796875, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:33:49 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:33:49 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:33:52 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:33:53 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 4445.08984375, 'cpu_memory_gb': 4.340908050537109, 'cpu_vms_mb': 1180391.1796875, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:33:53 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Final batch size: 1
2025-12-12 19:33:53 [ERROR] [lib.training.pipeline:1650] Error training fold 3: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:33:53 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: eb153b46630c4b81b4d5092694aa7cfd
2025-12-12 19:33:53 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 4/5 (20% sample)
2025-12-12 19:33:53 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:33:53 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:33:53 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:33:53 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:33:53 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:33:53 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: e5907404c99a46d2b471d861d4ae3eba (experiment: x3d)
2025-12-12 19:33:54 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 4...
2025-12-12 19:33:55 [INFO] [lib.models.video:417] Chunk size increased from 270 to 280 (after 3 successes, increment: 10)
2025-12-12 19:33:55 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:33:56 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:34:04 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.24 GiB is free. Including non-PyTorch memory, this process has 2.52 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 96.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:34:04 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5078.73828125, 'cpu_memory_gb': 4.959705352783203, 'cpu_vms_mb': 1181024.80859375, 'gpu_allocated_gb': 1.631732736, 'gpu_reserved_gb': 1.728053248, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.200288768}
2025-12-12 19:34:04 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.24 GiB is free. Including non-PyTorch memory, this process has 2.52 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 96.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:34:04 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:34:07 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:34:07 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5078.73828125, 'cpu_memory_gb': 4.959705352783203, 'cpu_vms_mb': 1181024.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:34:07 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:34:07 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:34:08 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:34:10 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:34:10 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5078.73828125, 'cpu_memory_gb': 4.959705352783203, 'cpu_vms_mb': 1181024.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:34:10 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:34:10 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:34:12 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:34:12 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5078.73828125, 'cpu_memory_gb': 4.959705352783203, 'cpu_vms_mb': 1181024.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:34:12 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Final batch size: 1
2025-12-12 19:34:12 [ERROR] [lib.training.pipeline:1650] Error training fold 4: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:34:12 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: e5907404c99a46d2b471d861d4ae3eba
2025-12-12 19:34:13 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 5/5 (20% sample)
2025-12-12 19:34:13 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:34:13 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:34:13 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:34:13 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:34:13 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:34:13 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 3f9ba4d945284ef68aae574c10435c0b (experiment: x3d)
2025-12-12 19:34:13 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 5...
2025-12-12 19:34:15 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:34:16 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:34:17 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:34:19 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.54 GiB is free. Including non-PyTorch memory, this process has 3.23 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 121.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:34:19 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5078.73828125, 'cpu_memory_gb': 4.959705352783203, 'cpu_vms_mb': 1217440.80859375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.294284288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.634057728}
2025-12-12 19:34:19 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.54 GiB is free. Including non-PyTorch memory, this process has 3.23 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 121.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:34:19 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:34:25 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 116.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:34:25 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5077.75390625, 'cpu_memory_gb': 4.958744049072266, 'cpu_vms_mb': 1217439.80859375, 'gpu_allocated_gb': 1.631732736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:34:25 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 116.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:34:25 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:34:28 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:34:28 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5077.75390625, 'cpu_memory_gb': 4.958744049072266, 'cpu_vms_mb': 1217439.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:34:28 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:34:28 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:34:29 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:34:31 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:34:31 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5077.75390625, 'cpu_memory_gb': 4.958744049072266, 'cpu_vms_mb': 1217439.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:34:31 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Final batch size: 1
2025-12-12 19:34:31 [ERROR] [lib.training.pipeline:1650] Error training fold 5: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:34:31 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 3f9ba4d945284ef68aae574c10435c0b
2025-12-12 19:34:31 [INFO] [lib.training.pipeline:818] 
================================================================================
2025-12-12 19:34:31 [INFO] [lib.training.pipeline:819] Grid Search: Hyperparameter combination 7/16
2025-12-12 19:34:31 [INFO] [lib.training.pipeline:820] Parameters: {'learning_rate': 5e-05, 'backbone_lr': 5e-06, 'head_lr': 0.0005, 'weight_decay': 0.0001}
2025-12-12 19:34:31 [INFO] [lib.training.pipeline:821] ================================================================================
2025-12-12 19:34:31 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 1/5 (20% sample)
2025-12-12 19:34:31 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:34:31 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:34:31 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:34:31 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:34:32 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:34:32 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 0924008ed1624f06affd2d5591842495 (experiment: x3d)
2025-12-12 19:34:32 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 1...
2025-12-12 19:34:33 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:34:34 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:34:42 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:34:42 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5079.79296875, 'cpu_memory_gb': 4.960735321044922, 'cpu_vms_mb': 1217441.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:34:42 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:34:42 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:34:43 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:34:44 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:34:45 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5079.79296875, 'cpu_memory_gb': 4.960735321044922, 'cpu_vms_mb': 1217441.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:34:45 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:34:45 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:34:48 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.52 GiB is free. Including non-PyTorch memory, this process has 3.25 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 141.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:34:48 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5079.79296875, 'cpu_memory_gb': 4.960735321044922, 'cpu_vms_mb': 1253857.80859375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.315255808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.613086208}
2025-12-12 19:34:48 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.52 GiB is free. Including non-PyTorch memory, this process has 3.25 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 141.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:34:48 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:34:50 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:34:50 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5079.79296875, 'cpu_memory_gb': 4.960735321044922, 'cpu_vms_mb': 1253857.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:34:50 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Final batch size: 1
2025-12-12 19:34:50 [ERROR] [lib.training.pipeline:1650] Error training fold 1: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:34:51 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 0924008ed1624f06affd2d5591842495
2025-12-12 19:34:51 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 2/5 (20% sample)
2025-12-12 19:34:51 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:34:51 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:34:51 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:34:51 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:34:51 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:34:51 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: b2185ec7f0a3465e94eac482acaadea4 (experiment: x3d)
2025-12-12 19:34:51 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 2...
2025-12-12 19:34:52 [INFO] [lib.models.video:417] Chunk size increased from 220 to 230 (after 3 successes, increment: 10)
2025-12-12 19:34:53 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:34:53 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:34:58 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.55 GiB is free. Including non-PyTorch memory, this process has 3.21 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 101.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:34:58 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5079.79296875, 'cpu_memory_gb': 4.960735321044922, 'cpu_vms_mb': 1290273.80859375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.273312768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.655029248}
2025-12-12 19:34:58 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.55 GiB is free. Including non-PyTorch memory, this process has 3.21 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 101.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:34:58 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:35:00 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:35:00 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5079.79296875, 'cpu_memory_gb': 4.960735321044922, 'cpu_vms_mb': 1290273.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.749024768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.179317248}
2025-12-12 19:35:00 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:35:00 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:35:02 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:35:03 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:35:03 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5079.79296875, 'cpu_memory_gb': 4.960735321044922, 'cpu_vms_mb': 1290273.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.749024768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.179317248}
2025-12-12 19:35:03 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:35:03 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:35:05 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:35:05 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5079.79296875, 'cpu_memory_gb': 4.960735321044922, 'cpu_vms_mb': 1290273.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.749024768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.179317248}
2025-12-12 19:35:05 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Final batch size: 1
2025-12-12 19:35:05 [ERROR] [lib.training.pipeline:1650] Error training fold 2: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:35:05 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: b2185ec7f0a3465e94eac482acaadea4
2025-12-12 19:35:06 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 3/5 (20% sample)
2025-12-12 19:35:06 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:35:06 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:35:06 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:35:06 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:35:07 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:35:07 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 0e9a42340c6344b395890111242895a7 (experiment: x3d)
2025-12-12 19:35:07 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 3...
2025-12-12 19:35:09 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:35:09 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:35:10 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:35:12 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 116.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:35:12 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5079.79296875, 'cpu_memory_gb': 4.960735321044922, 'cpu_vms_mb': 1290273.80859375, 'gpu_allocated_gb': 1.631732736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:35:12 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 116.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:35:12 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:35:15 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 21.42 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.32 GiB is free. Including non-PyTorch memory, this process has 3.44 GiB memory in use. Of the allocated memory 2.96 GiB is allocated by PyTorch, and 118.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:35:15 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5079.79296875, 'cpu_memory_gb': 4.960735321044922, 'cpu_vms_mb': 1326689.80859375, 'gpu_allocated_gb': 2.324468736, 'gpu_reserved_gb': 2.441084928, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.487257088}
2025-12-12 19:35:15 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 21.42 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.32 GiB is free. Including non-PyTorch memory, this process has 3.44 GiB memory in use. Of the allocated memory 2.96 GiB is allocated by PyTorch, and 118.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:35:15 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:35:18 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 21.01 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.36 GiB is free. Including non-PyTorch memory, this process has 3.40 GiB memory in use. Of the allocated memory 2.90 GiB is allocated by PyTorch, and 134.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:35:18 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5079.79296875, 'cpu_memory_gb': 4.960735321044922, 'cpu_vms_mb': 1363105.80859375, 'gpu_allocated_gb': 2.282484736, 'gpu_reserved_gb': 2.420113408, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.508228608}
2025-12-12 19:35:18 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 21.01 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.36 GiB is free. Including non-PyTorch memory, this process has 3.40 GiB memory in use. Of the allocated memory 2.90 GiB is allocated by PyTorch, and 134.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:35:18 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:35:20 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:35:21 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 16.48 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.97 GiB is free. Including non-PyTorch memory, this process has 2.80 GiB memory in use. Of the allocated memory 2.31 GiB is allocated by PyTorch, and 126.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:35:21 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5079.79296875, 'cpu_memory_gb': 4.960735321044922, 'cpu_vms_mb': 1363105.80859375, 'gpu_allocated_gb': 1.820660736, 'gpu_reserved_gb': 1.937768448, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.990573568}
2025-12-12 19:35:21 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 16.48 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.97 GiB is free. Including non-PyTorch memory, this process has 2.80 GiB memory in use. Of the allocated memory 2.31 GiB is allocated by PyTorch, and 126.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Final batch size: 1
2025-12-12 19:35:21 [ERROR] [lib.training.pipeline:1650] Error training fold 3: CUDA out of memory. Tried to allocate 16.48 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.97 GiB is free. Including non-PyTorch memory, this process has 2.80 GiB memory in use. Of the allocated memory 2.31 GiB is allocated by PyTorch, and 126.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.48 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.97 GiB is free. Including non-PyTorch memory, this process has 2.80 GiB memory in use. Of the allocated memory 2.31 GiB is allocated by PyTorch, and 126.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:35:21 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 0e9a42340c6344b395890111242895a7
2025-12-12 19:35:21 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 4/5 (20% sample)
2025-12-12 19:35:21 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:35:21 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:35:22 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:35:22 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:35:22 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:35:22 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 14ec4489fe7b4ac39464b0ad1f554809 (experiment: x3d)
2025-12-12 19:35:22 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 4...
2025-12-12 19:35:24 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:35:25 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:35:28 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.55 GiB is free. Including non-PyTorch memory, this process has 3.21 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 101.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:35:28 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5079.79296875, 'cpu_memory_gb': 4.960735321044922, 'cpu_vms_mb': 1399521.80859375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.273312768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.655029248}
2025-12-12 19:35:28 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.55 GiB is free. Including non-PyTorch memory, this process has 3.21 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 101.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:35:28 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:35:29 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:35:31 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.55 GiB is free. Including non-PyTorch memory, this process has 3.21 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 101.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:35:31 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5079.79296875, 'cpu_memory_gb': 4.960735321044922, 'cpu_vms_mb': 1435937.80859375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.273312768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.655029248}
2025-12-12 19:35:31 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.55 GiB is free. Including non-PyTorch memory, this process has 3.21 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 101.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:35:31 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:35:34 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:35:34 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5079.79296875, 'cpu_memory_gb': 4.960735321044922, 'cpu_vms_mb': 1435937.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.749024768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.179317248}
2025-12-12 19:35:34 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:35:34 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:35:37 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.55 GiB is free. Including non-PyTorch memory, this process has 3.21 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 101.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:35:37 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5079.79296875, 'cpu_memory_gb': 4.960735321044922, 'cpu_vms_mb': 1472353.80859375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.273312768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.655029248}
2025-12-12 19:35:37 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.55 GiB is free. Including non-PyTorch memory, this process has 3.21 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 101.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Final batch size: 1
2025-12-12 19:35:37 [ERROR] [lib.training.pipeline:1650] Error training fold 4: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.55 GiB is free. Including non-PyTorch memory, this process has 3.21 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 101.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.55 GiB is free. Including non-PyTorch memory, this process has 3.21 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 101.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:35:37 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 14ec4489fe7b4ac39464b0ad1f554809
2025-12-12 19:35:37 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 5/5 (20% sample)
2025-12-12 19:35:37 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:35:37 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:35:38 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:35:38 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:35:38 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:35:38 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 6913ba0144ae4178b9b31ecceeb40ec4 (experiment: x3d)
2025-12-12 19:35:38 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 5...
2025-12-12 19:35:39 [INFO] [lib.models.video:417] Chunk size increased from 280 to 290 (after 3 successes, increment: 10)
2025-12-12 19:35:40 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:35:40 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:35:45 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.66 GiB is free. Including non-PyTorch memory, this process has 4.11 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 130.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:35:45 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5079.79296875, 'cpu_memory_gb': 4.960735321044922, 'cpu_vms_mb': 1508769.80859375, 'gpu_allocated_gb': 2.828276736, 'gpu_reserved_gb': 2.965372928, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 13.962969088}
2025-12-12 19:35:45 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.66 GiB is free. Including non-PyTorch memory, this process has 4.11 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 130.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:35:45 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:35:49 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:35:50 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5079.79296875, 'cpu_memory_gb': 4.960735321044922, 'cpu_vms_mb': 1508769.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:35:50 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:35:50 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:35:51 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:35:52 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.98 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.50 GiB is free. Including non-PyTorch memory, this process has 3.27 GiB memory in use. Of the allocated memory 2.77 GiB is allocated by PyTorch, and 133.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:35:53 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5079.79296875, 'cpu_memory_gb': 4.960735321044922, 'cpu_vms_mb': 1545185.80859375, 'gpu_allocated_gb': 2.177524736, 'gpu_reserved_gb': 2.315255808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.613086208}
2025-12-12 19:35:53 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 19.98 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.50 GiB is free. Including non-PyTorch memory, this process has 3.27 GiB memory in use. Of the allocated memory 2.77 GiB is allocated by PyTorch, and 133.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:35:53 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:35:57 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.66 GiB is free. Including non-PyTorch memory, this process has 4.11 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 130.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:35:57 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5079.79296875, 'cpu_memory_gb': 4.960735321044922, 'cpu_vms_mb': 1581601.80859375, 'gpu_allocated_gb': 2.828276736, 'gpu_reserved_gb': 2.965372928, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 13.962969088}
2025-12-12 19:35:57 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.66 GiB is free. Including non-PyTorch memory, this process has 4.11 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 130.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Final batch size: 1
2025-12-12 19:35:57 [ERROR] [lib.training.pipeline:1650] Error training fold 5: CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.66 GiB is free. Including non-PyTorch memory, this process has 4.11 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 130.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.66 GiB is free. Including non-PyTorch memory, this process has 4.11 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 130.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:35:57 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 6913ba0144ae4178b9b31ecceeb40ec4
2025-12-12 19:35:57 [INFO] [lib.training.pipeline:818] 
================================================================================
2025-12-12 19:35:57 [INFO] [lib.training.pipeline:819] Grid Search: Hyperparameter combination 8/16
2025-12-12 19:35:57 [INFO] [lib.training.pipeline:820] Parameters: {'learning_rate': 5e-05, 'backbone_lr': 5e-06, 'head_lr': 0.0005, 'weight_decay': 0.001}
2025-12-12 19:35:57 [INFO] [lib.training.pipeline:821] ================================================================================
2025-12-12 19:35:57 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 1/5 (20% sample)
2025-12-12 19:35:57 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:35:57 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:35:57 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:35:57 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:35:58 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:35:58 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: b33823047dbe4ac2baf83b3ce7e6f53b (experiment: x3d)
2025-12-12 19:35:58 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 1...
2025-12-12 19:36:00 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:36:00 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:36:02 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:36:03 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:36:03 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5078.80859375, 'cpu_memory_gb': 4.959774017333984, 'cpu_vms_mb': 1581600.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:36:03 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:36:03 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:36:07 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.64 GiB is free. Including non-PyTorch memory, this process has 4.12 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 150.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:36:08 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5078.80859375, 'cpu_memory_gb': 4.959774017333984, 'cpu_vms_mb': 1618016.80859375, 'gpu_allocated_gb': 2.828276736, 'gpu_reserved_gb': 2.986344448, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 13.941997568}
2025-12-12 19:36:08 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.64 GiB is free. Including non-PyTorch memory, this process has 4.12 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 150.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:36:08 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:36:11 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:36:11 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5078.80859375, 'cpu_memory_gb': 4.959774017333984, 'cpu_vms_mb': 1618016.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:36:11 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:36:11 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:36:12 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:36:13 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:36:14 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5078.80859375, 'cpu_memory_gb': 4.959774017333984, 'cpu_vms_mb': 1618016.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:36:14 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Final batch size: 1
2025-12-12 19:36:14 [ERROR] [lib.training.pipeline:1650] Error training fold 1: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:36:14 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: b33823047dbe4ac2baf83b3ce7e6f53b
2025-12-12 19:36:14 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 2/5 (20% sample)
2025-12-12 19:36:14 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:36:14 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:36:14 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:36:14 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:36:14 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:36:15 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 9e5b8e8c65854a3da9517450cacba836 (experiment: x3d)
2025-12-12 19:36:15 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 2...
2025-12-12 19:36:16 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:36:16 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:36:19 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 21.01 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.38 GiB is free. Including non-PyTorch memory, this process has 3.38 GiB memory in use. Of the allocated memory 2.90 GiB is allocated by PyTorch, and 114.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:36:20 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5078.80859375, 'cpu_memory_gb': 4.959774017333984, 'cpu_vms_mb': 1654432.80859375, 'gpu_allocated_gb': 2.282484736, 'gpu_reserved_gb': 2.399141888, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.529200128}
2025-12-12 19:36:20 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 21.01 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.38 GiB is free. Including non-PyTorch memory, this process has 3.38 GiB memory in use. Of the allocated memory 2.90 GiB is allocated by PyTorch, and 114.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:36:20 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:36:22 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:36:23 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.54 GiB is free. Including non-PyTorch memory, this process has 3.23 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 121.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:36:24 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5078.80859375, 'cpu_memory_gb': 4.959774017333984, 'cpu_vms_mb': 1690848.80859375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.294284288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.634057728}
2025-12-12 19:36:24 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.54 GiB is free. Including non-PyTorch memory, this process has 3.23 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 121.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:36:24 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:36:27 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:36:27 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5078.80859375, 'cpu_memory_gb': 4.959774017333984, 'cpu_vms_mb': 1690848.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:36:27 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:36:27 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:36:30 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:36:31 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5078.80859375, 'cpu_memory_gb': 4.959774017333984, 'cpu_vms_mb': 1690848.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:36:31 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Final batch size: 1
2025-12-12 19:36:31 [ERROR] [lib.training.pipeline:1650] Error training fold 2: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:36:31 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 9e5b8e8c65854a3da9517450cacba836
2025-12-12 19:36:31 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 3/5 (20% sample)
2025-12-12 19:36:31 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:36:31 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:36:31 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:36:31 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:36:31 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:36:32 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 670e99e98a0145489f8c9f5ecb99f546 (experiment: x3d)
2025-12-12 19:36:32 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 3...
2025-12-12 19:36:33 [INFO] [lib.models.video:417] Chunk size increased from 290 to 300 (after 3 successes, increment: 10)
2025-12-12 19:36:33 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:36:34 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:36:37 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:36:37 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5078.80859375, 'cpu_memory_gb': 4.959774017333984, 'cpu_vms_mb': 1690848.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:36:37 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:36:37 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:36:40 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:36:40 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5078.80859375, 'cpu_memory_gb': 4.959774017333984, 'cpu_vms_mb': 1690848.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:36:40 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:36:40 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:36:41 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:36:43 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:36:43 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5078.80859375, 'cpu_memory_gb': 4.959774017333984, 'cpu_vms_mb': 1690848.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:36:43 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:36:43 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:36:46 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:36:46 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5078.80859375, 'cpu_memory_gb': 4.959774017333984, 'cpu_vms_mb': 1690848.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:36:46 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Final batch size: 1
2025-12-12 19:36:46 [ERROR] [lib.training.pipeline:1650] Error training fold 3: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:36:46 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 670e99e98a0145489f8c9f5ecb99f546
2025-12-12 19:36:46 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 4/5 (20% sample)
2025-12-12 19:36:46 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:36:46 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:36:47 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:36:47 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:36:47 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:36:47 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 280adaa4e2da40fa8d99e751e68c793c (experiment: x3d)
2025-12-12 19:36:47 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 4...
2025-12-12 19:36:49 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:36:50 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:36:51 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:36:52 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:36:52 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5078.80859375, 'cpu_memory_gb': 4.959774017333984, 'cpu_vms_mb': 1690848.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:36:52 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:36:52 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:36:56 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:36:56 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5078.80859375, 'cpu_memory_gb': 4.959774017333984, 'cpu_vms_mb': 1690848.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:36:56 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:36:56 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:36:59 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:36:59 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5078.80859375, 'cpu_memory_gb': 4.959774017333984, 'cpu_vms_mb': 1690848.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:36:59 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:36:59 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:37:00 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:37:02 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:37:02 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5078.80859375, 'cpu_memory_gb': 4.959774017333984, 'cpu_vms_mb': 1690848.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:37:02 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Final batch size: 1
2025-12-12 19:37:02 [ERROR] [lib.training.pipeline:1650] Error training fold 4: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:37:02 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 280adaa4e2da40fa8d99e751e68c793c
2025-12-12 19:37:02 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 5/5 (20% sample)
2025-12-12 19:37:02 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:37:02 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:37:02 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:37:02 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:37:03 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:37:03 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: f9d5b5c33974446d97950138aaca9630 (experiment: x3d)
2025-12-12 19:37:03 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 5...
2025-12-12 19:37:05 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:37:05 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:37:10 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.66 GiB is free. Including non-PyTorch memory, this process has 4.11 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 130.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:37:10 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5078.80859375, 'cpu_memory_gb': 4.959774017333984, 'cpu_vms_mb': 1727264.80859375, 'gpu_allocated_gb': 2.828276736, 'gpu_reserved_gb': 2.965372928, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 13.962969088}
2025-12-12 19:37:10 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.66 GiB is free. Including non-PyTorch memory, this process has 4.11 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 130.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:37:10 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:37:11 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:37:13 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.54 GiB is free. Including non-PyTorch memory, this process has 3.23 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 121.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:37:13 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5078.80859375, 'cpu_memory_gb': 4.959774017333984, 'cpu_vms_mb': 1763680.80859375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.294284288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.634057728}
2025-12-12 19:37:13 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.54 GiB is free. Including non-PyTorch memory, this process has 3.23 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 121.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:37:13 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:37:16 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:37:16 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5078.80859375, 'cpu_memory_gb': 4.959774017333984, 'cpu_vms_mb': 1763680.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:37:16 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:37:16 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:37:21 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:37:21 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5078.828125, 'cpu_memory_gb': 4.9597930908203125, 'cpu_vms_mb': 1763680.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:37:21 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Final batch size: 1
2025-12-12 19:37:21 [ERROR] [lib.training.pipeline:1650] Error training fold 5: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:37:21 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: f9d5b5c33974446d97950138aaca9630
2025-12-12 19:37:21 [INFO] [lib.training.pipeline:818] 
================================================================================
2025-12-12 19:37:21 [INFO] [lib.training.pipeline:819] Grid Search: Hyperparameter combination 9/16
2025-12-12 19:37:21 [INFO] [lib.training.pipeline:820] Parameters: {'learning_rate': 0.0001, 'backbone_lr': 1e-06, 'head_lr': 0.0001, 'weight_decay': 0.0001}
2025-12-12 19:37:21 [INFO] [lib.training.pipeline:821] ================================================================================
2025-12-12 19:37:21 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 1/5 (20% sample)
2025-12-12 19:37:21 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:37:21 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:37:22 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:37:22 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:37:22 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:37:22 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 9c676dca0a37471d995c5fd60725e8d9 (experiment: x3d)
2025-12-12 19:37:22 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 1...
2025-12-12 19:37:23 [INFO] [lib.models.video:417] Chunk size increased from 300 to 310 (after 3 successes, increment: 10)
2025-12-12 19:37:24 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:37:25 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:37:28 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.16 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.59 GiB is free. Including non-PyTorch memory, this process has 3.17 GiB memory in use. Of the allocated memory 2.66 GiB is allocated by PyTorch, and 144.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:37:28 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5078.8359375, 'cpu_memory_gb': 4.959800720214844, 'cpu_vms_mb': 1800096.80859375, 'gpu_allocated_gb': 2.093556736, 'gpu_reserved_gb': 2.252341248, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.676000768}
2025-12-12 19:37:28 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 19.16 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.59 GiB is free. Including non-PyTorch memory, this process has 3.17 GiB memory in use. Of the allocated memory 2.66 GiB is allocated by PyTorch, and 144.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:37:28 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:37:31 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:37:31 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5078.8359375, 'cpu_memory_gb': 4.959800720214844, 'cpu_vms_mb': 1800096.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:37:31 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:37:31 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:37:35 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:37:36 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:37:36 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5078.8359375, 'cpu_memory_gb': 4.959800720214844, 'cpu_vms_mb': 1800096.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:37:36 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:37:36 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:37:39 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.98 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.48 GiB is free. Including non-PyTorch memory, this process has 3.29 GiB memory in use. Of the allocated memory 2.77 GiB is allocated by PyTorch, and 153.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:37:39 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5078.8359375, 'cpu_memory_gb': 4.959800720214844, 'cpu_vms_mb': 1836512.80859375, 'gpu_allocated_gb': 2.177524736, 'gpu_reserved_gb': 2.336227328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.592114688}
2025-12-12 19:37:39 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 19.98 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.48 GiB is free. Including non-PyTorch memory, this process has 3.29 GiB memory in use. Of the allocated memory 2.77 GiB is allocated by PyTorch, and 153.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Final batch size: 1
2025-12-12 19:37:39 [ERROR] [lib.training.pipeline:1650] Error training fold 1: CUDA out of memory. Tried to allocate 19.98 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.48 GiB is free. Including non-PyTorch memory, this process has 3.29 GiB memory in use. Of the allocated memory 2.77 GiB is allocated by PyTorch, and 153.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 19.98 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.48 GiB is free. Including non-PyTorch memory, this process has 3.29 GiB memory in use. Of the allocated memory 2.77 GiB is allocated by PyTorch, and 153.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:37:39 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 9c676dca0a37471d995c5fd60725e8d9
2025-12-12 19:37:39 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 2/5 (20% sample)
2025-12-12 19:37:39 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:37:39 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:37:40 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:37:40 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:37:40 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:37:40 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: fe557b67fec64c64afdc040123aa44eb (experiment: x3d)
2025-12-12 19:37:40 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 2...
2025-12-12 19:37:42 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:37:42 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:37:49 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:37:50 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:37:51 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5080.79296875, 'cpu_memory_gb': 4.961711883544922, 'cpu_vms_mb': 1836514.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.749024768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.179317248}
2025-12-12 19:37:51 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:37:51 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:37:54 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.16 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.63 GiB is free. Including non-PyTorch memory, this process has 3.13 GiB memory in use. Of the allocated memory 2.66 GiB is allocated by PyTorch, and 104.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:37:54 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5080.79296875, 'cpu_memory_gb': 4.961711883544922, 'cpu_vms_mb': 1872930.80859375, 'gpu_allocated_gb': 2.093556736, 'gpu_reserved_gb': 2.189426688, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.738915328}
2025-12-12 19:37:54 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 19.16 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.63 GiB is free. Including non-PyTorch memory, this process has 3.13 GiB memory in use. Of the allocated memory 2.66 GiB is allocated by PyTorch, and 104.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:37:54 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:37:57 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.42 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.26 GiB is free. Including non-PyTorch memory, this process has 2.50 GiB memory in use. Of the allocated memory 2.03 GiB is allocated by PyTorch, and 104.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:37:57 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5080.79296875, 'cpu_memory_gb': 4.961711883544922, 'cpu_vms_mb': 1872930.80859375, 'gpu_allocated_gb': 1.610740736, 'gpu_reserved_gb': 1.728053248, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.200288768}
2025-12-12 19:37:57 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.42 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.26 GiB is free. Including non-PyTorch memory, this process has 2.50 GiB memory in use. Of the allocated memory 2.03 GiB is allocated by PyTorch, and 104.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:37:57 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:37:58 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:38:00 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.55 GiB is free. Including non-PyTorch memory, this process has 3.21 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 101.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:38:00 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5080.79296875, 'cpu_memory_gb': 4.961711883544922, 'cpu_vms_mb': 1909346.80859375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.273312768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.655029248}
2025-12-12 19:38:00 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.55 GiB is free. Including non-PyTorch memory, this process has 3.21 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 101.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Final batch size: 1
2025-12-12 19:38:00 [ERROR] [lib.training.pipeline:1650] Error training fold 2: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.55 GiB is free. Including non-PyTorch memory, this process has 3.21 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 101.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.55 GiB is free. Including non-PyTorch memory, this process has 3.21 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 101.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:38:00 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: fe557b67fec64c64afdc040123aa44eb
2025-12-12 19:38:00 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 3/5 (20% sample)
2025-12-12 19:38:00 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:38:00 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:38:01 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:38:01 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:38:01 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:38:01 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 8b5f3c39d0d140dfb244f82d61b4e7c5 (experiment: x3d)
2025-12-12 19:38:01 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 3...
2025-12-12 19:38:03 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:38:03 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:38:06 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:38:06 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5080.79296875, 'cpu_memory_gb': 4.961711883544922, 'cpu_vms_mb': 1909346.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:38:06 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:38:06 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:38:08 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:38:11 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.66 GiB is free. Including non-PyTorch memory, this process has 4.11 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 130.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:38:11 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5080.79296875, 'cpu_memory_gb': 4.961711883544922, 'cpu_vms_mb': 1945762.80859375, 'gpu_allocated_gb': 2.828276736, 'gpu_reserved_gb': 2.965372928, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 13.962969088}
2025-12-12 19:38:11 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.66 GiB is free. Including non-PyTorch memory, this process has 4.11 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 130.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:38:11 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:38:14 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:38:14 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5080.79296875, 'cpu_memory_gb': 4.961711883544922, 'cpu_vms_mb': 1945762.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:38:14 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:38:14 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:38:19 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:38:19 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5080.8046875, 'cpu_memory_gb': 4.961723327636719, 'cpu_vms_mb': 1945762.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:38:19 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Final batch size: 1
2025-12-12 19:38:19 [ERROR] [lib.training.pipeline:1650] Error training fold 3: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:38:19 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 8b5f3c39d0d140dfb244f82d61b4e7c5
2025-12-12 19:38:19 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 4/5 (20% sample)
2025-12-12 19:38:19 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:38:19 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:38:19 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:38:19 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:38:20 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:38:20 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: f17d4516593a4aa9a5172a91e2c0044c (experiment: x3d)
2025-12-12 19:38:20 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 4...
2025-12-12 19:38:21 [INFO] [lib.models.video:417] Chunk size increased from 310 to 320 (after 3 successes, increment: 10)
2025-12-12 19:38:22 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:38:22 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:38:25 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:38:25 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5080.8046875, 'cpu_memory_gb': 4.961723327636719, 'cpu_vms_mb': 1945762.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:38:25 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:38:25 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:38:28 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:38:28 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5080.8046875, 'cpu_memory_gb': 4.961723327636719, 'cpu_vms_mb': 1945762.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:38:28 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:38:28 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:38:29 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:38:31 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.52 GiB is free. Including non-PyTorch memory, this process has 3.25 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 141.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:38:31 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5080.8046875, 'cpu_memory_gb': 4.961723327636719, 'cpu_vms_mb': 1982178.80859375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.315255808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.613086208}
2025-12-12 19:38:31 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.52 GiB is free. Including non-PyTorch memory, this process has 3.25 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 141.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:38:31 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:38:34 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:38:34 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5080.8046875, 'cpu_memory_gb': 4.961723327636719, 'cpu_vms_mb': 1982178.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:38:34 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Final batch size: 1
2025-12-12 19:38:34 [ERROR] [lib.training.pipeline:1650] Error training fold 4: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:38:34 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: f17d4516593a4aa9a5172a91e2c0044c
2025-12-12 19:38:34 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 5/5 (20% sample)
2025-12-12 19:38:34 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:38:34 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:38:34 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:38:34 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:38:35 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:38:35 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: c36e69a2f4a343f0b324791d8426a9b4 (experiment: x3d)
2025-12-12 19:38:35 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 5...
2025-12-12 19:38:37 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:38:37 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:38:38 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:38:40 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:38:40 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5080.8046875, 'cpu_memory_gb': 4.961723327636719, 'cpu_vms_mb': 1982178.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:38:40 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:38:40 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:38:42 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.54 GiB is free. Including non-PyTorch memory, this process has 3.23 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 121.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:38:43 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5080.8046875, 'cpu_memory_gb': 4.961723327636719, 'cpu_vms_mb': 2018594.80859375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.294284288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.634057728}
2025-12-12 19:38:43 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.54 GiB is free. Including non-PyTorch memory, this process has 3.23 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 121.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:38:43 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:38:46 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.54 GiB is free. Including non-PyTorch memory, this process has 3.23 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 121.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:38:47 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5080.8046875, 'cpu_memory_gb': 4.961723327636719, 'cpu_vms_mb': 2055010.80859375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.294284288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.634057728}
2025-12-12 19:38:47 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.54 GiB is free. Including non-PyTorch memory, this process has 3.23 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 121.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:38:47 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:38:48 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:38:49 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:38:50 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5080.8046875, 'cpu_memory_gb': 4.961723327636719, 'cpu_vms_mb': 2055010.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:38:50 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Final batch size: 1
2025-12-12 19:38:50 [ERROR] [lib.training.pipeline:1650] Error training fold 5: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:38:50 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: c36e69a2f4a343f0b324791d8426a9b4
2025-12-12 19:38:50 [INFO] [lib.training.pipeline:818] 
================================================================================
2025-12-12 19:38:50 [INFO] [lib.training.pipeline:819] Grid Search: Hyperparameter combination 10/16
2025-12-12 19:38:50 [INFO] [lib.training.pipeline:820] Parameters: {'learning_rate': 0.0001, 'backbone_lr': 1e-06, 'head_lr': 0.0001, 'weight_decay': 0.001}
2025-12-12 19:38:50 [INFO] [lib.training.pipeline:821] ================================================================================
2025-12-12 19:38:50 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 1/5 (20% sample)
2025-12-12 19:38:50 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:38:50 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:38:50 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:38:50 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:38:50 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:38:50 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: fe2c551932de43dea65c1278609f2735 (experiment: x3d)
2025-12-12 19:38:51 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 1...
2025-12-12 19:38:52 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:38:53 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:38:56 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:38:56 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5080.8046875, 'cpu_memory_gb': 4.961723327636719, 'cpu_vms_mb': 2055010.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:38:56 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:38:56 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:38:58 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:39:00 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.66 GiB is free. Including non-PyTorch memory, this process has 4.11 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 130.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:39:01 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5080.8046875, 'cpu_memory_gb': 4.961723327636719, 'cpu_vms_mb': 2091426.80859375, 'gpu_allocated_gb': 2.828276736, 'gpu_reserved_gb': 2.965372928, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 13.962969088}
2025-12-12 19:39:01 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.66 GiB is free. Including non-PyTorch memory, this process has 4.11 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 130.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:39:01 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:39:03 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:39:03 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5080.8046875, 'cpu_memory_gb': 4.961723327636719, 'cpu_vms_mb': 2091426.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:39:03 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:39:03 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:39:06 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:39:07 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5080.8046875, 'cpu_memory_gb': 4.961723327636719, 'cpu_vms_mb': 2091426.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:39:07 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Final batch size: 1
2025-12-12 19:39:07 [ERROR] [lib.training.pipeline:1650] Error training fold 1: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:39:07 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: fe2c551932de43dea65c1278609f2735
2025-12-12 19:39:07 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 2/5 (20% sample)
2025-12-12 19:39:07 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:39:07 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:39:07 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:39:07 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:39:07 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:39:07 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 8a54e8dc7fd34ba6b118df7cbcf43122 (experiment: x3d)
2025-12-12 19:39:07 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 2...
2025-12-12 19:39:08 [INFO] [lib.models.video:417] Chunk size increased from 230 to 240 (after 3 successes, increment: 10)
2025-12-12 19:39:09 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:39:09 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:39:12 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 116.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:39:12 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5080.8046875, 'cpu_memory_gb': 4.961723327636719, 'cpu_vms_mb': 2091426.80859375, 'gpu_allocated_gb': 1.631732736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:39:12 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 116.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:39:12 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:39:16 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.66 GiB is free. Including non-PyTorch memory, this process has 4.11 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 130.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:39:16 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5080.8046875, 'cpu_memory_gb': 4.961723327636719, 'cpu_vms_mb': 2127842.80859375, 'gpu_allocated_gb': 2.828276736, 'gpu_reserved_gb': 2.965372928, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 13.962969088}
2025-12-12 19:39:16 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.66 GiB is free. Including non-PyTorch memory, this process has 4.11 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 130.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:39:16 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:39:18 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:39:19 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 13.18 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.42 GiB is free. Including non-PyTorch memory, this process has 3.34 GiB memory in use. Of the allocated memory 2.85 GiB is allocated by PyTorch, and 131.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:39:20 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5080.8671875, 'cpu_memory_gb': 4.961784362792969, 'cpu_vms_mb': 2127842.80859375, 'gpu_allocated_gb': 2.533586944, 'gpu_reserved_gb': 2.671771648, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.256570368}
2025-12-12 19:39:20 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 13.18 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.42 GiB is free. Including non-PyTorch memory, this process has 3.34 GiB memory in use. Of the allocated memory 2.85 GiB is allocated by PyTorch, and 131.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:39:20 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:39:22 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:39:23 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5080.8671875, 'cpu_memory_gb': 4.961784362792969, 'cpu_vms_mb': 2127842.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:39:23 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Final batch size: 1
2025-12-12 19:39:23 [ERROR] [lib.training.pipeline:1650] Error training fold 2: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:39:23 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 8a54e8dc7fd34ba6b118df7cbcf43122
2025-12-12 19:39:23 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 3/5 (20% sample)
2025-12-12 19:39:23 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:39:23 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:39:23 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:39:23 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:39:24 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:39:24 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 5a324649dd3348d58f1ddfae9993d953 (experiment: x3d)
2025-12-12 19:39:24 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 3...
2025-12-12 19:39:25 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:39:26 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:39:28 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:39:29 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 21.01 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.38 GiB is free. Including non-PyTorch memory, this process has 3.38 GiB memory in use. Of the allocated memory 2.90 GiB is allocated by PyTorch, and 114.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:39:30 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5080.8671875, 'cpu_memory_gb': 4.961784362792969, 'cpu_vms_mb': 2164258.80859375, 'gpu_allocated_gb': 2.282484736, 'gpu_reserved_gb': 2.399141888, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.529200128}
2025-12-12 19:39:30 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 21.01 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.38 GiB is free. Including non-PyTorch memory, this process has 3.38 GiB memory in use. Of the allocated memory 2.90 GiB is allocated by PyTorch, and 114.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:39:30 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:39:32 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:39:32 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5080.8671875, 'cpu_memory_gb': 4.961784362792969, 'cpu_vms_mb': 2164258.80859375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:39:32 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:39:32 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:39:36 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 21.22 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.34 GiB is free. Including non-PyTorch memory, this process has 3.42 GiB memory in use. Of the allocated memory 2.93 GiB is allocated by PyTorch, and 126.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:39:36 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5080.8671875, 'cpu_memory_gb': 4.961784362792969, 'cpu_vms_mb': 2200674.80859375, 'gpu_allocated_gb': 2.303476736, 'gpu_reserved_gb': 2.420113408, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.508228608}
2025-12-12 19:39:36 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 21.22 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.34 GiB is free. Including non-PyTorch memory, this process has 3.42 GiB memory in use. Of the allocated memory 2.93 GiB is allocated by PyTorch, and 126.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:39:36 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:39:49 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:39:50 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 116.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:39:50 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 7570.328125, 'cpu_memory_gb': 7.3928985595703125, 'cpu_vms_mb': 2203164.75390625, 'gpu_allocated_gb': 1.631732736, 'gpu_reserved_gb': 1.749024768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.179317248}
2025-12-12 19:39:50 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 116.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Final batch size: 1
2025-12-12 19:39:50 [ERROR] [lib.training.pipeline:1650] Error training fold 3: CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 116.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 116.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:39:50 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 5a324649dd3348d58f1ddfae9993d953
2025-12-12 19:39:51 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 4/5 (20% sample)
2025-12-12 19:39:51 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:39:51 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:39:51 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:39:51 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:39:51 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:39:51 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 6b43a32f13994195810a1eba82c6aaab (experiment: x3d)
2025-12-12 19:39:51 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 4...
2025-12-12 19:39:53 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:39:54 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:39:56 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:39:57 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 7570.328125, 'cpu_memory_gb': 7.3928985595703125, 'cpu_vms_mb': 2203164.75390625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:39:57 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:39:57 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:39:58 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:39:59 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:39:59 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 7570.328125, 'cpu_memory_gb': 7.3928985595703125, 'cpu_vms_mb': 2203164.75390625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:39:59 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:39:59 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:40:02 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:40:02 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 7570.328125, 'cpu_memory_gb': 7.3928985595703125, 'cpu_vms_mb': 2203164.75390625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:40:02 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:40:02 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:40:05 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:40:05 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 7570.328125, 'cpu_memory_gb': 7.3928985595703125, 'cpu_vms_mb': 2203164.75390625, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:40:05 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Final batch size: 1
2025-12-12 19:40:05 [ERROR] [lib.training.pipeline:1650] Error training fold 4: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:40:05 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 6b43a32f13994195810a1eba82c6aaab
2025-12-12 19:40:05 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 5/5 (20% sample)
2025-12-12 19:40:05 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:40:05 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:40:05 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:40:05 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:40:06 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:40:06 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: ac6c02cfd2004c978528bfacdc1d05ae (experiment: x3d)
2025-12-12 19:40:06 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 5...
2025-12-12 19:40:07 [INFO] [lib.models.video:417] Chunk size increased from 320 to 330 (after 3 successes, increment: 10)
2025-12-12 19:40:08 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:40:08 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:40:12 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:40:12 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.26953125, 'cpu_memory_gb': 4.962177276611328, 'cpu_vms_mb': 2200675.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:40:12 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:40:12 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:40:15 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:40:15 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.26953125, 'cpu_memory_gb': 4.962177276611328, 'cpu_vms_mb': 2200675.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:40:15 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:40:15 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:40:17 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:40:19 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 25.34 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.77 GiB is free. Including non-PyTorch memory, this process has 3.99 GiB memory in use. Of the allocated memory 3.47 GiB is allocated by PyTorch, and 149.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:40:20 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.26953125, 'cpu_memory_gb': 4.962177276611328, 'cpu_vms_mb': 2237091.5234375, 'gpu_allocated_gb': 2.723316736, 'gpu_reserved_gb': 2.881486848, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.046855168}
2025-12-12 19:40:20 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 25.34 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.77 GiB is free. Including non-PyTorch memory, this process has 3.99 GiB memory in use. Of the allocated memory 3.47 GiB is allocated by PyTorch, and 149.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:40:20 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:40:25 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:40:26 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.27734375, 'cpu_memory_gb': 4.962184906005859, 'cpu_vms_mb': 2237091.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:40:26 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Final batch size: 1
2025-12-12 19:40:26 [ERROR] [lib.training.pipeline:1650] Error training fold 5: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:40:26 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: ac6c02cfd2004c978528bfacdc1d05ae
2025-12-12 19:40:26 [INFO] [lib.training.pipeline:818] 
================================================================================
2025-12-12 19:40:26 [INFO] [lib.training.pipeline:819] Grid Search: Hyperparameter combination 11/16
2025-12-12 19:40:26 [INFO] [lib.training.pipeline:820] Parameters: {'learning_rate': 0.0001, 'backbone_lr': 1e-06, 'head_lr': 0.0005, 'weight_decay': 0.0001}
2025-12-12 19:40:26 [INFO] [lib.training.pipeline:821] ================================================================================
2025-12-12 19:40:26 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 1/5 (20% sample)
2025-12-12 19:40:26 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:40:26 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:40:27 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:40:27 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:40:27 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:40:27 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: b12560d754ad4d32a66c2b2003544866 (experiment: x3d)
2025-12-12 19:40:27 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 1...
2025-12-12 19:40:29 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:40:30 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:40:37 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:40:38 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:40:38 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.27734375, 'cpu_memory_gb': 4.962184906005859, 'cpu_vms_mb': 2237091.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:40:38 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:40:38 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:40:41 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.52 GiB is free. Including non-PyTorch memory, this process has 3.25 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 141.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:40:41 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.27734375, 'cpu_memory_gb': 4.962184906005859, 'cpu_vms_mb': 2273507.5234375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.315255808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.613086208}
2025-12-12 19:40:41 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.52 GiB is free. Including non-PyTorch memory, this process has 3.25 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 141.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:40:41 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:40:44 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.42 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.03 GiB is allocated by PyTorch, and 144.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:40:44 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.27734375, 'cpu_memory_gb': 4.962184906005859, 'cpu_vms_mb': 2273507.5234375, 'gpu_allocated_gb': 1.610740736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:40:44 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.42 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.03 GiB is allocated by PyTorch, and 144.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:40:44 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:40:45 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:40:47 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 21.01 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.34 GiB is free. Including non-PyTorch memory, this process has 3.42 GiB memory in use. Of the allocated memory 2.90 GiB is allocated by PyTorch, and 154.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:40:47 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.27734375, 'cpu_memory_gb': 4.962184906005859, 'cpu_vms_mb': 2309923.5234375, 'gpu_allocated_gb': 2.282484736, 'gpu_reserved_gb': 2.441084928, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.487257088}
2025-12-12 19:40:47 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 21.01 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.34 GiB is free. Including non-PyTorch memory, this process has 3.42 GiB memory in use. Of the allocated memory 2.90 GiB is allocated by PyTorch, and 154.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Final batch size: 1
2025-12-12 19:40:47 [ERROR] [lib.training.pipeline:1650] Error training fold 1: CUDA out of memory. Tried to allocate 21.01 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.34 GiB is free. Including non-PyTorch memory, this process has 3.42 GiB memory in use. Of the allocated memory 2.90 GiB is allocated by PyTorch, and 154.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 21.01 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.34 GiB is free. Including non-PyTorch memory, this process has 3.42 GiB memory in use. Of the allocated memory 2.90 GiB is allocated by PyTorch, and 154.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:40:47 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: b12560d754ad4d32a66c2b2003544866
2025-12-12 19:40:48 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 2/5 (20% sample)
2025-12-12 19:40:48 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:40:48 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:40:48 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:40:48 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:40:48 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:40:49 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 41441d20501c4168beff7afac81e42ec (experiment: x3d)
2025-12-12 19:40:49 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 2...
2025-12-12 19:40:50 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:40:51 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:40:55 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:40:55 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.27734375, 'cpu_memory_gb': 4.962184906005859, 'cpu_vms_mb': 2309923.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:40:55 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:40:55 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:40:56 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:40:58 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:40:58 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.27734375, 'cpu_memory_gb': 4.962184906005859, 'cpu_vms_mb': 2309923.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:40:58 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:40:58 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:41:01 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.50 GiB is free. Including non-PyTorch memory, this process has 3.27 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 161.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:41:01 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.27734375, 'cpu_memory_gb': 4.962184906005859, 'cpu_vms_mb': 2346339.5234375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.336227328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.592114688}
2025-12-12 19:41:01 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.50 GiB is free. Including non-PyTorch memory, this process has 3.27 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 161.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:41:01 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:41:05 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:41:05 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.27734375, 'cpu_memory_gb': 4.962184906005859, 'cpu_vms_mb': 2346339.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:41:05 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Final batch size: 1
2025-12-12 19:41:05 [ERROR] [lib.training.pipeline:1650] Error training fold 2: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:41:05 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 41441d20501c4168beff7afac81e42ec
2025-12-12 19:41:06 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 3/5 (20% sample)
2025-12-12 19:41:06 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:41:06 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:41:06 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:41:06 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:41:06 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:41:06 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: f81420ccf55f4bf2857c08c99e795ad3 (experiment: x3d)
2025-12-12 19:41:06 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 3...
2025-12-12 19:41:08 [INFO] [lib.models.video:417] Chunk size increased from 330 to 340 (after 3 successes, increment: 10)
2025-12-12 19:41:08 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:41:09 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:41:14 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:41:14 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.27734375, 'cpu_memory_gb': 4.962184906005859, 'cpu_vms_mb': 2346339.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:41:14 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:41:14 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:41:18 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.52 GiB is free. Including non-PyTorch memory, this process has 3.25 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 141.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:41:19 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.27734375, 'cpu_memory_gb': 4.962184906005859, 'cpu_vms_mb': 2382755.5234375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.315255808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.613086208}
2025-12-12 19:41:19 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.52 GiB is free. Including non-PyTorch memory, this process has 3.25 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 141.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:41:19 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:41:23 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:41:24 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:41:24 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.27734375, 'cpu_memory_gb': 4.962184906005859, 'cpu_vms_mb': 2382755.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:41:24 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:41:24 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:41:26 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.21 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.24 GiB is free. Including non-PyTorch memory, this process has 2.52 GiB memory in use. Of the allocated memory 2.01 GiB is allocated by PyTorch, and 152.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:41:27 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.27734375, 'cpu_memory_gb': 4.962184906005859, 'cpu_vms_mb': 2382755.5234375, 'gpu_allocated_gb': 1.589748736, 'gpu_reserved_gb': 1.749024768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.179317248}
2025-12-12 19:41:27 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.21 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.24 GiB is free. Including non-PyTorch memory, this process has 2.52 GiB memory in use. Of the allocated memory 2.01 GiB is allocated by PyTorch, and 152.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Final batch size: 1
2025-12-12 19:41:27 [ERROR] [lib.training.pipeline:1650] Error training fold 3: CUDA out of memory. Tried to allocate 14.21 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.24 GiB is free. Including non-PyTorch memory, this process has 2.52 GiB memory in use. Of the allocated memory 2.01 GiB is allocated by PyTorch, and 152.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.21 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.24 GiB is free. Including non-PyTorch memory, this process has 2.52 GiB memory in use. Of the allocated memory 2.01 GiB is allocated by PyTorch, and 152.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:41:27 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: f81420ccf55f4bf2857c08c99e795ad3
2025-12-12 19:41:27 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 4/5 (20% sample)
2025-12-12 19:41:27 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:41:27 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:41:27 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:41:27 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:41:28 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:41:28 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: d02e70e0647747cba9bd8375b6b36d6c (experiment: x3d)
2025-12-12 19:41:28 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 4...
2025-12-12 19:41:29 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:41:30 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:41:33 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:41:34 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:41:34 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.27734375, 'cpu_memory_gb': 4.962184906005859, 'cpu_vms_mb': 2382755.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:41:34 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:41:34 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:41:39 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:41:39 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.27734375, 'cpu_memory_gb': 4.962184906005859, 'cpu_vms_mb': 2382755.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:41:39 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:41:39 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:41:45 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 156.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:41:45 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.27734375, 'cpu_memory_gb': 4.962184906005859, 'cpu_vms_mb': 2382755.5234375, 'gpu_allocated_gb': 1.631732736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:41:45 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 156.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:41:45 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:41:47 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:41:48 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:41:48 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.27734375, 'cpu_memory_gb': 4.962184906005859, 'cpu_vms_mb': 2382755.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:41:48 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Final batch size: 1
2025-12-12 19:41:48 [ERROR] [lib.training.pipeline:1650] Error training fold 4: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:41:48 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: d02e70e0647747cba9bd8375b6b36d6c
2025-12-12 19:41:48 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 5/5 (20% sample)
2025-12-12 19:41:48 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:41:48 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:41:49 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:41:49 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:41:49 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:41:49 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 735f0d5dee184828b304c8999eff5d9d (experiment: x3d)
2025-12-12 19:41:49 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 5...
2025-12-12 19:41:51 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:41:52 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:41:55 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:41:55 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.27734375, 'cpu_memory_gb': 4.962184906005859, 'cpu_vms_mb': 2382755.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:41:55 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:41:55 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:41:56 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:41:58 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:41:58 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.28515625, 'cpu_memory_gb': 4.962192535400391, 'cpu_vms_mb': 2382755.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:41:58 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:41:58 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:42:02 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.62 GiB is free. Including non-PyTorch memory, this process has 4.14 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 170.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:42:02 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.28515625, 'cpu_memory_gb': 4.962192535400391, 'cpu_vms_mb': 2419171.5234375, 'gpu_allocated_gb': 2.828276736, 'gpu_reserved_gb': 3.007315968, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 13.921026048}
2025-12-12 19:42:02 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.62 GiB is free. Including non-PyTorch memory, this process has 4.14 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 170.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:42:02 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:42:05 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.98 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.46 GiB is free. Including non-PyTorch memory, this process has 3.30 GiB memory in use. Of the allocated memory 2.77 GiB is allocated by PyTorch, and 173.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:42:05 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.28515625, 'cpu_memory_gb': 4.962192535400391, 'cpu_vms_mb': 2455587.5234375, 'gpu_allocated_gb': 2.177524736, 'gpu_reserved_gb': 2.357198848, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.571143168}
2025-12-12 19:42:05 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 19.98 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.46 GiB is free. Including non-PyTorch memory, this process has 3.30 GiB memory in use. Of the allocated memory 2.77 GiB is allocated by PyTorch, and 173.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Final batch size: 1
2025-12-12 19:42:05 [ERROR] [lib.training.pipeline:1650] Error training fold 5: CUDA out of memory. Tried to allocate 19.98 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.46 GiB is free. Including non-PyTorch memory, this process has 3.30 GiB memory in use. Of the allocated memory 2.77 GiB is allocated by PyTorch, and 173.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 19.98 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.46 GiB is free. Including non-PyTorch memory, this process has 3.30 GiB memory in use. Of the allocated memory 2.77 GiB is allocated by PyTorch, and 173.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:42:05 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 735f0d5dee184828b304c8999eff5d9d
2025-12-12 19:42:06 [INFO] [lib.training.pipeline:818] 
================================================================================
2025-12-12 19:42:06 [INFO] [lib.training.pipeline:819] Grid Search: Hyperparameter combination 12/16
2025-12-12 19:42:06 [INFO] [lib.training.pipeline:820] Parameters: {'learning_rate': 0.0001, 'backbone_lr': 1e-06, 'head_lr': 0.0005, 'weight_decay': 0.001}
2025-12-12 19:42:06 [INFO] [lib.training.pipeline:821] ================================================================================
2025-12-12 19:42:06 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 1/5 (20% sample)
2025-12-12 19:42:06 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:42:06 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:42:06 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:42:06 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:42:06 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:42:06 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: bd5e83e5c4fb4c54aac9202494cdd186 (experiment: x3d)
2025-12-12 19:42:06 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 1...
2025-12-12 19:42:08 [INFO] [lib.models.video:417] Chunk size increased from 340 to 350 (after 3 successes, increment: 10)
2025-12-12 19:42:08 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:42:09 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:42:11 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:42:12 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.2421875, 'cpu_memory_gb': 4.962150573730469, 'cpu_vms_mb': 2455587.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:42:12 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:42:12 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:42:14 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.52 GiB is free. Including non-PyTorch memory, this process has 3.25 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 141.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:42:14 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2492003.5234375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.315255808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.613086208}
2025-12-12 19:42:14 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.52 GiB is free. Including non-PyTorch memory, this process has 3.25 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 141.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:42:14 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:42:15 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:42:17 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 136.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:42:17 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2492003.5234375, 'gpu_allocated_gb': 1.631732736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:42:17 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 136.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:42:17 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:42:22 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.64 GiB is free. Including non-PyTorch memory, this process has 4.12 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 150.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:42:22 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2528419.5234375, 'gpu_allocated_gb': 2.828276736, 'gpu_reserved_gb': 2.986344448, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 13.941997568}
2025-12-12 19:42:22 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.64 GiB is free. Including non-PyTorch memory, this process has 4.12 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 150.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Final batch size: 1
2025-12-12 19:42:22 [ERROR] [lib.training.pipeline:1650] Error training fold 1: CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.64 GiB is free. Including non-PyTorch memory, this process has 4.12 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 150.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.64 GiB is free. Including non-PyTorch memory, this process has 4.12 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 150.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:42:22 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: bd5e83e5c4fb4c54aac9202494cdd186
2025-12-12 19:42:22 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 2/5 (20% sample)
2025-12-12 19:42:22 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:42:22 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:42:22 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:42:22 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:42:23 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:42:23 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 32f4e2aa09e6470ebd2f283bb580a98d (experiment: x3d)
2025-12-12 19:42:23 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 2...
2025-12-12 19:42:24 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:42:25 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:42:26 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:42:28 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 21.42 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.26 GiB is free. Including non-PyTorch memory, this process has 3.50 GiB memory in use. Of the allocated memory 2.96 GiB is allocated by PyTorch, and 178.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:42:28 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2564835.5234375, 'gpu_allocated_gb': 2.324468736, 'gpu_reserved_gb': 2.524971008, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.403371008}
2025-12-12 19:42:28 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 21.42 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.26 GiB is free. Including non-PyTorch memory, this process has 3.50 GiB memory in use. Of the allocated memory 2.96 GiB is allocated by PyTorch, and 178.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:42:28 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:42:31 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 23.07 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.07 GiB is free. Including non-PyTorch memory, this process has 3.70 GiB memory in use. Of the allocated memory 3.18 GiB is allocated by PyTorch, and 155.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:42:31 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2601251.5234375, 'gpu_allocated_gb': 2.492404736, 'gpu_reserved_gb': 2.650800128, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.277541888}
2025-12-12 19:42:31 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 23.07 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.07 GiB is free. Including non-PyTorch memory, this process has 3.70 GiB memory in use. Of the allocated memory 3.18 GiB is allocated by PyTorch, and 155.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:42:31 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:42:34 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:42:34 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2601251.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.832910848, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.095431168}
2025-12-12 19:42:34 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:42:34 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:42:37 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:42:38 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 17.51 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.81 GiB is free. Including non-PyTorch memory, this process has 2.95 GiB memory in use. Of the allocated memory 2.44 GiB is allocated by PyTorch, and 147.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:42:38 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2601251.5234375, 'gpu_allocated_gb': 1.925620736, 'gpu_reserved_gb': 2.084569088, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.843772928}
2025-12-12 19:42:38 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 17.51 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.81 GiB is free. Including non-PyTorch memory, this process has 2.95 GiB memory in use. Of the allocated memory 2.44 GiB is allocated by PyTorch, and 147.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Final batch size: 1
2025-12-12 19:42:38 [ERROR] [lib.training.pipeline:1650] Error training fold 2: CUDA out of memory. Tried to allocate 17.51 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.81 GiB is free. Including non-PyTorch memory, this process has 2.95 GiB memory in use. Of the allocated memory 2.44 GiB is allocated by PyTorch, and 147.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 17.51 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.81 GiB is free. Including non-PyTorch memory, this process has 2.95 GiB memory in use. Of the allocated memory 2.44 GiB is allocated by PyTorch, and 147.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:42:38 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 32f4e2aa09e6470ebd2f283bb580a98d
2025-12-12 19:42:39 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 3/5 (20% sample)
2025-12-12 19:42:39 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:42:39 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:42:39 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:42:39 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:42:40 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:42:40 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: b04918d64fbf4fde979a54cb4a23c144 (experiment: x3d)
2025-12-12 19:42:40 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 3...
2025-12-12 19:42:41 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:42:42 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:42:45 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:42:46 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2601251.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.749024768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.179317248}
2025-12-12 19:42:46 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:42:46 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:42:48 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:42:50 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.55 GiB is free. Including non-PyTorch memory, this process has 3.21 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 101.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:42:50 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2637667.5234375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.273312768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.655029248}
2025-12-12 19:42:50 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.55 GiB is free. Including non-PyTorch memory, this process has 3.21 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 101.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:42:50 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:42:52 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:42:53 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2637667.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.749024768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.179317248}
2025-12-12 19:42:53 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:42:53 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:42:56 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.55 GiB is free. Including non-PyTorch memory, this process has 3.21 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 101.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:42:56 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2674083.5234375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.273312768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.655029248}
2025-12-12 19:42:56 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.55 GiB is free. Including non-PyTorch memory, this process has 3.21 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 101.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Final batch size: 1
2025-12-12 19:42:56 [ERROR] [lib.training.pipeline:1650] Error training fold 3: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.55 GiB is free. Including non-PyTorch memory, this process has 3.21 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 101.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.55 GiB is free. Including non-PyTorch memory, this process has 3.21 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 101.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:42:56 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: b04918d64fbf4fde979a54cb4a23c144
2025-12-12 19:42:56 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 4/5 (20% sample)
2025-12-12 19:42:56 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:42:56 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:42:56 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:42:56 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:42:57 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:42:57 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: c522ddced44949e6b2467fa52d12ba15 (experiment: x3d)
2025-12-12 19:42:57 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 4...
2025-12-12 19:42:58 [INFO] [lib.models.video:417] Chunk size increased from 350 to 360 (after 3 successes, increment: 10)
2025-12-12 19:42:59 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:42:59 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:43:02 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:43:02 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2674083.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:43:02 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:43:02 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:43:05 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 21.42 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.28 GiB is free. Including non-PyTorch memory, this process has 3.48 GiB memory in use. Of the allocated memory 2.96 GiB is allocated by PyTorch, and 158.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:43:05 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2710499.5234375, 'gpu_allocated_gb': 2.324468736, 'gpu_reserved_gb': 2.503999488, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.424342528}
2025-12-12 19:43:05 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 21.42 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.28 GiB is free. Including non-PyTorch memory, this process has 3.48 GiB memory in use. Of the allocated memory 2.96 GiB is allocated by PyTorch, and 158.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:43:05 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:43:06 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:43:08 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 21.22 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.32 GiB is free. Including non-PyTorch memory, this process has 3.44 GiB memory in use. Of the allocated memory 2.93 GiB is allocated by PyTorch, and 146.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:43:08 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2746915.5234375, 'gpu_allocated_gb': 2.303476736, 'gpu_reserved_gb': 2.441084928, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.487257088}
2025-12-12 19:43:08 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 21.22 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.32 GiB is free. Including non-PyTorch memory, this process has 3.44 GiB memory in use. Of the allocated memory 2.93 GiB is allocated by PyTorch, and 146.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:43:08 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:43:11 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 136.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:43:11 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2746915.5234375, 'gpu_allocated_gb': 1.631732736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:43:11 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 136.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Final batch size: 1
2025-12-12 19:43:11 [ERROR] [lib.training.pipeline:1650] Error training fold 4: CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 136.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 136.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:43:11 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: c522ddced44949e6b2467fa52d12ba15
2025-12-12 19:43:11 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 5/5 (20% sample)
2025-12-12 19:43:11 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:43:11 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:43:12 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:43:12 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:43:12 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:43:12 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 0dff6daa4a814bec871880dffc83c2cb (experiment: x3d)
2025-12-12 19:43:12 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 5...
2025-12-12 19:43:14 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:43:15 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:43:16 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:43:17 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.42 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.03 GiB is allocated by PyTorch, and 144.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:43:17 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2746915.5234375, 'gpu_allocated_gb': 1.610740736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:43:17 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.42 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.03 GiB is allocated by PyTorch, and 144.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:43:17 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:43:19 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:43:20 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2746915.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:43:20 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:43:20 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:43:22 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:43:22 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2746915.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:43:22 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:43:22 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:43:24 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:43:26 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.52 GiB is free. Including non-PyTorch memory, this process has 3.25 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 141.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:43:26 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2783331.5234375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.315255808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.613086208}
2025-12-12 19:43:26 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.52 GiB is free. Including non-PyTorch memory, this process has 3.25 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 141.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Final batch size: 1
2025-12-12 19:43:26 [ERROR] [lib.training.pipeline:1650] Error training fold 5: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.52 GiB is free. Including non-PyTorch memory, this process has 3.25 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 141.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.52 GiB is free. Including non-PyTorch memory, this process has 3.25 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 141.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:43:26 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 0dff6daa4a814bec871880dffc83c2cb
2025-12-12 19:43:26 [INFO] [lib.training.pipeline:818] 
================================================================================
2025-12-12 19:43:26 [INFO] [lib.training.pipeline:819] Grid Search: Hyperparameter combination 13/16
2025-12-12 19:43:26 [INFO] [lib.training.pipeline:820] Parameters: {'learning_rate': 0.0001, 'backbone_lr': 5e-06, 'head_lr': 0.0001, 'weight_decay': 0.0001}
2025-12-12 19:43:26 [INFO] [lib.training.pipeline:821] ================================================================================
2025-12-12 19:43:26 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 1/5 (20% sample)
2025-12-12 19:43:26 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:43:26 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:43:26 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:43:26 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:43:27 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:43:27 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 4d76ea8362f94e12b39b8d6e93172895 (experiment: x3d)
2025-12-12 19:43:27 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 1...
2025-12-12 19:43:29 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:43:29 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:43:32 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:43:32 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2783331.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:43:32 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:43:32 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:43:33 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:43:35 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:43:35 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2783331.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:43:35 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:43:35 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:43:42 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.52 GiB is free. Including non-PyTorch memory, this process has 3.25 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 141.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:43:42 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2819747.5234375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.315255808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.613086208}
2025-12-12 19:43:42 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.52 GiB is free. Including non-PyTorch memory, this process has 3.25 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 141.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:43:42 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:43:45 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:43:45 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2819747.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:43:45 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Final batch size: 1
2025-12-12 19:43:45 [ERROR] [lib.training.pipeline:1650] Error training fold 1: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:43:45 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 4d76ea8362f94e12b39b8d6e93172895
2025-12-12 19:43:45 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 2/5 (20% sample)
2025-12-12 19:43:45 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:43:45 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:43:46 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:43:46 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:43:46 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:43:46 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: c7b655c271914440a34578e1583e299f (experiment: x3d)
2025-12-12 19:43:46 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 2...
2025-12-12 19:43:47 [INFO] [lib.models.video:417] Chunk size increased from 240 to 250 (after 3 successes, increment: 10)
2025-12-12 19:43:48 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:43:48 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:43:51 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:43:51 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2819747.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:43:51 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:43:51 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:43:54 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:43:54 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2819747.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:43:54 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:43:54 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:43:56 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:43:58 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.54 GiB is free. Including non-PyTorch memory, this process has 3.23 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 121.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:43:58 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2856163.5234375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.294284288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.634057728}
2025-12-12 19:43:58 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.54 GiB is free. Including non-PyTorch memory, this process has 3.23 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 121.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:43:58 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:44:01 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:44:01 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2856163.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:44:01 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Final batch size: 1
2025-12-12 19:44:01 [ERROR] [lib.training.pipeline:1650] Error training fold 2: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    f"Retrying with batch_size={batch_size}, "
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:44:01 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: c7b655c271914440a34578e1583e299f
2025-12-12 19:44:02 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 3/5 (20% sample)
2025-12-12 19:44:02 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:44:02 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:44:02 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:44:02 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:44:02 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:44:02 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 64062353d9de4344a3b7913e5ddbe68c (experiment: x3d)
2025-12-12 19:44:02 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 3...
2025-12-12 19:44:04 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:44:05 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:44:06 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:44:08 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 21.01 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.34 GiB is free. Including non-PyTorch memory, this process has 3.42 GiB memory in use. Of the allocated memory 2.90 GiB is allocated by PyTorch, and 154.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:44:08 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2892579.5234375, 'gpu_allocated_gb': 2.282484736, 'gpu_reserved_gb': 2.441084928, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.487257088}
2025-12-12 19:44:08 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 21.01 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.34 GiB is free. Including non-PyTorch memory, this process has 3.42 GiB memory in use. Of the allocated memory 2.90 GiB is allocated by PyTorch, and 154.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:44:08 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:44:16 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:44:16 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2892579.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:44:16 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:44:16 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:44:18 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 16.48 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.95 GiB is free. Including non-PyTorch memory, this process has 2.82 GiB memory in use. Of the allocated memory 2.31 GiB is allocated by PyTorch, and 146.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:44:19 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2892579.5234375, 'gpu_allocated_gb': 1.820660736, 'gpu_reserved_gb': 1.958739968, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.969602048}
2025-12-12 19:44:19 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 16.48 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.95 GiB is free. Including non-PyTorch memory, this process has 2.82 GiB memory in use. Of the allocated memory 2.31 GiB is allocated by PyTorch, and 146.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:44:19 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:44:20 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:44:22 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 21.01 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.34 GiB is free. Including non-PyTorch memory, this process has 3.42 GiB memory in use. Of the allocated memory 2.90 GiB is allocated by PyTorch, and 154.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:44:22 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2928995.5234375, 'gpu_allocated_gb': 2.282484736, 'gpu_reserved_gb': 2.441084928, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.487257088}
2025-12-12 19:44:22 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 21.01 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.34 GiB is free. Including non-PyTorch memory, this process has 3.42 GiB memory in use. Of the allocated memory 2.90 GiB is allocated by PyTorch, and 154.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Final batch size: 1
2025-12-12 19:44:22 [ERROR] [lib.training.pipeline:1650] Error training fold 3: CUDA out of memory. Tried to allocate 21.01 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.34 GiB is free. Including non-PyTorch memory, this process has 3.42 GiB memory in use. Of the allocated memory 2.90 GiB is allocated by PyTorch, and 154.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    f"Retrying with batch_size={batch_size}, "
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 21.01 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.34 GiB is free. Including non-PyTorch memory, this process has 3.42 GiB memory in use. Of the allocated memory 2.90 GiB is allocated by PyTorch, and 154.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:44:22 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 64062353d9de4344a3b7913e5ddbe68c
2025-12-12 19:44:23 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 4/5 (20% sample)
2025-12-12 19:44:23 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:44:23 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:44:23 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:44:23 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:44:23 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:44:24 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 38ec8a14717943898ac2d4547d425274 (experiment: x3d)
2025-12-12 19:44:24 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 4...
2025-12-12 19:44:25 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:44:26 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:44:29 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:44:29 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5081.24609375, 'cpu_memory_gb': 4.962154388427734, 'cpu_vms_mb': 2928995.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.749024768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.179317248}
2025-12-12 19:44:29 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:44:29 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:44:35 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:44:36 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:44:37 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.38671875, 'cpu_memory_gb': 4.963268280029297, 'cpu_vms_mb': 2928996.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.749024768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.179317248}
2025-12-12 19:44:37 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:44:37 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:44:39 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:44:39 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.38671875, 'cpu_memory_gb': 4.963268280029297, 'cpu_vms_mb': 2928996.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.749024768, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.179317248}
2025-12-12 19:44:39 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 89.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:44:39 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:44:42 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.16 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.63 GiB is free. Including non-PyTorch memory, this process has 3.13 GiB memory in use. Of the allocated memory 2.66 GiB is allocated by PyTorch, and 104.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:44:42 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.38671875, 'cpu_memory_gb': 4.963268280029297, 'cpu_vms_mb': 2965412.5234375, 'gpu_allocated_gb': 2.093556736, 'gpu_reserved_gb': 2.189426688, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.738915328}
2025-12-12 19:44:42 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 19.16 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.63 GiB is free. Including non-PyTorch memory, this process has 3.13 GiB memory in use. Of the allocated memory 2.66 GiB is allocated by PyTorch, and 104.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Final batch size: 1
2025-12-12 19:44:42 [ERROR] [lib.training.pipeline:1650] Error training fold 4: CUDA out of memory. Tried to allocate 19.16 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.63 GiB is free. Including non-PyTorch memory, this process has 3.13 GiB memory in use. Of the allocated memory 2.66 GiB is allocated by PyTorch, and 104.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    f"Retrying with batch_size={batch_size}, "
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 19.16 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.63 GiB is free. Including non-PyTorch memory, this process has 3.13 GiB memory in use. Of the allocated memory 2.66 GiB is allocated by PyTorch, and 104.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:44:42 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 38ec8a14717943898ac2d4547d425274
2025-12-12 19:44:43 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 5/5 (20% sample)
2025-12-12 19:44:43 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:44:43 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:44:43 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:44:43 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:44:43 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:44:43 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 610c215ae513481cba6426cc20ca3aca (experiment: x3d)
2025-12-12 19:44:43 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 5...
2025-12-12 19:44:44 [INFO] [lib.models.video:417] Chunk size increased from 360 to 370 (after 3 successes, increment: 10)
2025-12-12 19:44:45 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:44:46 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:44:48 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 156.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:44:49 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.38671875, 'cpu_memory_gb': 4.963268280029297, 'cpu_vms_mb': 2965412.5234375, 'gpu_allocated_gb': 1.631732736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:44:49 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 156.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:44:49 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:44:51 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:44:51 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.38671875, 'cpu_memory_gb': 4.963268280029297, 'cpu_vms_mb': 2965412.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:44:51 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:44:51 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:44:52 [INFO] [lib.models.video:417] Chunk size increased from 210 to 220 (after 3 successes, increment: 10)
2025-12-12 19:44:54 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:44:54 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.38671875, 'cpu_memory_gb': 4.963268280029297, 'cpu_vms_mb': 2965412.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:44:54 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:44:54 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:44:57 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.50 GiB is free. Including non-PyTorch memory, this process has 3.27 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 161.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:44:58 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.38671875, 'cpu_memory_gb': 4.963268280029297, 'cpu_vms_mb': 3001828.5234375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.336227328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.592114688}
2025-12-12 19:44:58 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.50 GiB is free. Including non-PyTorch memory, this process has 3.27 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 161.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Final batch size: 1
2025-12-12 19:44:58 [ERROR] [lib.training.pipeline:1650] Error training fold 5: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.50 GiB is free. Including non-PyTorch memory, this process has 3.27 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 161.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    f"Retrying with batch_size={batch_size}, "
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.50 GiB is free. Including non-PyTorch memory, this process has 3.27 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 161.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:44:58 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 610c215ae513481cba6426cc20ca3aca
2025-12-12 19:44:58 [INFO] [lib.training.pipeline:818] 
================================================================================
2025-12-12 19:44:58 [INFO] [lib.training.pipeline:819] Grid Search: Hyperparameter combination 14/16
2025-12-12 19:44:58 [INFO] [lib.training.pipeline:820] Parameters: {'learning_rate': 0.0001, 'backbone_lr': 5e-06, 'head_lr': 0.0001, 'weight_decay': 0.001}
2025-12-12 19:44:58 [INFO] [lib.training.pipeline:821] ================================================================================
2025-12-12 19:44:58 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 1/5 (20% sample)
2025-12-12 19:44:58 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:44:58 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:44:59 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:44:59 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:44:59 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:44:59 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: e331b6eb56e8458895f97982d0be3cab (experiment: x3d)
2025-12-12 19:44:59 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 1...
2025-12-12 19:45:01 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:45:01 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:45:03 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:45:04 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:45:04 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.38671875, 'cpu_memory_gb': 4.963268280029297, 'cpu_vms_mb': 3001828.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:45:04 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:45:04 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:45:07 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.54 GiB is free. Including non-PyTorch memory, this process has 3.23 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 121.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:45:07 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.38671875, 'cpu_memory_gb': 4.963268280029297, 'cpu_vms_mb': 3038244.5234375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.294284288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.634057728}
2025-12-12 19:45:07 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.54 GiB is free. Including non-PyTorch memory, this process has 3.23 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 121.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:45:07 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:45:10 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:45:10 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.38671875, 'cpu_memory_gb': 4.963268280029297, 'cpu_vms_mb': 3038244.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:45:10 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:45:10 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:45:12 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:45:13 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:45:13 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.38671875, 'cpu_memory_gb': 4.963268280029297, 'cpu_vms_mb': 3038244.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:45:13 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Final batch size: 1
2025-12-12 19:45:13 [ERROR] [lib.training.pipeline:1650] Error training fold 1: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    f"Retrying with batch_size={batch_size}, "
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:45:13 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: e331b6eb56e8458895f97982d0be3cab
2025-12-12 19:45:14 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 2/5 (20% sample)
2025-12-12 19:45:14 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:45:14 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:45:14 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:45:14 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:45:14 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:45:14 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 1c7824b42e5746b691ff5b241bae5fcc (experiment: x3d)
2025-12-12 19:45:14 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 2...
2025-12-12 19:45:16 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:45:16 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:45:19 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:45:20 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.38671875, 'cpu_memory_gb': 4.963268280029297, 'cpu_vms_mb': 3038244.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:45:20 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:45:20 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:45:32 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:45:34 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:45:34 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 7035.17578125, 'cpu_memory_gb': 6.870288848876953, 'cpu_vms_mb': 3040197.88671875, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:45:34 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:45:34 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:45:37 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:45:37 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 7035.17578125, 'cpu_memory_gb': 6.870288848876953, 'cpu_vms_mb': 3040197.88671875, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:45:37 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:45:37 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:45:40 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:45:41 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.96875, 'cpu_memory_gb': 4.963836669921875, 'cpu_vms_mb': 3038245.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:45:41 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Final batch size: 1
2025-12-12 19:45:41 [ERROR] [lib.training.pipeline:1650] Error training fold 2: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    f"Retrying with batch_size={batch_size}, "
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:45:41 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 1c7824b42e5746b691ff5b241bae5fcc
2025-12-12 19:45:41 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 3/5 (20% sample)
2025-12-12 19:45:41 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:45:41 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:45:41 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:45:41 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:45:42 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:45:42 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: ac2e83b834d64fc4bd30d8306315ef6a (experiment: x3d)
2025-12-12 19:45:42 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 3...
2025-12-12 19:45:43 [INFO] [lib.models.video:417] Chunk size increased from 370 to 380 (after 3 successes, increment: 10)
2025-12-12 19:45:44 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:45:44 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:45:47 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:45:47 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.96875, 'cpu_memory_gb': 4.963836669921875, 'cpu_vms_mb': 3038245.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:45:47 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:45:47 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:45:50 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:45:50 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.96875, 'cpu_memory_gb': 4.963836669921875, 'cpu_vms_mb': 3038245.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:45:50 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:45:50 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:45:52 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:45:53 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.52 GiB is free. Including non-PyTorch memory, this process has 3.25 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 141.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:45:53 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.96875, 'cpu_memory_gb': 4.963836669921875, 'cpu_vms_mb': 3074661.5234375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.315255808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.613086208}
2025-12-12 19:45:53 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.52 GiB is free. Including non-PyTorch memory, this process has 3.25 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 141.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:45:53 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:45:56 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:45:57 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.96875, 'cpu_memory_gb': 4.963836669921875, 'cpu_vms_mb': 3074661.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:45:57 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Final batch size: 1
2025-12-12 19:45:57 [ERROR] [lib.training.pipeline:1650] Error training fold 3: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    f"Retrying with batch_size={batch_size}, "
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:45:57 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: ac2e83b834d64fc4bd30d8306315ef6a
2025-12-12 19:45:57 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 4/5 (20% sample)
2025-12-12 19:45:57 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:45:57 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:45:57 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:45:57 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:45:57 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:45:57 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: fce1a0dded464b86abc2fdaaf48edcce (experiment: x3d)
2025-12-12 19:45:57 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 4...
2025-12-12 19:45:59 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:46:00 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:46:01 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:46:03 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:46:03 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.96875, 'cpu_memory_gb': 4.963836669921875, 'cpu_vms_mb': 3074661.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:46:03 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:46:03 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:46:05 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:46:06 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.96875, 'cpu_memory_gb': 4.963836669921875, 'cpu_vms_mb': 3074661.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.790967808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.137374208}
2025-12-12 19:46:06 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 129.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:46:06 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:46:09 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.52 GiB is free. Including non-PyTorch memory, this process has 3.25 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 141.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:46:09 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.96875, 'cpu_memory_gb': 4.963836669921875, 'cpu_vms_mb': 3111077.5234375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.315255808, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.613086208}
2025-12-12 19:46:09 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.52 GiB is free. Including non-PyTorch memory, this process has 3.25 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 141.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:46:09 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:46:10 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:46:11 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 15.04 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.12 GiB is allocated by PyTorch, and 141.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:46:12 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.96875, 'cpu_memory_gb': 4.963836669921875, 'cpu_vms_mb': 3111077.5234375, 'gpu_allocated_gb': 1.673716736, 'gpu_reserved_gb': 1.832910848, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.095431168}
2025-12-12 19:46:12 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 15.04 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.12 GiB is allocated by PyTorch, and 141.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Final batch size: 1
2025-12-12 19:46:12 [ERROR] [lib.training.pipeline:1650] Error training fold 4: CUDA out of memory. Tried to allocate 15.04 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.12 GiB is allocated by PyTorch, and 141.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    f"Retrying with batch_size={batch_size}, "
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 15.04 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.12 GiB is allocated by PyTorch, and 141.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:46:12 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: fce1a0dded464b86abc2fdaaf48edcce
2025-12-12 19:46:12 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 5/5 (20% sample)
2025-12-12 19:46:12 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:46:12 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:46:13 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:46:13 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:46:13 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:46:13 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 4b55c024b2f3485e97de5ce9aad86263 (experiment: x3d)
2025-12-12 19:46:13 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 5...
2025-12-12 19:46:15 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:46:15 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:46:18 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.48 GiB is free. Including non-PyTorch memory, this process has 3.29 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 181.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:46:18 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.96875, 'cpu_memory_gb': 4.963836669921875, 'cpu_vms_mb': 3147493.5234375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.357198848, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.571143168}
2025-12-12 19:46:18 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.48 GiB is free. Including non-PyTorch memory, this process has 3.29 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 181.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:46:18 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:46:19 [INFO] [lib.models.video:417] Chunk size increased from 210 to 220 (after 3 successes, increment: 10)
2025-12-12 19:46:21 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:46:21 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.96875, 'cpu_memory_gb': 4.963836669921875, 'cpu_vms_mb': 3147493.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.832910848, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.095431168}
2025-12-12 19:46:21 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:46:21 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:46:24 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.98 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.44 GiB is free. Including non-PyTorch memory, this process has 3.32 GiB memory in use. Of the allocated memory 2.77 GiB is allocated by PyTorch, and 193.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:46:24 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.96875, 'cpu_memory_gb': 4.963836669921875, 'cpu_vms_mb': 3183909.5234375, 'gpu_allocated_gb': 2.177524736, 'gpu_reserved_gb': 2.378170368, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.550171648}
2025-12-12 19:46:24 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 19.98 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.44 GiB is free. Including non-PyTorch memory, this process has 3.32 GiB memory in use. Of the allocated memory 2.77 GiB is allocated by PyTorch, and 193.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:46:24 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:46:26 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:46:26 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.96875, 'cpu_memory_gb': 4.963836669921875, 'cpu_vms_mb': 3183909.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.832910848, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.095431168}
2025-12-12 19:46:26 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Final batch size: 1
2025-12-12 19:46:26 [ERROR] [lib.training.pipeline:1650] Error training fold 5: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    f"Retrying with batch_size={batch_size}, "
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:46:26 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 4b55c024b2f3485e97de5ce9aad86263
2025-12-12 19:46:27 [INFO] [lib.training.pipeline:818] 
================================================================================
2025-12-12 19:46:27 [INFO] [lib.training.pipeline:819] Grid Search: Hyperparameter combination 15/16
2025-12-12 19:46:27 [INFO] [lib.training.pipeline:820] Parameters: {'learning_rate': 0.0001, 'backbone_lr': 5e-06, 'head_lr': 0.0005, 'weight_decay': 0.0001}
2025-12-12 19:46:27 [INFO] [lib.training.pipeline:821] ================================================================================
2025-12-12 19:46:27 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 1/5 (20% sample)
2025-12-12 19:46:27 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:46:27 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:46:27 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:46:27 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:46:27 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:46:27 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: a3db486ade3e4096980f0f92e85a07bd (experiment: x3d)
2025-12-12 19:46:27 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 1...
2025-12-12 19:46:29 [INFO] [lib.models.video:417] Chunk size increased from 380 to 390 (after 3 successes, increment: 10)
2025-12-12 19:46:29 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:46:30 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:46:33 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.50 GiB is free. Including non-PyTorch memory, this process has 3.27 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 161.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:46:33 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.0078125, 'cpu_memory_gb': 4.962898254394531, 'cpu_vms_mb': 3220324.5234375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.336227328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.592114688}
2025-12-12 19:46:33 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.50 GiB is free. Including non-PyTorch memory, this process has 3.27 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 161.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:46:33 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:46:36 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:46:36 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.0078125, 'cpu_memory_gb': 4.962898254394531, 'cpu_vms_mb': 3220324.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:46:36 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:46:36 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:46:39 [INFO] [lib.models.video:417] Chunk size increased from 210 to 220 (after 3 successes, increment: 10)
2025-12-12 19:46:41 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:46:41 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.0078125, 'cpu_memory_gb': 4.962898254394531, 'cpu_vms_mb': 3220324.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:46:41 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:46:41 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:46:43 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:46:43 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.0078125, 'cpu_memory_gb': 4.962898254394531, 'cpu_vms_mb': 3220324.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:46:43 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Final batch size: 1
2025-12-12 19:46:43 [ERROR] [lib.training.pipeline:1650] Error training fold 1: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    f"Retrying with batch_size={batch_size}, "
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:46:43 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: a3db486ade3e4096980f0f92e85a07bd
2025-12-12 19:46:44 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 2/5 (20% sample)
2025-12-12 19:46:44 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:46:44 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:46:44 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:46:44 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:46:44 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:46:44 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: a1110e6796844e57afae4f2d70f985ea (experiment: x3d)
2025-12-12 19:46:44 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 2...
2025-12-12 19:46:45 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:46:46 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:46:48 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:46:49 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:46:49 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.0078125, 'cpu_memory_gb': 4.962898254394531, 'cpu_vms_mb': 3220324.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:46:49 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:46:49 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:46:52 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.50 GiB is free. Including non-PyTorch memory, this process has 3.27 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 161.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:46:52 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.0078125, 'cpu_memory_gb': 4.962898254394531, 'cpu_vms_mb': 3256740.5234375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.336227328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.592114688}
2025-12-12 19:46:52 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.50 GiB is free. Including non-PyTorch memory, this process has 3.27 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 161.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:46:52 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:46:56 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.50 GiB is free. Including non-PyTorch memory, this process has 3.27 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 161.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:46:56 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.0078125, 'cpu_memory_gb': 4.962898254394531, 'cpu_vms_mb': 3293156.5234375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.336227328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.592114688}
2025-12-12 19:46:56 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.50 GiB is free. Including non-PyTorch memory, this process has 3.27 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 161.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Current batch size: 1
2025-12-12 19:46:56 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:46:57 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:46:59 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 156.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:46:59 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.0078125, 'cpu_memory_gb': 4.962898254394531, 'cpu_vms_mb': 3293156.5234375, 'gpu_allocated_gb': 1.631732736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:46:59 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 156.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 2, Final batch size: 1
2025-12-12 19:46:59 [ERROR] [lib.training.pipeline:1650] Error training fold 2: CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 156.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    f"Retrying with batch_size={batch_size}, "
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 156.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:46:59 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: a1110e6796844e57afae4f2d70f985ea
2025-12-12 19:46:59 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 3/5 (20% sample)
2025-12-12 19:46:59 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:46:59 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:46:59 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:46:59 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:47:00 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:47:00 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: d460ba80e07f40988023b6851d24f89b (experiment: x3d)
2025-12-12 19:47:00 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 3...
2025-12-12 19:47:01 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:47:02 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:47:06 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.54 GiB is free. Including non-PyTorch memory, this process has 3.23 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 121.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:47:06 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.0078125, 'cpu_memory_gb': 4.962898254394531, 'cpu_vms_mb': 3329572.5234375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.294284288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.634057728}
2025-12-12 19:47:06 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.54 GiB is free. Including non-PyTorch memory, this process has 3.23 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 121.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:47:06 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:47:08 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:47:09 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 21.22 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.36 GiB is free. Including non-PyTorch memory, this process has 3.40 GiB memory in use. Of the allocated memory 2.93 GiB is allocated by PyTorch, and 106.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:47:10 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.0078125, 'cpu_memory_gb': 4.962898254394531, 'cpu_vms_mb': 3365988.5234375, 'gpu_allocated_gb': 2.303476736, 'gpu_reserved_gb': 2.420113408, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.508228608}
2025-12-12 19:47:10 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 21.22 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.36 GiB is free. Including non-PyTorch memory, this process has 3.40 GiB memory in use. Of the allocated memory 2.93 GiB is allocated by PyTorch, and 106.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:47:10 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:47:12 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:47:12 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.0078125, 'cpu_memory_gb': 4.962898254394531, 'cpu_vms_mb': 3365988.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:47:12 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Current batch size: 1
2025-12-12 19:47:12 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:47:16 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.54 GiB is free. Including non-PyTorch memory, this process has 3.23 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 121.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:47:16 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.0078125, 'cpu_memory_gb': 4.962898254394531, 'cpu_vms_mb': 3402404.5234375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.294284288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.634057728}
2025-12-12 19:47:16 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.54 GiB is free. Including non-PyTorch memory, this process has 3.23 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 121.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 3, Final batch size: 1
2025-12-12 19:47:16 [ERROR] [lib.training.pipeline:1650] Error training fold 3: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.54 GiB is free. Including non-PyTorch memory, this process has 3.23 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 121.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    f"Retrying with batch_size={batch_size}, "
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.54 GiB is free. Including non-PyTorch memory, this process has 3.23 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 121.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:47:16 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: d460ba80e07f40988023b6851d24f89b
2025-12-12 19:47:16 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 4/5 (20% sample)
2025-12-12 19:47:16 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:47:16 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:47:17 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:47:17 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:47:17 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:47:17 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 0e76b8f999a04af5ac7b1579a2f06054 (experiment: x3d)
2025-12-12 19:47:17 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 4...
2025-12-12 19:47:19 [INFO] [lib.models.video:417] Chunk size increased from 390 to 400 (after 3 successes, increment: 10)
2025-12-12 19:47:19 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:47:20 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:47:23 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 116.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:47:23 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.0078125, 'cpu_memory_gb': 4.962898254394531, 'cpu_vms_mb': 3402404.5234375, 'gpu_allocated_gb': 1.631732736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:47:23 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 116.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:47:23 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:47:26 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:47:26 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.0078125, 'cpu_memory_gb': 4.962898254394531, 'cpu_vms_mb': 3402404.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:47:26 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.20 GiB is free. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 109.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:47:26 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:47:28 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:47:30 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.66 GiB is free. Including non-PyTorch memory, this process has 4.11 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 130.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:47:31 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.0078125, 'cpu_memory_gb': 4.962898254394531, 'cpu_vms_mb': 3438820.5234375, 'gpu_allocated_gb': 2.828276736, 'gpu_reserved_gb': 2.965372928, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 13.962969088}
2025-12-12 19:47:31 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.66 GiB is free. Including non-PyTorch memory, this process has 4.11 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 130.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Current batch size: 1
2025-12-12 19:47:31 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:47:33 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 116.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:47:33 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.0078125, 'cpu_memory_gb': 4.962898254394531, 'cpu_vms_mb': 3438820.5234375, 'gpu_allocated_gb': 1.631732736, 'gpu_reserved_gb': 1.769996288, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.158345728}
2025-12-12 19:47:33 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 116.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 4, Final batch size: 1
2025-12-12 19:47:33 [ERROR] [lib.training.pipeline:1650] Error training fold 4: CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 116.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    f"Retrying with batch_size={batch_size}, "
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.22 GiB is free. Including non-PyTorch memory, this process has 2.54 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 116.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:47:33 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 0e76b8f999a04af5ac7b1579a2f06054
2025-12-12 19:47:34 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 5/5 (20% sample)
2025-12-12 19:47:34 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:47:34 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:47:34 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:47:34 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:47:34 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:47:34 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 92a6bf20c8f241dca11eb249b99ef614 (experiment: x3d)
2025-12-12 19:47:34 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 5...
2025-12-12 19:47:36 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:47:37 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:47:38 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:47:40 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.50 GiB is free. Including non-PyTorch memory, this process has 3.27 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 161.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:47:40 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.0078125, 'cpu_memory_gb': 4.962898254394531, 'cpu_vms_mb': 3475236.5234375, 'gpu_allocated_gb': 2.156532736, 'gpu_reserved_gb': 2.336227328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 14.592114688}
2025-12-12 19:47:40 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 19.78 GiB. GPU 0 has a total capacity of 15.77 GiB of which 12.50 GiB is free. Including non-PyTorch memory, this process has 3.27 GiB memory in use. Of the allocated memory 2.74 GiB is allocated by PyTorch, and 161.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:47:40 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:47:44 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:47:44 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.0078125, 'cpu_memory_gb': 4.962898254394531, 'cpu_vms_mb': 3475236.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.832910848, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.095431168}
2025-12-12 19:47:44 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:47:44 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:47:48 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:47:48 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.0078125, 'cpu_memory_gb': 4.962898254394531, 'cpu_vms_mb': 3475236.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.832910848, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.095431168}
2025-12-12 19:47:48 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Current batch size: 1
2025-12-12 19:47:48 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:47:50 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:47:51 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:47:52 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.0078125, 'cpu_memory_gb': 4.962898254394531, 'cpu_vms_mb': 3475236.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.832910848, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.095431168}
2025-12-12 19:47:52 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 5, Final batch size: 1
2025-12-12 19:47:52 [ERROR] [lib.training.pipeline:1650] Error training fold 5: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    f"Retrying with batch_size={batch_size}, "
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.14 GiB is free. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 169.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:47:52 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 92a6bf20c8f241dca11eb249b99ef614
2025-12-12 19:47:52 [INFO] [lib.training.pipeline:818] 
================================================================================
2025-12-12 19:47:52 [INFO] [lib.training.pipeline:819] Grid Search: Hyperparameter combination 16/16
2025-12-12 19:47:52 [INFO] [lib.training.pipeline:820] Parameters: {'learning_rate': 0.0001, 'backbone_lr': 5e-06, 'head_lr': 0.0005, 'weight_decay': 0.001}
2025-12-12 19:47:52 [INFO] [lib.training.pipeline:821] ================================================================================
2025-12-12 19:47:52 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 1/5 (20% sample)
2025-12-12 19:47:52 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:47:52 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:47:52 [WARNING] [lib.training.x3d:109] Failed to load X3D from PyTorchVideo: No module named 'fvcore'. Trying torchvision...
2025-12-12 19:47:52 [WARNING] [lib.training.x3d:139] torchvision X3D not available: cannot import name 'x3d_m' from 'torchvision.models.video' (/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/__init__.py). Using r3d_18 as approximation.
2025-12-12 19:47:52 [WARNING] [lib.training.x3d:156] ⚠ Using r3d_18 as X3D approximation (X3D not available)
2025-12-12 19:47:53 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 58a11117984c4300a4bf67f4dc3ff6bb (experiment: x3d)
2025-12-12 19:47:53 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 1...
2025-12-12 19:47:54 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 1])
2025-12-12 19:47:55 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:47:58 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:47:58 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.0078125, 'cpu_memory_gb': 4.962898254394531, 'cpu_vms_mb': 3475236.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:47:58 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:47:58 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:48:00 [INFO] [lib.models.video:417] Chunk size increased from 200 to 210 (after 3 successes, increment: 10)
2025-12-12 19:48:01 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:48:01 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.0078125, 'cpu_memory_gb': 4.962898254394531, 'cpu_vms_mb': 3475236.5234375, 'gpu_allocated_gb': 1.652724736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:48:01 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 14.83 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.16 GiB is free. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.09 GiB is allocated by PyTorch, and 149.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:48:01 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:48:04 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 156.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:48:04 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.0078125, 'cpu_memory_gb': 4.962898254394531, 'cpu_vms_mb': 3475236.5234375, 'gpu_allocated_gb': 1.631732736, 'gpu_reserved_gb': 1.811939328, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 15.116402688}
2025-12-12 19:48:04 [WARNING] [lib.training.pipeline:1256] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 14.63 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.18 GiB is free. Including non-PyTorch memory, this process has 2.58 GiB memory in use. Of the allocated memory 2.06 GiB is allocated by PyTorch, and 156.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Current batch size: 1
2025-12-12 19:48:04 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:48:10 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.62 GiB is free. Including non-PyTorch memory, this process has 4.14 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 170.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:48:10 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5082.0078125, 'cpu_memory_gb': 4.962898254394531, 'cpu_vms_mb': 3511652.5234375, 'gpu_allocated_gb': 2.828276736, 'gpu_reserved_gb': 3.007315968, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 13.921026048}
2025-12-12 19:48:10 [ERROR] [lib.training.pipeline:1271] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.62 GiB is free. Including non-PyTorch memory, this process has 4.14 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 170.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: x3d, Fold: 1, Final batch size: 1
2025-12-12 19:48:10 [ERROR] [lib.training.pipeline:1650] Error training fold 1: CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.62 GiB is free. Including non-PyTorch memory, this process has 4.14 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 170.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    f"Retrying with batch_size={batch_size}, "
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 254, in forward
    x = self.layer1(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torchvision/models/video/resnet.py", line 113, in forward
    out = self.conv1(x)
          ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 717, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 712, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 15.77 GiB of which 11.62 GiB is free. Including non-PyTorch memory, this process has 4.14 GiB memory in use. Of the allocated memory 3.61 GiB is allocated by PyTorch, and 170.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:48:10 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 58a11117984c4300a4bf67f4dc3ff6bb
2025-12-12 19:48:11 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 2/5 (20% sample)
2025-12-12 19:48:11 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:48:11 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:48:13 [INFO] [lib.training.x3d:106] ✓ Successfully loaded X3D model from PyTorchVideo: x3d_m
2025-12-12 19:48:13 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 0b23c2154f56434ab9404d79401b5d88 (experiment: x3d)
2025-12-12 19:48:13 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 2...
2025-12-12 19:48:14 [INFO] [lib.models.video:417] Chunk size increased from 250 to 260 (after 3 successes, increment: 10)
2025-12-12 19:48:16 [ERROR] [lib.training.pipeline:1157] Model forward pass test failed: input image (T: 500 H: 5 W: 8) smaller than kernel size (kT: 16 kH: 7 kW: 7)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1114, in stage5_train_models
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/net.py", line 43, in forward
    x = block(x)
        ^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/head.py", line 374, in forward
    x = self.pool(x)
        ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/x3d.py", line 799, in forward
    x = self.pool(x)
        ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/pooling.py", line 898, in forward
    return F.avg_pool3d(
           ^^^^^^^^^^^^^
RuntimeError: input image (T: 500 H: 5 W: 8) smaller than kernel size (kT: 16 kH: 7 kW: 7)
2025-12-12 19:48:16 [ERROR] [lib.training.pipeline:1650] Error training fold 2: Model initialization failed: input image (T: 500 H: 5 W: 8) smaller than kernel size (kT: 16 kH: 7 kW: 7)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1114, in stage5_train_models
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/net.py", line 43, in forward
    x = block(x)
        ^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/head.py", line 374, in forward
    x = self.pool(x)
        ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/x3d.py", line 799, in forward
    x = self.pool(x)
        ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/pooling.py", line 898, in forward
    return F.avg_pool3d(
           ^^^^^^^^^^^^^
RuntimeError: input image (T: 500 H: 5 W: 8) smaller than kernel size (kT: 16 kH: 7 kW: 7)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1158, in stage5_train_models
    f"GPU memory may be insufficient for this model."
ValueError: Model initialization failed: input image (T: 500 H: 5 W: 8) smaller than kernel size (kT: 16 kH: 7 kW: 7)
2025-12-12 19:48:16 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 0b23c2154f56434ab9404d79401b5d88
2025-12-12 19:48:17 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 3/5 (20% sample)
2025-12-12 19:48:17 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:48:17 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:48:17 [INFO] [lib.training.x3d:106] ✓ Successfully loaded X3D model from PyTorchVideo: x3d_m
2025-12-12 19:48:17 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: e20a1e7928714703a9c5eb5c6747d6de (experiment: x3d)
2025-12-12 19:48:17 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 3...
2025-12-12 19:48:19 [ERROR] [lib.training.pipeline:1157] Model forward pass test failed: input image (T: 500 H: 5 W: 8) smaller than kernel size (kT: 16 kH: 7 kW: 7)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1114, in stage5_train_models
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/net.py", line 43, in forward
    x = block(x)
        ^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/head.py", line 374, in forward
    x = self.pool(x)
        ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/x3d.py", line 799, in forward
    x = self.pool(x)
        ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/pooling.py", line 898, in forward
    return F.avg_pool3d(
           ^^^^^^^^^^^^^
RuntimeError: input image (T: 500 H: 5 W: 8) smaller than kernel size (kT: 16 kH: 7 kW: 7)
2025-12-12 19:48:19 [ERROR] [lib.training.pipeline:1650] Error training fold 3: Model initialization failed: input image (T: 500 H: 5 W: 8) smaller than kernel size (kT: 16 kH: 7 kW: 7)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1114, in stage5_train_models
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/net.py", line 43, in forward
    x = block(x)
        ^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/head.py", line 374, in forward
    x = self.pool(x)
        ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/x3d.py", line 799, in forward
    x = self.pool(x)
        ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/pooling.py", line 898, in forward
    return F.avg_pool3d(
           ^^^^^^^^^^^^^
RuntimeError: input image (T: 500 H: 5 W: 8) smaller than kernel size (kT: 16 kH: 7 kW: 7)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1158, in stage5_train_models
    f"GPU memory may be insufficient for this model."
ValueError: Model initialization failed: input image (T: 500 H: 5 W: 8) smaller than kernel size (kT: 16 kH: 7 kW: 7)
2025-12-12 19:48:19 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: e20a1e7928714703a9c5eb5c6747d6de
2025-12-12 19:48:20 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 4/5 (20% sample)
2025-12-12 19:48:20 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:48:20 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:48:20 [INFO] [lib.training.x3d:106] ✓ Successfully loaded X3D model from PyTorchVideo: x3d_m
2025-12-12 19:48:20 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: dbb86870c7c84c4eb3a1d6be22956262 (experiment: x3d)
2025-12-12 19:48:20 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 4...
2025-12-12 19:48:22 [ERROR] [lib.training.pipeline:1157] Model forward pass test failed: input image (T: 500 H: 5 W: 8) smaller than kernel size (kT: 16 kH: 7 kW: 7)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1114, in stage5_train_models
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/net.py", line 43, in forward
    x = block(x)
        ^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/head.py", line 374, in forward
    x = self.pool(x)
        ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/x3d.py", line 799, in forward
    x = self.pool(x)
        ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/pooling.py", line 898, in forward
    return F.avg_pool3d(
           ^^^^^^^^^^^^^
RuntimeError: input image (T: 500 H: 5 W: 8) smaller than kernel size (kT: 16 kH: 7 kW: 7)
2025-12-12 19:48:22 [ERROR] [lib.training.pipeline:1650] Error training fold 4: Model initialization failed: input image (T: 500 H: 5 W: 8) smaller than kernel size (kT: 16 kH: 7 kW: 7)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1114, in stage5_train_models
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/net.py", line 43, in forward
    x = block(x)
        ^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/head.py", line 374, in forward
    x = self.pool(x)
        ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/x3d.py", line 799, in forward
    x = self.pool(x)
        ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/pooling.py", line 898, in forward
    return F.avg_pool3d(
           ^^^^^^^^^^^^^
RuntimeError: input image (T: 500 H: 5 W: 8) smaller than kernel size (kT: 16 kH: 7 kW: 7)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1158, in stage5_train_models
    f"GPU memory may be insufficient for this model."
ValueError: Model initialization failed: input image (T: 500 H: 5 W: 8) smaller than kernel size (kT: 16 kH: 7 kW: 7)
2025-12-12 19:48:22 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: dbb86870c7c84c4eb3a1d6be22956262
2025-12-12 19:48:22 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - x3d - Fold 5/5 (20% sample)
2025-12-12 19:48:22 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 1, Gradient accumulation steps: 32, Effective batch size: 32
2025-12-12 19:48:22 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:48:22 [INFO] [lib.training.x3d:106] ✓ Successfully loaded X3D model from PyTorchVideo: x3d_m
2025-12-12 19:48:23 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: ec139851a07444d9b0cddaee6119767c (experiment: x3d)
2025-12-12 19:48:23 [INFO] [lib.training.pipeline:1073] Training PyTorch model x3d on fold 5...
2025-12-12 19:48:24 [INFO] [lib.models.video:417] Chunk size increased from 400 to 410 (after 3 successes, increment: 10)
2025-12-12 19:48:24 [ERROR] [lib.training.pipeline:1157] Model forward pass test failed: input image (T: 500 H: 5 W: 8) smaller than kernel size (kT: 16 kH: 7 kW: 7)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1114, in stage5_train_models
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/net.py", line 43, in forward
    x = block(x)
        ^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/head.py", line 374, in forward
    x = self.pool(x)
        ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/x3d.py", line 799, in forward
    x = self.pool(x)
        ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/pooling.py", line 898, in forward
    return F.avg_pool3d(
           ^^^^^^^^^^^^^
RuntimeError: input image (T: 500 H: 5 W: 8) smaller than kernel size (kT: 16 kH: 7 kW: 7)
2025-12-12 19:48:24 [ERROR] [lib.training.pipeline:1650] Error training fold 5: Model initialization failed: input image (T: 500 H: 5 W: 8) smaller than kernel size (kT: 16 kH: 7 kW: 7)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1114, in stage5_train_models
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/net.py", line 43, in forward
    x = block(x)
        ^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/head.py", line 374, in forward
    x = self.pool(x)
        ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/x3d.py", line 799, in forward
    x = self.pool(x)
        ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/pooling.py", line 898, in forward
    return F.avg_pool3d(
           ^^^^^^^^^^^^^
RuntimeError: input image (T: 500 H: 5 W: 8) smaller than kernel size (kT: 16 kH: 7 kW: 7)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1158, in stage5_train_models
    f"GPU memory may be insufficient for this model."
ValueError: Model initialization failed: input image (T: 500 H: 5 W: 8) smaller than kernel size (kT: 16 kH: 7 kW: 7)
2025-12-12 19:48:24 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: ec139851a07444d9b0cddaee6119767c
2025-12-12 19:48:25 [INFO] [lib.training.pipeline:1709] ================================================================================
2025-12-12 19:48:25 [INFO] [lib.training.pipeline:1710] FINAL TRAINING: Using full dataset with best hyperparameters
2025-12-12 19:48:25 [INFO] [lib.training.pipeline:1711] ================================================================================
2025-12-12 19:48:25 [INFO] [lib.training.pipeline:1724] Final training: Using 5-fold stratified cross-validation on full dataset (3278 rows)
2025-12-12 19:48:25 [INFO] [lib.training.pipeline:1734] Final training using default hyperparameters (no grid search)
2025-12-12 19:48:25 [INFO] [lib.training.pipeline:1738] 
Final Training - x3d - Fold 1/5 (full dataset)
2025-12-12 19:48:25 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:48:25 [INFO] [lib.training.x3d:106] ✓ Successfully loaded X3D model from PyTorchVideo: x3d_m
2025-12-12 19:48:25 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 1e809a98e01b4f959360bfd3fc882316 (experiment: x3d)
2025-12-12 19:48:25 [INFO] [lib.training.pipeline:1895] Training PyTorch model x3d on fold 1 (full dataset)...
2025-12-12 19:48:25 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:48:27 [ERROR] [lib.training.pipeline:2095] Error training final fold 1: input image (T: 500 H: 5 W: 8) smaller than kernel size (kT: 16 kH: 7 kW: 7)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1901, in stage5_train_models
    mlflow_tracker.set_tag("phase", "final_training")
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 330, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/net.py", line 43, in forward
    x = block(x)
        ^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/head.py", line 374, in forward
    x = self.pool(x)
        ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/x3d.py", line 799, in forward
    x = self.pool(x)
        ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/pooling.py", line 898, in forward
    return F.avg_pool3d(
           ^^^^^^^^^^^^^
RuntimeError: input image (T: 500 H: 5 W: 8) smaller than kernel size (kT: 16 kH: 7 kW: 7)
2025-12-12 19:48:27 [INFO] [lib.training.pipeline:1738] 
Final Training - x3d - Fold 2/5 (full dataset)
2025-12-12 19:48:27 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:48:27 [INFO] [lib.training.x3d:106] ✓ Successfully loaded X3D model from PyTorchVideo: x3d_m
2025-12-12 19:48:27 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: af79f9c0bb9d4fffbbfc8724c1b524b1 (experiment: x3d)
2025-12-12 19:48:27 [INFO] [lib.training.pipeline:1895] Training PyTorch model x3d on fold 2 (full dataset)...
2025-12-12 19:48:27 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:48:30 [ERROR] [lib.training.pipeline:2095] Error training final fold 2: input image (T: 500 H: 5 W: 8) smaller than kernel size (kT: 16 kH: 7 kW: 7)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1901, in stage5_train_models
    mlflow_tracker.set_tag("phase", "final_training")
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 330, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/net.py", line 43, in forward
    x = block(x)
        ^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/head.py", line 374, in forward
    x = self.pool(x)
        ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/x3d.py", line 799, in forward
    x = self.pool(x)
        ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/pooling.py", line 898, in forward
    return F.avg_pool3d(
           ^^^^^^^^^^^^^
RuntimeError: input image (T: 500 H: 5 W: 8) smaller than kernel size (kT: 16 kH: 7 kW: 7)
2025-12-12 19:48:30 [INFO] [lib.training.pipeline:1738] 
Final Training - x3d - Fold 3/5 (full dataset)
2025-12-12 19:48:30 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:48:30 [INFO] [lib.training.x3d:106] ✓ Successfully loaded X3D model from PyTorchVideo: x3d_m
2025-12-12 19:48:30 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 24ec2c9b00384ea48869cbf1b76374c4 (experiment: x3d)
2025-12-12 19:48:30 [INFO] [lib.training.pipeline:1895] Training PyTorch model x3d on fold 3 (full dataset)...
2025-12-12 19:48:30 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:48:33 [INFO] [lib.models.video:417] Chunk size increased from 210 to 220 (after 3 successes, increment: 10)
2025-12-12 19:48:35 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 106.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 4.19 MiB is free. Including non-PyTorch memory, this process has 15.76 GiB memory in use. Of the allocated memory 15.28 GiB is allocated by PyTorch, and 112.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:48:36 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5091.0546875, 'cpu_memory_gb': 4.971733093261719, 'cpu_vms_mb': 3511720.55078125, 'gpu_allocated_gb': 16.405076992, 'gpu_reserved_gb': 16.523460608, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.404881408}
2025-12-12 19:48:36 [ERROR] [lib.training.pipeline:2095] Error training final fold 3: CUDA out of memory. Tried to allocate 106.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 4.19 MiB is free. Including non-PyTorch memory, this process has 15.76 GiB memory in use. Of the allocated memory 15.28 GiB is allocated by PyTorch, and 112.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1901, in stage5_train_models
    mlflow_tracker.set_tag("phase", "final_training")
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/net.py", line 43, in forward
    x = block(x)
        ^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/resnet.py", line 1399, in forward
    x = res_block(x)
        ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/resnet.py", line 1186, in forward
    x = self.branch_fusion(shortcut, self.branch2(x))
                                     ^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/resnet.py", line 1350, in forward
    x = self.norm_a(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
           ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/functional.py", line 2813, in batch_norm
    return torch.batch_norm(
           ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 106.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 4.19 MiB is free. Including non-PyTorch memory, this process has 15.76 GiB memory in use. Of the allocated memory 15.28 GiB is allocated by PyTorch, and 112.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:48:36 [INFO] [lib.training.pipeline:1738] 
Final Training - x3d - Fold 4/5 (full dataset)
2025-12-12 19:48:36 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:48:36 [INFO] [lib.training.x3d:106] ✓ Successfully loaded X3D model from PyTorchVideo: x3d_m
2025-12-12 19:48:36 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 4567c7583511419a94951d891f27be6d (experiment: x3d)
2025-12-12 19:48:36 [INFO] [lib.training.pipeline:1895] Training PyTorch model x3d on fold 4 (full dataset)...
2025-12-12 19:48:36 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:48:38 [ERROR] [lib.training.pipeline:2095] Error training final fold 4: input image (T: 500 H: 5 W: 8) smaller than kernel size (kT: 16 kH: 7 kW: 7)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1901, in stage5_train_models
    mlflow_tracker.set_tag("phase", "final_training")
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 330, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/net.py", line 43, in forward
    x = block(x)
        ^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/head.py", line 374, in forward
    x = self.pool(x)
        ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/x3d.py", line 799, in forward
    x = self.pool(x)
        ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/pooling.py", line 898, in forward
    return F.avg_pool3d(
           ^^^^^^^^^^^^^
RuntimeError: input image (T: 500 H: 5 W: 8) smaller than kernel size (kT: 16 kH: 7 kW: 7)
2025-12-12 19:48:38 [INFO] [lib.training.pipeline:1738] 
Final Training - x3d - Fold 5/5 (full dataset)
2025-12-12 19:48:38 [INFO] [lib.training.x3d:47] Loading X3D model from PyTorchVideo: x3d_m (pretrained=True)
2025-12-12 19:48:38 [INFO] [lib.training.x3d:106] ✓ Successfully loaded X3D model from PyTorchVideo: x3d_m
2025-12-12 19:48:38 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 3661f2b91d3046f6934900d252f474a3 (experiment: x3d)
2025-12-12 19:48:38 [INFO] [lib.training.pipeline:1895] Training PyTorch model x3d on fold 5 (full dataset)...
2025-12-12 19:48:38 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:48:44 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 106.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 4.19 MiB is free. Including non-PyTorch memory, this process has 15.76 GiB memory in use. Of the allocated memory 15.28 GiB is allocated by PyTorch, and 112.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:48:44 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 5091.16796875, 'cpu_memory_gb': 4.971843719482422, 'cpu_vms_mb': 3511720.55078125, 'gpu_allocated_gb': 16.405076992, 'gpu_reserved_gb': 16.523460608, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.404881408}
2025-12-12 19:48:44 [ERROR] [lib.training.pipeline:2095] Error training final fold 5: CUDA out of memory. Tried to allocate 106.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 4.19 MiB is free. Including non-PyTorch memory, this process has 15.76 GiB memory in use. Of the allocated memory 15.28 GiB is allocated by PyTorch, and 112.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1901, in stage5_train_models
    mlflow_tracker.set_tag("phase", "final_training")
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/x3d.py", line 176, in forward
    return self.backbone(x)
           ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/net.py", line 43, in forward
    x = block(x)
        ^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/resnet.py", line 1399, in forward
    x = res_block(x)
        ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/resnet.py", line 1186, in forward
    x = self.branch_fusion(shortcut, self.branch2(x))
                                     ^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/urvim/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/resnet.py", line 1350, in forward
    x = self.norm_a(x)
        ^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
           ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/functional.py", line 2813, in batch_norm
    return torch.batch_norm(
           ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 106.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 4.19 MiB is free. Including non-PyTorch memory, this process has 15.76 GiB memory in use. Of the allocated memory 15.28 GiB is allocated by PyTorch, and 112.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:48:44 [WARNING] [lib.training.pipeline:118] No model files found in /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/stage5/x3d/fold_1 to copy
2025-12-12 19:48:44 [INFO] [lib.training.pipeline:2168] 
x3d - Avg Val Loss: nan, Avg Val Acc: nan, Avg Val F1: nan
2025-12-12 19:48:46 [INFO] [lib.training.visualization:256] Saved CV fold comparison to /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/stage5/x3d/plots/cv_fold_comparison.png
2025-12-12 19:48:46 [INFO] [lib.training.pipeline:2196] Generated plots for x3d in /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/stage5/x3d/plots
2025-12-12 19:48:46 [INFO] [lib.training.pipeline:2233] ================================================================================
2025-12-12 19:48:46 [INFO] [lib.training.pipeline:2234] Stage 5: Model Training Pipeline Completed
2025-12-12 19:48:46 [INFO] [lib.training.pipeline:2235] ================================================================================
2025-12-12 19:48:46 [INFO] [__main__:401] ================================================================================
2025-12-12 19:48:46 [INFO] [__main__:402] STAGE 5 COMPLETED SUCCESSFULLY
2025-12-12 19:48:46 [INFO] [__main__:403] ================================================================================
2025-12-12 19:48:46 [INFO] [__main__:404] Execution time: 1449.56 seconds (24.16 minutes)
2025-12-12 19:48:46 [INFO] [__main__:406] Output directory: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/stage5
2025-12-12 19:48:46 [INFO] [__main__:407] Models trained: ['x3d']
2025-12-12 19:48:46 [INFO] [__main__:408] K-fold splits: 5
2025-12-12 19:48:46 [INFO] [__main__:414] ================================================================================
2025-12-12 19:48:46 [INFO] [__main__:415] Final memory statistics:
2025-12-12 19:48:46 [INFO] [__main__:416] ================================================================================
2025-12-12 19:48:46 [INFO] [lib.utils.memory:89] Memory stats (Stage 5: after training): {'cpu_memory_mb': 5091.56640625, 'cpu_memory_gb': 4.972232818603516, 'cpu_vms_mb': 3511752.11328125, 'gpu_allocated_gb': 0.00851968, 'gpu_reserved_gb': 16.523460608, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.404881408}
2025-12-12 19:48:46 [INFO] [__main__:419] ================================================================================
2025-12-12 19:48:46 [INFO] [__main__:420] Training complete!
2025-12-12 19:48:46 [INFO] [__main__:421] Results saved to: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/stage5
2025-12-12 19:48:46 [INFO] [__main__:422] ================================================================================
