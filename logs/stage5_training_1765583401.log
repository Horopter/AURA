2025-12-12 18:50:01 [INFO] [__main__:310] ================================================================================
2025-12-12 18:50:01 [INFO] [__main__:311] STAGE 5: MODEL TRAINING
2025-12-12 18:50:01 [INFO] [__main__:312] ================================================================================
2025-12-12 18:50:01 [INFO] [__main__:313] Project root: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc
2025-12-12 18:50:01 [INFO] [__main__:314] Scaled metadata: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/scaled_videos/scaled_metadata.arrow
2025-12-12 18:50:01 [INFO] [__main__:315] Features Stage 2: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/features_stage2/features_metadata.arrow
2025-12-12 18:50:01 [INFO] [__main__:316] Features Stage 4: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/features_stage4/features_scaled_metadata.arrow
2025-12-12 18:50:01 [INFO] [__main__:317] Model types: ['naive_cnn']
2025-12-12 18:50:01 [INFO] [__main__:318] K-fold splits: 5
2025-12-12 18:50:01 [INFO] [__main__:319] Number of frames: 1000
2025-12-12 18:50:01 [INFO] [__main__:320] Output directory: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/stage5
2025-12-12 18:50:01 [INFO] [__main__:321] Experiment tracking: Enabled
2025-12-12 18:50:01 [INFO] [__main__:324] Delete existing: True
2025-12-12 18:50:01 [INFO] [__main__:325] Resume mode: True
2025-12-12 18:50:01 [INFO] [__main__:326] Log file: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/logs/stage5_training_1765583401.log
2025-12-12 18:50:01 [INFO] [__main__:333] ================================================================================
2025-12-12 18:50:01 [INFO] [__main__:334] Checking prerequisites...
2025-12-12 18:50:01 [INFO] [__main__:335] ================================================================================
2025-12-12 18:50:01 [INFO] [__main__:342] ✓ Scaled metadata file found: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/scaled_videos/scaled_metadata.arrow
2025-12-12 18:50:01 [INFO] [__main__:352] ✓ All model types are valid
2025-12-12 18:50:01 [INFO] [__main__:358] ================================================================================
2025-12-12 18:50:01 [INFO] [__main__:359] Initial memory statistics:
2025-12-12 18:50:01 [INFO] [__main__:360] ================================================================================
2025-12-12 18:50:01 [INFO] [lib.utils.memory:89] Memory stats (Stage 5: before training): {'cpu_memory_mb': 735.77734375, 'cpu_memory_gb': 0.7185325622558594, 'cpu_vms_mb': 10217.5859375, 'gpu_allocated_gb': 0.0, 'gpu_reserved_gb': 0.0, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 16.928342016}
2025-12-12 18:50:01 [INFO] [__main__:364] ================================================================================
2025-12-12 18:50:01 [INFO] [__main__:365] Starting Stage 5: Model Training
2025-12-12 18:50:01 [INFO] [__main__:366] ================================================================================
2025-12-12 18:50:01 [INFO] [__main__:367] Training 1 model(s) with 5-fold cross-validation
2025-12-12 18:50:01 [INFO] [__main__:368] This may take a while depending on dataset size and model complexity...
2025-12-12 18:50:01 [INFO] [__main__:369] Progress will be logged in real-time
2025-12-12 18:50:01 [INFO] [__main__:370] ================================================================================
2025-12-12 18:50:01 [INFO] [__main__:375] Calling Stage 5 training pipeline...
2025-12-12 18:50:01 [INFO] [__main__:376] This may take a while - progress will be logged in real-time
2025-12-12 18:50:01 [INFO] [lib.training.pipeline:490] ================================================================================
2025-12-12 18:50:01 [INFO] [lib.training.pipeline:491] Stage 5: Model Training Pipeline Started
2025-12-12 18:50:01 [INFO] [lib.training.pipeline:492] ================================================================================
2025-12-12 18:50:01 [INFO] [lib.training.pipeline:493] Model types: ['naive_cnn']
2025-12-12 18:50:01 [INFO] [lib.training.pipeline:494] K-fold splits: 5
2025-12-12 18:50:01 [INFO] [lib.training.pipeline:495] Frames per video: 1000
2025-12-12 18:50:01 [INFO] [lib.training.pipeline:496] Output directory: data/stage5
2025-12-12 18:50:01 [INFO] [lib.training.pipeline:497] Initializing pipeline...
2025-12-12 18:50:01 [INFO] [lib.training.pipeline:265] ================================================================================
2025-12-12 18:50:01 [INFO] [lib.training.pipeline:266] STAGE 5 PREREQUISITE VALIDATION
2025-12-12 18:50:01 [INFO] [lib.training.pipeline:267] ================================================================================
2025-12-12 18:50:01 [INFO] [lib.training.pipeline:270] 
[1/3] Checking Stage 3 (scaled videos) - REQUIRED for all models...
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:279] ✓ Stage 3 metadata found: 3278 scaled videos
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:280]   Path: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/scaled_videos/scaled_metadata.arrow
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:283] 
[2/3] Checking Stage 2 (features) - REQUIRED for *_stage2 models...
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:291] ✓ Stage 2 metadata found: 3278 feature rows
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:292]   Path: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/features_stage2/features_metadata.arrow
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:295] 
[3/3] Checking Stage 4 (scaled features) - REQUIRED for *_stage2_stage4 models...
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:303] ✓ Stage 4 metadata found: 3277 feature rows
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:304]   Path: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/features_stage4/features_scaled_metadata.arrow
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:307] 
================================================================================
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:308] MODEL REQUIREMENTS CHECK
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:309] ================================================================================
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:333] ✓ naive_cnn: CAN RUN
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:336] 
================================================================================
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:337] VALIDATION SUMMARY
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:338] ================================================================================
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:339] Stage 3 (scaled videos): ✓ Available
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:340]   Count: 3278 videos
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:341] Stage 2 (features): ✓ Available
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:342]   Count: 3278 feature rows
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:343] Stage 4 (scaled features): ✓ Available
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:344]   Count: 3277 feature rows
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:345] 
Runnable models: 1/1
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:346]   ['naive_cnn']
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:353] ================================================================================
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:583] 
Stage 5: Loading metadata...
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:609] Loaded metadata: 3278 rows
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:619] ✓ Data validation passed: 3278 rows (> 3000 required)
2025-12-12 18:50:02 [WARNING] [lib.utils.guardrails:171] Disk usage high: free=834080.49GB, used=1665759.51GB (66.6%)
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:651] Stage 5: Found 3278 scaled videos
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:730] Stage 5: Deleting existing model results (clean mode)...
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:739] Deleted existing results for naive_cnn
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:742] Stage 5: Deleted 1 existing model directories
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:747] 
================================================================================
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:748] Stage 5: Training model: naive_cnn
2025-12-12 18:50:02 [INFO] [lib.training.pipeline:749] ================================================================================
2025-12-12 18:50:03 [INFO] [lib.training.pipeline:773] Grid search: 18 hyperparameter combinations to try
2025-12-12 18:50:03 [INFO] [lib.training.pipeline:781] ================================================================================
2025-12-12 18:50:03 [INFO] [lib.training.pipeline:782] HYPERPARAMETER SEARCH: Using 20% stratified sample for efficiency
2025-12-12 18:50:03 [INFO] [lib.training.pipeline:783] ================================================================================
2025-12-12 18:50:04 [INFO] [lib.training.pipeline:792] Hyperparameter search sample: 655 rows (20.0% of 3278 total)
2025-12-12 18:50:05 [INFO] [lib.training.pipeline:805] Using 5-fold stratified cross-validation on 20% sample for hyperparameter search
2025-12-12 18:50:05 [INFO] [lib.training.pipeline:818] 
================================================================================
2025-12-12 18:50:05 [INFO] [lib.training.pipeline:819] Grid Search: Hyperparameter combination 1/18
2025-12-12 18:50:05 [INFO] [lib.training.pipeline:820] Parameters: {'learning_rate': 0.0001, 'weight_decay': 1e-05, 'batch_size': 2, 'num_epochs': 20}
2025-12-12 18:50:05 [INFO] [lib.training.pipeline:821] ================================================================================
2025-12-12 18:50:05 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 1/5 (20% sample)
2025-12-12 18:50:05 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 18:50:15 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: c6c94f3653df4f319ad3c39bd5e49824 (experiment: naive_cnn)
2025-12-12 18:50:15 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 1...
2025-12-12 18:50:25 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 18:50:25 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:50:35 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 15.62 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.88 GiB is free. Including non-PyTorch memory, this process has 1.88 GiB memory in use. Of the allocated memory 1.47 GiB is allocated by PyTorch, and 34.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 256, 256]), Model num_frames: 1000
2025-12-12 18:50:35 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 15.62 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.88 GiB is free. Including non-PyTorch memory, this process has 1.88 GiB memory in use. Of the allocated memory 1.47 GiB is allocated by PyTorch, and 34.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Current batch size: 2
2025-12-12 18:50:35 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 18:50:35 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 18:50:35 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:50:48 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 1.71 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.54 GiB is free. Including non-PyTorch memory, this process has 14.21 GiB memory in use. Of the allocated memory 13.80 GiB is allocated by PyTorch, and 36.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 224, 256]), Model num_frames: 1000
2025-12-12 18:50:48 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 2: CUDA out of memory. Tried to allocate 1.71 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.54 GiB is free. Including non-PyTorch memory, this process has 14.21 GiB memory in use. Of the allocated memory 13.80 GiB is allocated by PyTorch, and 36.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:50:49 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 2): {'cpu_memory_mb': 6759.10546875, 'cpu_memory_gb': 6.600688934326172, 'cpu_vms_mb': 53837.1328125, 'gpu_allocated_gb': 14.813709824, 'gpu_reserved_gb': 14.852030464, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 2.076311552}
2025-12-12 18:50:49 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 1.71 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.54 GiB is free. Including non-PyTorch memory, this process has 14.21 GiB memory in use. Of the allocated memory 13.80 GiB is allocated by PyTorch, and 36.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Current batch size: 1
2025-12-12 18:50:49 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:50:55 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:50:55 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 1: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:50:55 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 1): {'cpu_memory_mb': 6759.3125, 'cpu_memory_gb': 6.60089111328125, 'cpu_vms_mb': 53837.1328125, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 18:50:55 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Current batch size: 1
2025-12-12 18:50:55 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:51:14 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 256, 256]), Model num_frames: 1000
2025-12-12 18:51:14 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 5: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:51:14 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 5): {'cpu_memory_mb': 6760.51171875, 'cpu_memory_gb': 6.602062225341797, 'cpu_vms_mb': 53839.1328125, 'gpu_allocated_gb': 14.83005696, 'gpu_reserved_gb': 14.873001984, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 2.055340032}
2025-12-12 18:51:14 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Final batch size: 1
2025-12-12 18:51:14 [ERROR] [lib.training.pipeline:1640] Error training fold 1: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 110, in forward
    x = F.relu(self.bn2(self.conv2(x)))
                        ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 543, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:51:14 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: c6c94f3653df4f319ad3c39bd5e49824
2025-12-12 18:51:15 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 2/5 (20% sample)
2025-12-12 18:51:15 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 18:51:15 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: a550a9bd6c61466587e2e366cda1ede6 (experiment: naive_cnn)
2025-12-12 18:51:15 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 2...
2025-12-12 18:51:16 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 18:51:17 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:51:24 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 144, 256]), Model num_frames: 1000
2025-12-12 18:51:24 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Current batch size: 2
2025-12-12 18:51:24 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 18:51:24 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 18:51:24 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:51:34 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 1.57 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.07 GiB is free. Including non-PyTorch memory, this process has 14.68 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 40.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 256, 206]), Model num_frames: 1000
2025-12-12 18:51:34 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 3: CUDA out of memory. Tried to allocate 1.57 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.07 GiB is free. Including non-PyTorch memory, this process has 14.68 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 40.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:51:34 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 3): {'cpu_memory_mb': 6763.80859375, 'cpu_memory_gb': 6.605281829833984, 'cpu_vms_mb': 72049.8359375, 'gpu_allocated_gb': 15.312398848, 'gpu_reserved_gb': 15.355346944, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 1.572995072}
2025-12-12 18:51:34 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 1.57 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.07 GiB is free. Including non-PyTorch memory, this process has 14.68 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 40.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Current batch size: 1
2025-12-12 18:51:34 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:51:46 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.54 GiB is free. Including non-PyTorch memory, this process has 14.21 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 20.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 256, 256]), Model num_frames: 1000
2025-12-12 18:51:46 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 3: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.54 GiB is free. Including non-PyTorch memory, this process has 14.21 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 20.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:51:46 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 3): {'cpu_memory_mb': 6764.1328125, 'cpu_memory_gb': 6.605598449707031, 'cpu_vms_mb': 72049.8359375, 'gpu_allocated_gb': 14.83005696, 'gpu_reserved_gb': 14.852030464, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 2.076311552}
2025-12-12 18:51:46 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.54 GiB is free. Including non-PyTorch memory, this process has 14.21 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 20.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Current batch size: 1
2025-12-12 18:51:46 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:51:51 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:51:51 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 1: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:51:51 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 1): {'cpu_memory_mb': 6763.1484375, 'cpu_memory_gb': 6.604637145996094, 'cpu_vms_mb': 72048.8359375, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 18:51:51 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Final batch size: 1
2025-12-12 18:51:51 [ERROR] [lib.training.pipeline:1640] Error training fold 2: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 112, in forward
    x = F.relu(self.bn3(self.conv3(x)))
                        ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 543, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:51:51 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: a550a9bd6c61466587e2e366cda1ede6
2025-12-12 18:51:51 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 3/5 (20% sample)
2025-12-12 18:51:51 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 18:51:51 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 8f2ec2ff28ff47f68cb5984d5d51b042 (experiment: naive_cnn)
2025-12-12 18:51:51 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 3...
2025-12-12 18:51:54 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 18:51:54 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:51:58 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 12.45 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.72 GiB is free. Including non-PyTorch memory, this process has 14.04 GiB memory in use. Of the allocated memory 13.64 GiB is allocated by PyTorch, and 21.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 204, 256]), Model num_frames: 1000
2025-12-12 18:51:58 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 12.45 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.72 GiB is free. Including non-PyTorch memory, this process has 14.04 GiB memory in use. Of the allocated memory 13.64 GiB is allocated by PyTorch, and 21.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Current batch size: 2
2025-12-12 18:51:58 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 18:51:58 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 18:51:58 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:52:06 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:52:06 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 2: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:52:06 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 2): {'cpu_memory_mb': 6762.1484375, 'cpu_memory_gb': 6.603660583496094, 'cpu_vms_mb': 72047.8359375, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 18:52:06 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Current batch size: 1
2025-12-12 18:52:06 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:52:09 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:52:09 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:52:10 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 6762.1484375, 'cpu_memory_gb': 6.603660583496094, 'cpu_vms_mb': 72047.8359375, 'gpu_allocated_gb': 16.238706176, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 18:52:10 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Current batch size: 1
2025-12-12 18:52:10 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:52:22 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:52:22 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 4: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:52:22 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 4): {'cpu_memory_mb': 6762.1484375, 'cpu_memory_gb': 6.603660583496094, 'cpu_vms_mb': 72047.8359375, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 18:52:22 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Final batch size: 1
2025-12-12 18:52:22 [ERROR] [lib.training.pipeline:1640] Error training fold 3: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 112, in forward
    x = F.relu(self.bn3(self.conv3(x)))
                        ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 543, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:52:22 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 8f2ec2ff28ff47f68cb5984d5d51b042
2025-12-12 18:52:23 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 4/5 (20% sample)
2025-12-12 18:52:23 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 18:52:23 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 958dd04d229f4961abe57fe73da4ae03 (experiment: naive_cnn)
2025-12-12 18:52:23 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 4...
2025-12-12 18:52:25 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 18:52:25 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:52:30 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 144, 256]), Model num_frames: 1000
2025-12-12 18:52:30 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Current batch size: 2
2025-12-12 18:52:30 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 18:52:30 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 18:52:30 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:52:33 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:52:33 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:52:33 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 6762.1484375, 'cpu_memory_gb': 6.603660583496094, 'cpu_vms_mb': 72047.8359375, 'gpu_allocated_gb': 16.238706176, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 18:52:33 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Current batch size: 1
2025-12-12 18:52:33 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:52:46 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:52:46 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 4: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:52:46 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 4): {'cpu_memory_mb': 6761.15625, 'cpu_memory_gb': 6.602691650390625, 'cpu_vms_mb': 72046.8359375, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 18:52:46 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Current batch size: 1
2025-12-12 18:52:46 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:53:04 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:53:04 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 2: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:53:04 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 2): {'cpu_memory_mb': 7263.453125, 'cpu_memory_gb': 7.0932159423828125, 'cpu_vms_mb': 72549.1015625, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 18:53:04 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Final batch size: 1
2025-12-12 18:53:04 [ERROR] [lib.training.pipeline:1640] Error training fold 4: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 112, in forward
    x = F.relu(self.bn3(self.conv3(x)))
                        ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 543, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:53:04 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 958dd04d229f4961abe57fe73da4ae03
2025-12-12 18:53:05 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 5/5 (20% sample)
2025-12-12 18:53:05 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 18:53:05 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: b5541b934449440f930c05e665d7203d (experiment: naive_cnn)
2025-12-12 18:53:05 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 5...
2025-12-12 18:53:06 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 18:53:07 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:53:12 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 11.60 GiB. GPU 0 has a total capacity of 15.77 GiB of which 2.66 GiB is free. Including non-PyTorch memory, this process has 13.10 GiB memory in use. Of the allocated memory 12.70 GiB is allocated by PyTorch, and 18.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 190, 256]), Model num_frames: 1000
2025-12-12 18:53:12 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 11.60 GiB. GPU 0 has a total capacity of 15.77 GiB of which 2.66 GiB is free. Including non-PyTorch memory, this process has 13.10 GiB memory in use. Of the allocated memory 12.70 GiB is allocated by PyTorch, and 18.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 5, Current batch size: 2
2025-12-12 18:53:12 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 18:53:12 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 18:53:12 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:53:21 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 1.57 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.09 GiB is free. Including non-PyTorch memory, this process has 14.66 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 20.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 256, 206]), Model num_frames: 1000
2025-12-12 18:53:21 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 2: CUDA out of memory. Tried to allocate 1.57 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.09 GiB is free. Including non-PyTorch memory, this process has 14.66 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 20.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:53:21 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 2): {'cpu_memory_mb': 6762.19140625, 'cpu_memory_gb': 6.603702545166016, 'cpu_vms_mb': 72047.8359375, 'gpu_allocated_gb': 15.312398848, 'gpu_reserved_gb': 15.334375424, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 1.593966592}
2025-12-12 18:53:21 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 1.57 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.09 GiB is free. Including non-PyTorch memory, this process has 14.66 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 20.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 5, Current batch size: 1
2025-12-12 18:53:21 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:53:24 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:53:24 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:53:24 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 6762.19140625, 'cpu_memory_gb': 6.603702545166016, 'cpu_vms_mb': 72047.8359375, 'gpu_allocated_gb': 16.238706176, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 18:53:24 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 5, Current batch size: 1
2025-12-12 18:53:24 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:53:27 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:53:27 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:53:27 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 6762.19140625, 'cpu_memory_gb': 6.603702545166016, 'cpu_vms_mb': 72047.8359375, 'gpu_allocated_gb': 16.238706176, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 18:53:27 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 5, Final batch size: 1
2025-12-12 18:53:27 [ERROR] [lib.training.pipeline:1640] Error training fold 5: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 112, in forward
    x = F.relu(self.bn3(self.conv3(x)))
                        ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 543, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:53:27 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: b5541b934449440f930c05e665d7203d
2025-12-12 18:53:28 [INFO] [lib.training.pipeline:818] 
================================================================================
2025-12-12 18:53:28 [INFO] [lib.training.pipeline:819] Grid Search: Hyperparameter combination 2/18
2025-12-12 18:53:28 [INFO] [lib.training.pipeline:820] Parameters: {'learning_rate': 0.0001, 'weight_decay': 1e-05, 'batch_size': 2, 'num_epochs': 30}
2025-12-12 18:53:28 [INFO] [lib.training.pipeline:821] ================================================================================
2025-12-12 18:53:28 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 1/5 (20% sample)
2025-12-12 18:53:28 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 1 directory (clean mode)
2025-12-12 18:53:28 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 18:53:28 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 122bbd578d0d4733b966c6dc11abde67 (experiment: naive_cnn)
2025-12-12 18:53:28 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 1...
2025-12-12 18:53:30 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 18:53:30 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:53:35 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 144, 256]), Model num_frames: 1000
2025-12-12 18:53:35 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Current batch size: 2
2025-12-12 18:53:35 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 18:53:35 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 18:53:35 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:53:53 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:53:53 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 5: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:53:53 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 5): {'cpu_memory_mb': 6762.19140625, 'cpu_memory_gb': 6.603702545166016, 'cpu_vms_mb': 72047.8359375, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 18:53:53 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Current batch size: 1
2025-12-12 18:53:53 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:54:12 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:54:12 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 5: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:54:12 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 5): {'cpu_memory_mb': 7164.68359375, 'cpu_memory_gb': 6.996761322021484, 'cpu_vms_mb': 72450.49609375, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 18:54:12 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Current batch size: 1
2025-12-12 18:54:12 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:54:24 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 40.19 MiB is free. Including non-PyTorch memory, this process has 15.71 GiB memory in use. Of the allocated memory 15.26 GiB is allocated by PyTorch, and 75.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 194, 256]), Model num_frames: 1000
2025-12-12 18:54:24 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 3: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 40.19 MiB is free. Including non-PyTorch memory, this process has 15.71 GiB memory in use. Of the allocated memory 15.26 GiB is allocated by PyTorch, and 75.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:54:24 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 3): {'cpu_memory_mb': 7164.68359375, 'cpu_memory_gb': 6.996761322021484, 'cpu_vms_mb': 72450.49609375, 'gpu_allocated_gb': 16.387598848, 'gpu_reserved_gb': 16.466837504, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.461504512}
2025-12-12 18:54:24 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 40.19 MiB is free. Including non-PyTorch memory, this process has 15.71 GiB memory in use. Of the allocated memory 15.26 GiB is allocated by PyTorch, and 75.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Final batch size: 1
2025-12-12 18:54:24 [ERROR] [lib.training.pipeline:1640] Error training fold 1: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 40.19 MiB is free. Including non-PyTorch memory, this process has 15.71 GiB memory in use. Of the allocated memory 15.26 GiB is allocated by PyTorch, and 75.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 112, in forward
    x = F.relu(self.bn3(self.conv3(x)))
                        ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 543, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 40.19 MiB is free. Including non-PyTorch memory, this process has 15.71 GiB memory in use. Of the allocated memory 15.26 GiB is allocated by PyTorch, and 75.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:54:24 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 122bbd578d0d4733b966c6dc11abde67
2025-12-12 18:54:24 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 2/5 (20% sample)
2025-12-12 18:54:24 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 2 directory (clean mode)
2025-12-12 18:54:24 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 18:54:24 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 64629314280e465d9fe9f0dd07ba9aef (experiment: naive_cnn)
2025-12-12 18:54:24 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 2...
2025-12-12 18:54:26 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 18:54:26 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:54:32 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 11.72 GiB. GPU 0 has a total capacity of 15.77 GiB of which 2.52 GiB is free. Including non-PyTorch memory, this process has 13.23 GiB memory in use. Of the allocated memory 12.83 GiB is allocated by PyTorch, and 21.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:54:32 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 11.72 GiB. GPU 0 has a total capacity of 15.77 GiB of which 2.52 GiB is free. Including non-PyTorch memory, this process has 13.23 GiB memory in use. Of the allocated memory 12.83 GiB is allocated by PyTorch, and 21.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Current batch size: 2
2025-12-12 18:54:32 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 18:54:32 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 18:54:32 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:54:43 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:54:43 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 3: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:54:43 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 3): {'cpu_memory_mb': 7164.75390625, 'cpu_memory_gb': 6.996829986572266, 'cpu_vms_mb': 72450.49609375, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 18:54:43 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Current batch size: 1
2025-12-12 18:54:43 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:54:57 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:54:57 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 3: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:54:57 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 3): {'cpu_memory_mb': 8598.87109375, 'cpu_memory_gb': 8.397335052490234, 'cpu_vms_mb': 73884.5, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 18:54:57 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Current batch size: 1
2025-12-12 18:54:57 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:55:01 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:55:01 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:55:01 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 8598.87109375, 'cpu_memory_gb': 8.397335052490234, 'cpu_vms_mb': 73884.5, 'gpu_allocated_gb': 16.238706176, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 18:55:01 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Final batch size: 1
2025-12-12 18:55:01 [ERROR] [lib.training.pipeline:1640] Error training fold 2: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 112, in forward
    x = F.relu(self.bn3(self.conv3(x)))
                        ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 543, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:55:01 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 64629314280e465d9fe9f0dd07ba9aef
2025-12-12 18:55:01 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 3/5 (20% sample)
2025-12-12 18:55:01 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 3 directory (clean mode)
2025-12-12 18:55:01 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 18:55:01 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 451d670c51ed407d83703b3427e9c535 (experiment: naive_cnn)
2025-12-12 18:55:01 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 3...
2025-12-12 18:55:03 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 18:55:03 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:55:07 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 11.72 GiB. GPU 0 has a total capacity of 15.77 GiB of which 2.52 GiB is free. Including non-PyTorch memory, this process has 13.23 GiB memory in use. Of the allocated memory 12.83 GiB is allocated by PyTorch, and 21.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:55:07 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 11.72 GiB. GPU 0 has a total capacity of 15.77 GiB of which 2.52 GiB is free. Including non-PyTorch memory, this process has 13.23 GiB memory in use. Of the allocated memory 12.83 GiB is allocated by PyTorch, and 21.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Current batch size: 2
2025-12-12 18:55:07 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 18:55:07 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 18:55:07 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:55:16 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 40.19 MiB is free. Including non-PyTorch memory, this process has 15.71 GiB memory in use. Of the allocated memory 15.26 GiB is allocated by PyTorch, and 75.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 194, 256]), Model num_frames: 1000
2025-12-12 18:55:16 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 3: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 40.19 MiB is free. Including non-PyTorch memory, this process has 15.71 GiB memory in use. Of the allocated memory 15.26 GiB is allocated by PyTorch, and 75.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:55:17 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 3): {'cpu_memory_mb': 8598.98828125, 'cpu_memory_gb': 8.397449493408203, 'cpu_vms_mb': 73884.5, 'gpu_allocated_gb': 16.387598848, 'gpu_reserved_gb': 16.466837504, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.461504512}
2025-12-12 18:55:17 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 40.19 MiB is free. Including non-PyTorch memory, this process has 15.71 GiB memory in use. Of the allocated memory 15.26 GiB is allocated by PyTorch, and 75.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Current batch size: 1
2025-12-12 18:55:17 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:55:40 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:55:40 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 7: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:55:40 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 7): {'cpu_memory_mb': 8599.53515625, 'cpu_memory_gb': 8.39798355102539, 'cpu_vms_mb': 73884.5, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 18:55:40 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Current batch size: 1
2025-12-12 18:55:40 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:55:46 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 256, 256]), Model num_frames: 1000
2025-12-12 18:55:46 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 1: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:55:46 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 1): {'cpu_memory_mb': 8599.53515625, 'cpu_memory_gb': 8.39798355102539, 'cpu_vms_mb': 73884.5, 'gpu_allocated_gb': 14.83005696, 'gpu_reserved_gb': 14.873001984, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 2.055340032}
2025-12-12 18:55:46 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Final batch size: 1
2025-12-12 18:55:46 [ERROR] [lib.training.pipeline:1640] Error training fold 3: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 110, in forward
    x = F.relu(self.bn2(self.conv2(x)))
                        ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 543, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:55:46 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 451d670c51ed407d83703b3427e9c535
2025-12-12 18:55:47 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 4/5 (20% sample)
2025-12-12 18:55:47 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 4 directory (clean mode)
2025-12-12 18:55:47 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 18:55:47 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 336ecac3021c4fb2b6f68c69a683d518 (experiment: naive_cnn)
2025-12-12 18:55:47 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 4...
2025-12-12 18:55:48 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 18:55:48 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:55:55 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 11.72 GiB. GPU 0 has a total capacity of 15.77 GiB of which 2.52 GiB is free. Including non-PyTorch memory, this process has 13.23 GiB memory in use. Of the allocated memory 12.83 GiB is allocated by PyTorch, and 21.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:55:55 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 11.72 GiB. GPU 0 has a total capacity of 15.77 GiB of which 2.52 GiB is free. Including non-PyTorch memory, this process has 13.23 GiB memory in use. Of the allocated memory 12.83 GiB is allocated by PyTorch, and 21.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Current batch size: 2
2025-12-12 18:55:55 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 18:55:55 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 18:55:55 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:55:57 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:55:57 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:55:57 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 8599.53515625, 'cpu_memory_gb': 8.39798355102539, 'cpu_vms_mb': 73884.5, 'gpu_allocated_gb': 16.238706176, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 18:55:57 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Current batch size: 1
2025-12-12 18:55:57 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:56:00 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:56:00 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:56:00 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 8599.53515625, 'cpu_memory_gb': 8.39798355102539, 'cpu_vms_mb': 73884.5, 'gpu_allocated_gb': 16.238706176, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 18:56:00 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Current batch size: 1
2025-12-12 18:56:00 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:56:30 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:56:30 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 7: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:56:31 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 7): {'cpu_memory_mb': 8601.74609375, 'cpu_memory_gb': 8.400142669677734, 'cpu_vms_mb': 73886.5, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 18:56:31 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Final batch size: 1
2025-12-12 18:56:31 [ERROR] [lib.training.pipeline:1640] Error training fold 4: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 112, in forward
    x = F.relu(self.bn3(self.conv3(x)))
                        ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 543, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:56:31 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 336ecac3021c4fb2b6f68c69a683d518
2025-12-12 18:56:31 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 5/5 (20% sample)
2025-12-12 18:56:31 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 5 directory (clean mode)
2025-12-12 18:56:31 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 18:56:31 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: e84b2c8841e249c5a31faf2eb82808ef (experiment: naive_cnn)
2025-12-12 18:56:31 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 5...
2025-12-12 18:56:33 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 18:56:33 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:56:36 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 144, 256]), Model num_frames: 1000
2025-12-12 18:56:36 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 5, Current batch size: 2
2025-12-12 18:56:36 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 18:56:36 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 18:56:37 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:56:58 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:56:58 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 5: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:56:58 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 5): {'cpu_memory_mb': 8603.734375, 'cpu_memory_gb': 8.402084350585938, 'cpu_vms_mb': 73888.5, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 18:56:58 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 5, Current batch size: 1
2025-12-12 18:56:58 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:57:04 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:57:04 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 2: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:57:04 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 2): {'cpu_memory_mb': 8603.734375, 'cpu_memory_gb': 8.402084350585938, 'cpu_vms_mb': 73888.5, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 18:57:04 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 5, Current batch size: 1
2025-12-12 18:57:04 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:57:08 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 7.81 GiB. GPU 0 has a total capacity of 15.77 GiB of which 6.80 GiB is free. Including non-PyTorch memory, this process has 8.96 GiB memory in use. Of the allocated memory 8.56 GiB is allocated by PyTorch, and 16.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 256, 256]), Model num_frames: 1000
2025-12-12 18:57:08 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 7.81 GiB. GPU 0 has a total capacity of 15.77 GiB of which 6.80 GiB is free. Including non-PyTorch memory, this process has 8.96 GiB memory in use. Of the allocated memory 8.56 GiB is allocated by PyTorch, and 16.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 5, Final batch size: 1
2025-12-12 18:57:08 [ERROR] [lib.training.pipeline:1640] Error training fold 5: CUDA out of memory. Tried to allocate 7.81 GiB. GPU 0 has a total capacity of 15.77 GiB of which 6.80 GiB is free. Including non-PyTorch memory, this process has 8.96 GiB memory in use. Of the allocated memory 8.56 GiB is allocated by PyTorch, and 16.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 330, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 108, in forward
    x = F.relu(self.bn1(self.conv1(x)))
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
           ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/functional.py", line 2813, in batch_norm
    return torch.batch_norm(
           ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.81 GiB. GPU 0 has a total capacity of 15.77 GiB of which 6.80 GiB is free. Including non-PyTorch memory, this process has 8.96 GiB memory in use. Of the allocated memory 8.56 GiB is allocated by PyTorch, and 16.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:57:08 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: e84b2c8841e249c5a31faf2eb82808ef
2025-12-12 18:57:08 [INFO] [lib.training.pipeline:818] 
================================================================================
2025-12-12 18:57:08 [INFO] [lib.training.pipeline:819] Grid Search: Hyperparameter combination 3/18
2025-12-12 18:57:08 [INFO] [lib.training.pipeline:820] Parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0001, 'batch_size': 2, 'num_epochs': 20}
2025-12-12 18:57:08 [INFO] [lib.training.pipeline:821] ================================================================================
2025-12-12 18:57:08 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 1/5 (20% sample)
2025-12-12 18:57:08 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 1 directory (clean mode)
2025-12-12 18:57:08 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 18:57:08 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 279734efb1384dd2a3c8562c66c374ec (experiment: naive_cnn)
2025-12-12 18:57:08 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 1...
2025-12-12 18:57:10 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 18:57:10 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:57:15 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 12.45 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.72 GiB is free. Including non-PyTorch memory, this process has 14.04 GiB memory in use. Of the allocated memory 13.64 GiB is allocated by PyTorch, and 21.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 204, 256]), Model num_frames: 1000
2025-12-12 18:57:15 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 12.45 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.72 GiB is free. Including non-PyTorch memory, this process has 14.04 GiB memory in use. Of the allocated memory 13.64 GiB is allocated by PyTorch, and 21.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Current batch size: 2
2025-12-12 18:57:15 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 18:57:15 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 18:57:15 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:57:40 [INFO] [lib.training.trainer:419] Epoch 1, Batch 10/523, Loss: 0.6938
2025-12-12 18:57:46 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:57:46 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 11: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:57:46 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 11): {'cpu_memory_mb': 8602.76171875, 'cpu_memory_gb': 8.401134490966797, 'cpu_vms_mb': 73887.5, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 18:57:46 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Current batch size: 1
2025-12-12 18:57:46 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:57:50 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:57:50 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:57:50 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 8602.76171875, 'cpu_memory_gb': 8.401134490966797, 'cpu_vms_mb': 73887.5, 'gpu_allocated_gb': 16.238706176, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 18:57:50 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Current batch size: 1
2025-12-12 18:57:50 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:58:00 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:58:00 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 1: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:58:00 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 1): {'cpu_memory_mb': 8602.76171875, 'cpu_memory_gb': 8.401134490966797, 'cpu_vms_mb': 73887.5, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 18:58:00 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Final batch size: 1
2025-12-12 18:58:00 [ERROR] [lib.training.pipeline:1640] Error training fold 1: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 112, in forward
    x = F.relu(self.bn3(self.conv3(x)))
                        ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 543, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:58:00 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 279734efb1384dd2a3c8562c66c374ec
2025-12-12 18:58:01 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 2/5 (20% sample)
2025-12-12 18:58:01 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 2 directory (clean mode)
2025-12-12 18:58:01 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 18:58:01 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 13122b7ecc224e5ca4c044d91a262cc0 (experiment: naive_cnn)
2025-12-12 18:58:01 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 2...
2025-12-12 18:58:02 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 18:58:02 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:58:08 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 15.62 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.87 GiB is free. Including non-PyTorch memory, this process has 1.89 GiB memory in use. Of the allocated memory 1.48 GiB is allocated by PyTorch, and 26.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 256, 256]), Model num_frames: 1000
2025-12-12 18:58:08 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 15.62 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.87 GiB is free. Including non-PyTorch memory, this process has 1.89 GiB memory in use. Of the allocated memory 1.48 GiB is allocated by PyTorch, and 26.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Current batch size: 2
2025-12-12 18:58:08 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 18:58:08 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 18:58:08 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:58:14 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:58:14 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 1: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:58:14 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 1): {'cpu_memory_mb': 8601.76171875, 'cpu_memory_gb': 8.400157928466797, 'cpu_vms_mb': 73886.5, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 18:58:14 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Current batch size: 1
2025-12-12 18:58:14 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:58:22 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 1.57 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.07 GiB is free. Including non-PyTorch memory, this process has 14.68 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 40.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 256, 206]), Model num_frames: 1000
2025-12-12 18:58:22 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 2: CUDA out of memory. Tried to allocate 1.57 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.07 GiB is free. Including non-PyTorch memory, this process has 14.68 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 40.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:58:22 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 2): {'cpu_memory_mb': 8601.76171875, 'cpu_memory_gb': 8.400157928466797, 'cpu_vms_mb': 73886.5, 'gpu_allocated_gb': 15.312398848, 'gpu_reserved_gb': 15.355346944, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 1.572995072}
2025-12-12 18:58:22 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 1.57 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.07 GiB is free. Including non-PyTorch memory, this process has 14.68 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 40.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Current batch size: 1
2025-12-12 18:58:22 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:58:33 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:58:33 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 3: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:58:33 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 3): {'cpu_memory_mb': 8601.76171875, 'cpu_memory_gb': 8.400157928466797, 'cpu_vms_mb': 73886.5, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 18:58:33 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Final batch size: 1
2025-12-12 18:58:33 [ERROR] [lib.training.pipeline:1640] Error training fold 2: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 112, in forward
    x = F.relu(self.bn3(self.conv3(x)))
                        ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 543, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:58:33 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 13122b7ecc224e5ca4c044d91a262cc0
2025-12-12 18:58:33 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 3/5 (20% sample)
2025-12-12 18:58:33 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 3 directory (clean mode)
2025-12-12 18:58:33 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 18:58:34 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: fff2718b9eb64357a07ae1d1e028678c (experiment: naive_cnn)
2025-12-12 18:58:34 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 3...
2025-12-12 18:58:35 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 18:58:35 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:58:40 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 15.62 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.87 GiB is free. Including non-PyTorch memory, this process has 1.89 GiB memory in use. Of the allocated memory 1.48 GiB is allocated by PyTorch, and 26.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 256, 256]), Model num_frames: 1000
2025-12-12 18:58:40 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 15.62 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.87 GiB is free. Including non-PyTorch memory, this process has 1.89 GiB memory in use. Of the allocated memory 1.48 GiB is allocated by PyTorch, and 26.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Current batch size: 2
2025-12-12 18:58:40 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 18:58:40 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 18:58:40 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:58:42 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 204, 256]), Model num_frames: 1000
2025-12-12 18:58:42 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:58:43 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 8601.76171875, 'cpu_memory_gb': 8.400157928466797, 'cpu_vms_mb': 73886.5, 'gpu_allocated_gb': 15.163506176, 'gpu_reserved_gb': 15.208546304, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 1.719795712}
2025-12-12 18:58:43 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Current batch size: 1
2025-12-12 18:58:43 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:58:50 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:58:50 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 2: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:58:50 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 2): {'cpu_memory_mb': 8601.76171875, 'cpu_memory_gb': 8.400157928466797, 'cpu_vms_mb': 73886.5, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 18:58:50 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Current batch size: 1
2025-12-12 18:58:50 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:59:02 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:59:02 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 4: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:59:02 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 4): {'cpu_memory_mb': 8601.76171875, 'cpu_memory_gb': 8.400157928466797, 'cpu_vms_mb': 73886.5, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 18:59:02 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Final batch size: 1
2025-12-12 18:59:02 [ERROR] [lib.training.pipeline:1640] Error training fold 3: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 112, in forward
    x = F.relu(self.bn3(self.conv3(x)))
                        ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 543, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:59:02 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: fff2718b9eb64357a07ae1d1e028678c
2025-12-12 18:59:02 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 4/5 (20% sample)
2025-12-12 18:59:02 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 4 directory (clean mode)
2025-12-12 18:59:02 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 18:59:02 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: d0df6cb4b5db42e3a7c382845cfe8192 (experiment: naive_cnn)
2025-12-12 18:59:02 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 4...
2025-12-12 18:59:04 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 18:59:04 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:59:08 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 144, 256]), Model num_frames: 1000
2025-12-12 18:59:08 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Current batch size: 2
2025-12-12 18:59:08 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 18:59:08 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 18:59:08 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:59:27 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 256, 256]), Model num_frames: 1000
2025-12-12 18:59:27 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 2: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:59:27 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 2): {'cpu_memory_mb': 10543.4296875, 'cpu_memory_gb': 10.296318054199219, 'cpu_vms_mb': 75828.80859375, 'gpu_allocated_gb': 14.83005696, 'gpu_reserved_gb': 14.873001984, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 2.055340032}
2025-12-12 18:59:27 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Current batch size: 1
2025-12-12 18:59:27 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:59:29 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:59:29 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:59:29 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 9351.19921875, 'cpu_memory_gb': 9.132030487060547, 'cpu_vms_mb': 74636.41796875, 'gpu_allocated_gb': 16.238706176, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 18:59:29 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Current batch size: 1
2025-12-12 18:59:29 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:59:38 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 256, 256]), Model num_frames: 1000
2025-12-12 18:59:38 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 2: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:59:38 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 2): {'cpu_memory_mb': 8601.203125, 'cpu_memory_gb': 8.399612426757812, 'cpu_vms_mb': 73886.41796875, 'gpu_allocated_gb': 14.83005696, 'gpu_reserved_gb': 14.873001984, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 2.055340032}
2025-12-12 18:59:38 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Final batch size: 1
2025-12-12 18:59:38 [ERROR] [lib.training.pipeline:1640] Error training fold 4: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 110, in forward
    x = F.relu(self.bn2(self.conv2(x)))
                        ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 543, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:59:38 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: d0df6cb4b5db42e3a7c382845cfe8192
2025-12-12 18:59:38 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 5/5 (20% sample)
2025-12-12 18:59:38 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 5 directory (clean mode)
2025-12-12 18:59:38 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 18:59:38 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 23950edc0c3b461eb8ae1082155c260c (experiment: naive_cnn)
2025-12-12 18:59:38 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 5...
2025-12-12 18:59:40 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 18:59:40 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:59:44 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 144, 256]), Model num_frames: 1000
2025-12-12 18:59:44 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 5, Current batch size: 2
2025-12-12 18:59:44 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 18:59:44 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 18:59:44 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:59:46 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:59:46 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:59:46 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 8601.203125, 'cpu_memory_gb': 8.399612426757812, 'cpu_vms_mb': 73886.41796875, 'gpu_allocated_gb': 16.238706176, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 18:59:46 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 5, Current batch size: 1
2025-12-12 18:59:46 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 18:59:57 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 18:59:57 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 4: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 18:59:57 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 4): {'cpu_memory_mb': 8601.203125, 'cpu_memory_gb': 8.399612426757812, 'cpu_vms_mb': 73886.41796875, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 18:59:57 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 5, Current batch size: 1
2025-12-12 18:59:57 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:00:13 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 256, 256]), Model num_frames: 1000
2025-12-12 19:00:13 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 4: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:00:14 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 4): {'cpu_memory_mb': 8601.2109375, 'cpu_memory_gb': 8.399620056152344, 'cpu_vms_mb': 73886.41796875, 'gpu_allocated_gb': 14.83005696, 'gpu_reserved_gb': 14.873001984, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 2.055340032}
2025-12-12 19:00:14 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 5, Final batch size: 1
2025-12-12 19:00:14 [ERROR] [lib.training.pipeline:1640] Error training fold 5: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 110, in forward
    x = F.relu(self.bn2(self.conv2(x)))
                        ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 543, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:00:14 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 23950edc0c3b461eb8ae1082155c260c
2025-12-12 19:00:14 [INFO] [lib.training.pipeline:818] 
================================================================================
2025-12-12 19:00:14 [INFO] [lib.training.pipeline:819] Grid Search: Hyperparameter combination 4/18
2025-12-12 19:00:14 [INFO] [lib.training.pipeline:820] Parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0001, 'batch_size': 2, 'num_epochs': 30}
2025-12-12 19:00:14 [INFO] [lib.training.pipeline:821] ================================================================================
2025-12-12 19:00:14 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 1/5 (20% sample)
2025-12-12 19:00:14 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 1 directory (clean mode)
2025-12-12 19:00:14 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 19:00:14 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: b74af5e132284cd3a33619284c2dd018 (experiment: naive_cnn)
2025-12-12 19:00:14 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 1...
2025-12-12 19:00:16 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 19:00:16 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:00:21 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 11.72 GiB. GPU 0 has a total capacity of 15.77 GiB of which 2.52 GiB is free. Including non-PyTorch memory, this process has 13.23 GiB memory in use. Of the allocated memory 12.83 GiB is allocated by PyTorch, and 21.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:00:21 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 11.72 GiB. GPU 0 has a total capacity of 15.77 GiB of which 2.52 GiB is free. Including non-PyTorch memory, this process has 13.23 GiB memory in use. Of the allocated memory 12.83 GiB is allocated by PyTorch, and 21.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Current batch size: 2
2025-12-12 19:00:21 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 19:00:21 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 19:00:21 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:00:31 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 1.59 GiB. GPU 0 has a total capacity of 15.77 GiB of which 940.19 MiB is free. Including non-PyTorch memory, this process has 14.84 GiB memory in use. Of the allocated memory 14.40 GiB is allocated by PyTorch, and 59.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 208, 256]), Model num_frames: 1000
2025-12-12 19:00:31 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 3: CUDA out of memory. Tried to allocate 1.59 GiB. GPU 0 has a total capacity of 15.77 GiB of which 940.19 MiB is free. Including non-PyTorch memory, this process has 14.84 GiB memory in use. Of the allocated memory 14.40 GiB is allocated by PyTorch, and 59.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:00:31 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 3): {'cpu_memory_mb': 8601.2109375, 'cpu_memory_gb': 8.399620056152344, 'cpu_vms_mb': 73886.41796875, 'gpu_allocated_gb': 15.460878848, 'gpu_reserved_gb': 15.523119104, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 1.405222912}
2025-12-12 19:00:31 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 1.59 GiB. GPU 0 has a total capacity of 15.77 GiB of which 940.19 MiB is free. Including non-PyTorch memory, this process has 14.84 GiB memory in use. Of the allocated memory 14.40 GiB is allocated by PyTorch, and 59.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Current batch size: 1
2025-12-12 19:00:31 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:00:36 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 720.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 192.00 KiB is free. Including non-PyTorch memory, this process has 15.75 GiB memory in use. Of the allocated memory 15.33 GiB is allocated by PyTorch, and 41.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 186, 256]), Model num_frames: 1000
2025-12-12 19:00:36 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 1: CUDA out of memory. Tried to allocate 720.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 192.00 KiB is free. Including non-PyTorch memory, this process has 15.75 GiB memory in use. Of the allocated memory 15.33 GiB is allocated by PyTorch, and 41.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:00:36 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 1): {'cpu_memory_mb': 8601.2421875, 'cpu_memory_gb': 8.399650573730469, 'cpu_vms_mb': 73886.41796875, 'gpu_allocated_gb': 16.465570304, 'gpu_reserved_gb': 16.508780544, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.419561472}
2025-12-12 19:00:36 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 720.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 192.00 KiB is free. Including non-PyTorch memory, this process has 15.75 GiB memory in use. Of the allocated memory 15.33 GiB is allocated by PyTorch, and 41.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Current batch size: 1
2025-12-12 19:00:36 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:00:43 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:00:43 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 1: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:00:44 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 1): {'cpu_memory_mb': 8601.2421875, 'cpu_memory_gb': 8.399650573730469, 'cpu_vms_mb': 73886.41796875, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 19:00:44 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Final batch size: 1
2025-12-12 19:00:44 [ERROR] [lib.training.pipeline:1640] Error training fold 1: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 112, in forward
    x = F.relu(self.bn3(self.conv3(x)))
                        ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 543, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:00:44 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: b74af5e132284cd3a33619284c2dd018
2025-12-12 19:00:44 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 2/5 (20% sample)
2025-12-12 19:00:44 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 2 directory (clean mode)
2025-12-12 19:00:44 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 19:00:44 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 10f2a5af95214d0ba24ff9b8d0649394 (experiment: naive_cnn)
2025-12-12 19:00:44 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 2...
2025-12-12 19:00:45 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 19:00:45 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:00:50 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 11.72 GiB. GPU 0 has a total capacity of 15.77 GiB of which 2.52 GiB is free. Including non-PyTorch memory, this process has 13.23 GiB memory in use. Of the allocated memory 12.83 GiB is allocated by PyTorch, and 21.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:00:50 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 11.72 GiB. GPU 0 has a total capacity of 15.77 GiB of which 2.52 GiB is free. Including non-PyTorch memory, this process has 13.23 GiB memory in use. Of the allocated memory 12.83 GiB is allocated by PyTorch, and 21.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Current batch size: 2
2025-12-12 19:00:50 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 19:00:50 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 19:00:50 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:00:56 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 256, 256]), Model num_frames: 1000
2025-12-12 19:00:56 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 1: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:00:56 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 1): {'cpu_memory_mb': 8601.2421875, 'cpu_memory_gb': 8.399650573730469, 'cpu_vms_mb': 73886.41796875, 'gpu_allocated_gb': 14.83005696, 'gpu_reserved_gb': 14.873001984, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 2.055340032}
2025-12-12 19:00:56 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Current batch size: 1
2025-12-12 19:00:56 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:00:59 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:00:59 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:00:59 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 8601.2421875, 'cpu_memory_gb': 8.399650573730469, 'cpu_vms_mb': 73886.41796875, 'gpu_allocated_gb': 16.238706176, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 19:00:59 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Current batch size: 1
2025-12-12 19:00:59 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:01:11 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 658.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 20.19 MiB is free. Including non-PyTorch memory, this process has 15.73 GiB memory in use. Of the allocated memory 15.30 GiB is allocated by PyTorch, and 60.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Final batch size: 1
2025-12-12 19:01:11 [ERROR] [lib.training.pipeline:1640] Error training fold 2: CUDA out of memory. Tried to allocate 658.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 20.19 MiB is free. Including non-PyTorch memory, this process has 15.73 GiB memory in use. Of the allocated memory 15.30 GiB is allocated by PyTorch, and 60.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 379, in train_one_epoch
    scaler.scale(loss).backward()
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 658.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 20.19 MiB is free. Including non-PyTorch memory, this process has 15.73 GiB memory in use. Of the allocated memory 15.30 GiB is allocated by PyTorch, and 60.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:01:11 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 10f2a5af95214d0ba24ff9b8d0649394
2025-12-12 19:01:11 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 3/5 (20% sample)
2025-12-12 19:01:11 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 3 directory (clean mode)
2025-12-12 19:01:11 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 19:01:11 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 3cfea156cd92466e83f185709666300a (experiment: naive_cnn)
2025-12-12 19:01:11 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 3...
2025-12-12 19:01:13 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 19:01:13 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:01:17 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 144, 256]), Model num_frames: 1000
2025-12-12 19:01:17 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Current batch size: 2
2025-12-12 19:01:17 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 19:01:17 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 19:01:17 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:01:27 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:01:27 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 3: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:01:27 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 3): {'cpu_memory_mb': 8601.58203125, 'cpu_memory_gb': 8.399982452392578, 'cpu_vms_mb': 73886.41796875, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 19:01:27 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Current batch size: 1
2025-12-12 19:01:27 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:01:33 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:01:33 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 2: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:01:34 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 2): {'cpu_memory_mb': 8601.58203125, 'cpu_memory_gb': 8.399982452392578, 'cpu_vms_mb': 73886.41796875, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 19:01:34 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Current batch size: 1
2025-12-12 19:01:34 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:01:36 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:01:36 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:01:36 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 8601.58203125, 'cpu_memory_gb': 8.399982452392578, 'cpu_vms_mb': 73886.41796875, 'gpu_allocated_gb': 16.238706176, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 19:01:36 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Final batch size: 1
2025-12-12 19:01:36 [ERROR] [lib.training.pipeline:1640] Error training fold 3: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 112, in forward
    x = F.relu(self.bn3(self.conv3(x)))
                        ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 543, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:01:36 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 3cfea156cd92466e83f185709666300a
2025-12-12 19:01:37 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 4/5 (20% sample)
2025-12-12 19:01:37 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 4 directory (clean mode)
2025-12-12 19:01:37 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 19:01:37 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 3de856306b4146f6b6e5c68f1e66e6da (experiment: naive_cnn)
2025-12-12 19:01:37 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 4...
2025-12-12 19:01:39 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 19:01:39 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:01:44 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 144, 256]), Model num_frames: 1000
2025-12-12 19:01:44 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Current batch size: 2
2025-12-12 19:01:44 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 19:01:44 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 19:01:44 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:02:03 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 1.71 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.54 GiB is free. Including non-PyTorch memory, this process has 14.21 GiB memory in use. Of the allocated memory 13.80 GiB is allocated by PyTorch, and 36.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 224, 256]), Model num_frames: 1000
2025-12-12 19:02:03 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 6: CUDA out of memory. Tried to allocate 1.71 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.54 GiB is free. Including non-PyTorch memory, this process has 14.21 GiB memory in use. Of the allocated memory 13.80 GiB is allocated by PyTorch, and 36.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:02:03 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 6): {'cpu_memory_mb': 8601.71484375, 'cpu_memory_gb': 8.40011215209961, 'cpu_vms_mb': 73886.41796875, 'gpu_allocated_gb': 14.813709824, 'gpu_reserved_gb': 14.852030464, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 2.076311552}
2025-12-12 19:02:03 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 1.71 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.54 GiB is free. Including non-PyTorch memory, this process has 14.21 GiB memory in use. Of the allocated memory 13.80 GiB is allocated by PyTorch, and 36.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Current batch size: 1
2025-12-12 19:02:03 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:02:12 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:02:12 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 1: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:02:12 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 1): {'cpu_memory_mb': 8601.71484375, 'cpu_memory_gb': 8.40011215209961, 'cpu_vms_mb': 73886.41796875, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 19:02:12 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Current batch size: 1
2025-12-12 19:02:12 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:02:17 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:02:17 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 1: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:02:17 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 1): {'cpu_memory_mb': 8601.71484375, 'cpu_memory_gb': 8.40011215209961, 'cpu_vms_mb': 73886.41796875, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 19:02:17 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Final batch size: 1
2025-12-12 19:02:17 [ERROR] [lib.training.pipeline:1640] Error training fold 4: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 112, in forward
    x = F.relu(self.bn3(self.conv3(x)))
                        ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 543, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:02:17 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 3de856306b4146f6b6e5c68f1e66e6da
2025-12-12 19:02:17 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 5/5 (20% sample)
2025-12-12 19:02:17 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 5 directory (clean mode)
2025-12-12 19:02:17 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 19:02:17 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: ceb51e0f74da4e7c899b68c1401fb481 (experiment: naive_cnn)
2025-12-12 19:02:17 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 5...
2025-12-12 19:02:19 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 19:02:19 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:02:26 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 144, 256]), Model num_frames: 1000
2025-12-12 19:02:26 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 5, Current batch size: 2
2025-12-12 19:02:26 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 19:02:26 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 19:02:26 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:02:48 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:02:48 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 6: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:02:48 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 6): {'cpu_memory_mb': 8601.71484375, 'cpu_memory_gb': 8.40011215209961, 'cpu_vms_mb': 73886.41796875, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 19:02:48 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 5, Current batch size: 1
2025-12-12 19:02:48 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:03:07 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:03:07 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 6: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:03:08 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 6): {'cpu_memory_mb': 8601.77734375, 'cpu_memory_gb': 8.40017318725586, 'cpu_vms_mb': 73886.41796875, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 19:03:08 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 5, Current batch size: 1
2025-12-12 19:03:08 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:03:15 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 204, 256]), Model num_frames: 1000
2025-12-12 19:03:15 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 2: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:03:15 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 2): {'cpu_memory_mb': 8601.77734375, 'cpu_memory_gb': 8.40017318725586, 'cpu_vms_mb': 73886.41796875, 'gpu_allocated_gb': 15.163918848, 'gpu_reserved_gb': 15.208546304, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 1.719795712}
2025-12-12 19:03:15 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 5, Final batch size: 1
2025-12-12 19:03:15 [ERROR] [lib.training.pipeline:1640] Error training fold 5: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 110, in forward
    x = F.relu(self.bn2(self.conv2(x)))
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/functional.py", line 1697, in relu
    result = torch.relu(input)
             ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:03:15 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: ceb51e0f74da4e7c899b68c1401fb481
2025-12-12 19:03:16 [INFO] [lib.training.pipeline:818] 
================================================================================
2025-12-12 19:03:16 [INFO] [lib.training.pipeline:819] Grid Search: Hyperparameter combination 5/18
2025-12-12 19:03:16 [INFO] [lib.training.pipeline:820] Parameters: {'learning_rate': 0.0001, 'weight_decay': 0.001, 'batch_size': 2, 'num_epochs': 20}
2025-12-12 19:03:16 [INFO] [lib.training.pipeline:821] ================================================================================
2025-12-12 19:03:16 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 1/5 (20% sample)
2025-12-12 19:03:16 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 1 directory (clean mode)
2025-12-12 19:03:16 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 19:03:16 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 5371dd4e8aa74186934d68f8dca404e6 (experiment: naive_cnn)
2025-12-12 19:03:16 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 1...
2025-12-12 19:03:18 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 19:03:18 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:03:25 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 15.62 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.87 GiB is free. Including non-PyTorch memory, this process has 1.89 GiB memory in use. Of the allocated memory 1.48 GiB is allocated by PyTorch, and 26.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 256, 256]), Model num_frames: 1000
2025-12-12 19:03:25 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 15.62 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.87 GiB is free. Including non-PyTorch memory, this process has 1.89 GiB memory in use. Of the allocated memory 1.48 GiB is allocated by PyTorch, and 26.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Current batch size: 2
2025-12-12 19:03:25 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 19:03:25 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 19:03:25 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:03:44 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 204, 256]), Model num_frames: 1000
2025-12-12 19:03:44 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 6: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:03:45 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 6): {'cpu_memory_mb': 8601.77734375, 'cpu_memory_gb': 8.40017318725586, 'cpu_vms_mb': 73886.41796875, 'gpu_allocated_gb': 15.163918848, 'gpu_reserved_gb': 15.208546304, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 1.719795712}
2025-12-12 19:03:45 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Current batch size: 1
2025-12-12 19:03:45 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:03:53 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:03:53 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 2: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:03:53 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 2): {'cpu_memory_mb': 8601.77734375, 'cpu_memory_gb': 8.40017318725586, 'cpu_vms_mb': 73886.41796875, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 19:03:53 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Current batch size: 1
2025-12-12 19:03:53 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:03:56 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 40.19 MiB is free. Including non-PyTorch memory, this process has 15.71 GiB memory in use. Of the allocated memory 15.26 GiB is allocated by PyTorch, and 75.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 194, 256]), Model num_frames: 1000
2025-12-12 19:03:56 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 40.19 MiB is free. Including non-PyTorch memory, this process has 15.71 GiB memory in use. Of the allocated memory 15.26 GiB is allocated by PyTorch, and 75.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:03:56 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 8601.77734375, 'cpu_memory_gb': 8.40017318725586, 'cpu_vms_mb': 73886.41796875, 'gpu_allocated_gb': 16.387186176, 'gpu_reserved_gb': 16.466837504, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.461504512}
2025-12-12 19:03:56 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 40.19 MiB is free. Including non-PyTorch memory, this process has 15.71 GiB memory in use. Of the allocated memory 15.26 GiB is allocated by PyTorch, and 75.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Final batch size: 1
2025-12-12 19:03:56 [ERROR] [lib.training.pipeline:1640] Error training fold 1: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 40.19 MiB is free. Including non-PyTorch memory, this process has 15.71 GiB memory in use. Of the allocated memory 15.26 GiB is allocated by PyTorch, and 75.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 112, in forward
    x = F.relu(self.bn3(self.conv3(x)))
                        ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 543, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 40.19 MiB is free. Including non-PyTorch memory, this process has 15.71 GiB memory in use. Of the allocated memory 15.26 GiB is allocated by PyTorch, and 75.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:03:56 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 5371dd4e8aa74186934d68f8dca404e6
2025-12-12 19:03:57 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 2/5 (20% sample)
2025-12-12 19:03:57 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 2 directory (clean mode)
2025-12-12 19:03:57 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 19:03:57 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 7df1426f4a994a8c827f1873717892c1 (experiment: naive_cnn)
2025-12-12 19:03:57 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 2...
2025-12-12 19:03:58 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 19:03:58 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:04:04 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 15.62 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.87 GiB is free. Including non-PyTorch memory, this process has 1.89 GiB memory in use. Of the allocated memory 1.48 GiB is allocated by PyTorch, and 26.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 256, 256]), Model num_frames: 1000
2025-12-12 19:04:04 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 15.62 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.87 GiB is free. Including non-PyTorch memory, this process has 1.89 GiB memory in use. Of the allocated memory 1.48 GiB is allocated by PyTorch, and 26.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Current batch size: 2
2025-12-12 19:04:04 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 19:04:04 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 19:04:04 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:04:07 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:04:07 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:04:07 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 8601.77734375, 'cpu_memory_gb': 8.40017318725586, 'cpu_vms_mb': 73886.41796875, 'gpu_allocated_gb': 16.238706176, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 19:04:07 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Current batch size: 1
2025-12-12 19:04:07 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:04:29 [INFO] [lib.training.trainer:419] Epoch 1, Batch 10/524, Loss: 0.6914
2025-12-12 19:04:34 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 256, 256]), Model num_frames: 1000
2025-12-12 19:04:34 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 11: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:04:35 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 11): {'cpu_memory_mb': 8601.77734375, 'cpu_memory_gb': 8.40017318725586, 'cpu_vms_mb': 73886.41796875, 'gpu_allocated_gb': 14.83005696, 'gpu_reserved_gb': 14.873001984, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 2.055340032}
2025-12-12 19:04:35 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Current batch size: 1
2025-12-12 19:04:35 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:04:44 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:04:44 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 3: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:04:44 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 3): {'cpu_memory_mb': 8601.77734375, 'cpu_memory_gb': 8.40017318725586, 'cpu_vms_mb': 73886.41796875, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 19:04:44 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Final batch size: 1
2025-12-12 19:04:44 [ERROR] [lib.training.pipeline:1640] Error training fold 2: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 112, in forward
    x = F.relu(self.bn3(self.conv3(x)))
                        ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 543, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:04:44 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 7df1426f4a994a8c827f1873717892c1
2025-12-12 19:04:44 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 3/5 (20% sample)
2025-12-12 19:04:44 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 3 directory (clean mode)
2025-12-12 19:04:44 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 19:04:44 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 38797d8f4fce479eae4cb3dfec974ade (experiment: naive_cnn)
2025-12-12 19:04:45 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 3...
2025-12-12 19:04:46 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 19:04:46 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:05:02 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 10.01 GiB. GPU 0 has a total capacity of 15.77 GiB of which 4.39 GiB is free. Including non-PyTorch memory, this process has 11.36 GiB memory in use. Of the allocated memory 10.97 GiB is allocated by PyTorch, and 15.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 164, 256]), Model num_frames: 1000
2025-12-12 19:05:02 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 10.01 GiB. GPU 0 has a total capacity of 15.77 GiB of which 4.39 GiB is free. Including non-PyTorch memory, this process has 11.36 GiB memory in use. Of the allocated memory 10.97 GiB is allocated by PyTorch, and 15.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Current batch size: 2
2025-12-12 19:05:02 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 19:05:02 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 19:05:02 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:05:10 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 256, 204]), Model num_frames: 1000
2025-12-12 19:05:10 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 2: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:05:10 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 2): {'cpu_memory_mb': 8603.7109375, 'cpu_memory_gb': 8.402061462402344, 'cpu_vms_mb': 73887.41796875, 'gpu_allocated_gb': 15.163918848, 'gpu_reserved_gb': 15.208546304, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 1.719795712}
2025-12-12 19:05:10 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Current batch size: 1
2025-12-12 19:05:10 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:05:28 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:05:28 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 6: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:05:28 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 6): {'cpu_memory_mb': 8603.7109375, 'cpu_memory_gb': 8.402061462402344, 'cpu_vms_mb': 73887.41796875, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 19:05:28 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Current batch size: 1
2025-12-12 19:05:28 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:05:30 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:05:30 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:05:30 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 8603.7109375, 'cpu_memory_gb': 8.402061462402344, 'cpu_vms_mb': 73887.41796875, 'gpu_allocated_gb': 16.238706176, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 19:05:30 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Final batch size: 1
2025-12-12 19:05:30 [ERROR] [lib.training.pipeline:1640] Error training fold 3: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 112, in forward
    x = F.relu(self.bn3(self.conv3(x)))
                        ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 543, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:05:30 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 38797d8f4fce479eae4cb3dfec974ade
2025-12-12 19:05:31 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 4/5 (20% sample)
2025-12-12 19:05:31 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 4 directory (clean mode)
2025-12-12 19:05:31 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 19:05:31 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: d5373c6a08f84c1696136af6a0b4fb28 (experiment: naive_cnn)
2025-12-12 19:05:31 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 4...
2025-12-12 19:05:33 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 19:05:33 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:05:37 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 144, 256]), Model num_frames: 1000
2025-12-12 19:05:37 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Current batch size: 2
2025-12-12 19:05:37 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 19:05:37 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 19:05:37 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:06:02 [INFO] [lib.training.trainer:419] Epoch 1, Batch 10/524, Loss: 0.6953
2025-12-12 19:06:05 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 204, 256]), Model num_frames: 1000
2025-12-12 19:06:05 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 10: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:06:05 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 10): {'cpu_memory_mb': 8603.53515625, 'cpu_memory_gb': 8.40188980102539, 'cpu_vms_mb': 73887.41796875, 'gpu_allocated_gb': 15.163918848, 'gpu_reserved_gb': 15.208546304, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 1.719795712}
2025-12-12 19:06:05 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Current batch size: 1
2025-12-12 19:06:05 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:06:34 [INFO] [lib.training.trainer:419] Epoch 1, Batch 10/524, Loss: 0.6914
2025-12-12 19:06:43 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 256, 256]), Model num_frames: 1000
2025-12-12 19:06:43 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 11: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:06:43 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 11): {'cpu_memory_mb': 8603.54296875, 'cpu_memory_gb': 8.401897430419922, 'cpu_vms_mb': 73887.41796875, 'gpu_allocated_gb': 14.83005696, 'gpu_reserved_gb': 14.873001984, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 2.055340032}
2025-12-12 19:06:43 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Current batch size: 1
2025-12-12 19:06:43 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:06:58 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 40.19 MiB is free. Including non-PyTorch memory, this process has 15.71 GiB memory in use. Of the allocated memory 15.26 GiB is allocated by PyTorch, and 75.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 194, 256]), Model num_frames: 1000
2025-12-12 19:06:58 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 4: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 40.19 MiB is free. Including non-PyTorch memory, this process has 15.71 GiB memory in use. Of the allocated memory 15.26 GiB is allocated by PyTorch, and 75.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:06:59 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 4): {'cpu_memory_mb': 8603.54296875, 'cpu_memory_gb': 8.401897430419922, 'cpu_vms_mb': 73887.41796875, 'gpu_allocated_gb': 16.387598848, 'gpu_reserved_gb': 16.466837504, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.461504512}
2025-12-12 19:06:59 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 40.19 MiB is free. Including non-PyTorch memory, this process has 15.71 GiB memory in use. Of the allocated memory 15.26 GiB is allocated by PyTorch, and 75.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Final batch size: 1
2025-12-12 19:06:59 [ERROR] [lib.training.pipeline:1640] Error training fold 4: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 40.19 MiB is free. Including non-PyTorch memory, this process has 15.71 GiB memory in use. Of the allocated memory 15.26 GiB is allocated by PyTorch, and 75.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 112, in forward
    x = F.relu(self.bn3(self.conv3(x)))
                        ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 543, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 40.19 MiB is free. Including non-PyTorch memory, this process has 15.71 GiB memory in use. Of the allocated memory 15.26 GiB is allocated by PyTorch, and 75.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:06:59 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: d5373c6a08f84c1696136af6a0b4fb28
2025-12-12 19:06:59 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 5/5 (20% sample)
2025-12-12 19:06:59 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 5 directory (clean mode)
2025-12-12 19:06:59 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 19:06:59 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 5bd139a6ef7e47e6a7391e8cbb8f4786 (experiment: naive_cnn)
2025-12-12 19:06:59 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 5...
2025-12-12 19:07:01 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 19:07:01 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:07:09 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 144, 256]), Model num_frames: 1000
2025-12-12 19:07:09 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 5, Current batch size: 2
2025-12-12 19:07:09 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 19:07:09 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 19:07:09 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:07:12 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 204, 256]), Model num_frames: 1000
2025-12-12 19:07:12 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:07:12 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 8603.54296875, 'cpu_memory_gb': 8.401897430419922, 'cpu_vms_mb': 73887.41796875, 'gpu_allocated_gb': 15.163506176, 'gpu_reserved_gb': 15.208546304, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 1.719795712}
2025-12-12 19:07:12 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 5, Current batch size: 1
2025-12-12 19:07:12 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:07:15 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:07:15 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:07:15 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 8603.54296875, 'cpu_memory_gb': 8.401897430419922, 'cpu_vms_mb': 73887.41796875, 'gpu_allocated_gb': 16.238706176, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 19:07:15 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 5, Current batch size: 1
2025-12-12 19:07:15 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:07:31 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:07:31 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 5: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:07:31 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 5): {'cpu_memory_mb': 8603.54296875, 'cpu_memory_gb': 8.401897430419922, 'cpu_vms_mb': 73887.41796875, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 19:07:31 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 5, Final batch size: 1
2025-12-12 19:07:31 [ERROR] [lib.training.pipeline:1640] Error training fold 5: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 112, in forward
    x = F.relu(self.bn3(self.conv3(x)))
                        ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 543, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:07:31 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 5bd139a6ef7e47e6a7391e8cbb8f4786
2025-12-12 19:07:31 [INFO] [lib.training.pipeline:818] 
================================================================================
2025-12-12 19:07:31 [INFO] [lib.training.pipeline:819] Grid Search: Hyperparameter combination 6/18
2025-12-12 19:07:31 [INFO] [lib.training.pipeline:820] Parameters: {'learning_rate': 0.0001, 'weight_decay': 0.001, 'batch_size': 2, 'num_epochs': 30}
2025-12-12 19:07:31 [INFO] [lib.training.pipeline:821] ================================================================================
2025-12-12 19:07:31 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 1/5 (20% sample)
2025-12-12 19:07:31 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 1 directory (clean mode)
2025-12-12 19:07:31 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 19:07:31 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 7eab90378cd643039228f7cf76aecc97 (experiment: naive_cnn)
2025-12-12 19:07:31 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 1...
2025-12-12 19:07:33 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 19:07:33 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:07:42 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 144, 256]), Model num_frames: 1000
2025-12-12 19:07:42 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Current batch size: 2
2025-12-12 19:07:42 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 19:07:42 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 19:07:42 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:07:53 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 256, 204]), Model num_frames: 1000
2025-12-12 19:07:53 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 3: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:07:53 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 3): {'cpu_memory_mb': 8604.46484375, 'cpu_memory_gb': 8.40279769897461, 'cpu_vms_mb': 73888.41796875, 'gpu_allocated_gb': 15.163918848, 'gpu_reserved_gb': 15.208546304, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 1.719795712}
2025-12-12 19:07:53 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Current batch size: 1
2025-12-12 19:07:53 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:07:57 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 40.19 MiB is free. Including non-PyTorch memory, this process has 15.71 GiB memory in use. Of the allocated memory 15.26 GiB is allocated by PyTorch, and 75.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 194, 256]), Model num_frames: 1000
2025-12-12 19:07:57 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 1: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 40.19 MiB is free. Including non-PyTorch memory, this process has 15.71 GiB memory in use. Of the allocated memory 15.26 GiB is allocated by PyTorch, and 75.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:07:58 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 1): {'cpu_memory_mb': 8604.46484375, 'cpu_memory_gb': 8.40279769897461, 'cpu_vms_mb': 73888.41796875, 'gpu_allocated_gb': 16.387598848, 'gpu_reserved_gb': 16.466837504, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.461504512}
2025-12-12 19:07:58 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 40.19 MiB is free. Including non-PyTorch memory, this process has 15.71 GiB memory in use. Of the allocated memory 15.26 GiB is allocated by PyTorch, and 75.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Current batch size: 1
2025-12-12 19:07:58 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:08:05 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 688.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 660.19 MiB is free. Including non-PyTorch memory, this process has 15.11 GiB memory in use. Of the allocated memory 14.67 GiB is allocated by PyTorch, and 57.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 178, 256]), Model num_frames: 1000
2025-12-12 19:08:05 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 688.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 660.19 MiB is free. Including non-PyTorch memory, this process has 15.11 GiB memory in use. Of the allocated memory 14.67 GiB is allocated by PyTorch, and 57.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:08:05 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 8604.46484375, 'cpu_memory_gb': 8.40279769897461, 'cpu_vms_mb': 73888.41796875, 'gpu_allocated_gb': 15.756549632, 'gpu_reserved_gb': 15.816720384, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 1.111621632}
2025-12-12 19:08:05 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 688.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 660.19 MiB is free. Including non-PyTorch memory, this process has 15.11 GiB memory in use. Of the allocated memory 14.67 GiB is allocated by PyTorch, and 57.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Final batch size: 1
2025-12-12 19:08:05 [ERROR] [lib.training.pipeline:1640] Error training fold 1: CUDA out of memory. Tried to allocate 688.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 660.19 MiB is free. Including non-PyTorch memory, this process has 15.11 GiB memory in use. Of the allocated memory 14.67 GiB is allocated by PyTorch, and 57.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 112, in forward
    x = F.relu(self.bn3(self.conv3(x)))
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
           ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/functional.py", line 2813, in batch_norm
    return torch.batch_norm(
           ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 688.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 660.19 MiB is free. Including non-PyTorch memory, this process has 15.11 GiB memory in use. Of the allocated memory 14.67 GiB is allocated by PyTorch, and 57.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:08:05 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 7eab90378cd643039228f7cf76aecc97
2025-12-12 19:08:06 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 2/5 (20% sample)
2025-12-12 19:08:06 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 2 directory (clean mode)
2025-12-12 19:08:06 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 19:08:06 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 04c10054d7664abeb25d08ac7c6e9d5d (experiment: naive_cnn)
2025-12-12 19:08:06 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 2...
2025-12-12 19:08:07 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 19:08:07 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:08:12 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 15.62 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.87 GiB is free. Including non-PyTorch memory, this process has 1.89 GiB memory in use. Of the allocated memory 1.48 GiB is allocated by PyTorch, and 26.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 256, 256]), Model num_frames: 1000
2025-12-12 19:08:12 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 15.62 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.87 GiB is free. Including non-PyTorch memory, this process has 1.89 GiB memory in use. Of the allocated memory 1.48 GiB is allocated by PyTorch, and 26.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Current batch size: 2
2025-12-12 19:08:12 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 19:08:12 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 19:08:12 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:08:18 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 256, 256]), Model num_frames: 1000
2025-12-12 19:08:18 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 1: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:08:19 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 1): {'cpu_memory_mb': 8604.46484375, 'cpu_memory_gb': 8.40279769897461, 'cpu_vms_mb': 73888.41796875, 'gpu_allocated_gb': 14.83005696, 'gpu_reserved_gb': 14.873001984, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 2.055340032}
2025-12-12 19:08:19 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 14.23 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 40.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Current batch size: 1
2025-12-12 19:08:19 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:08:24 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:08:24 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 1: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:08:24 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 1): {'cpu_memory_mb': 8604.46484375, 'cpu_memory_gb': 8.40279769897461, 'cpu_vms_mb': 73888.41796875, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 19:08:24 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Current batch size: 1
2025-12-12 19:08:24 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:08:28 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 7.81 GiB. GPU 0 has a total capacity of 15.77 GiB of which 6.80 GiB is free. Including non-PyTorch memory, this process has 8.96 GiB memory in use. Of the allocated memory 8.56 GiB is allocated by PyTorch, and 16.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 256, 256]), Model num_frames: 1000
2025-12-12 19:08:28 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 7.81 GiB. GPU 0 has a total capacity of 15.77 GiB of which 6.80 GiB is free. Including non-PyTorch memory, this process has 8.96 GiB memory in use. Of the allocated memory 8.56 GiB is allocated by PyTorch, and 16.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Final batch size: 1
2025-12-12 19:08:28 [ERROR] [lib.training.pipeline:1640] Error training fold 2: CUDA out of memory. Tried to allocate 7.81 GiB. GPU 0 has a total capacity of 15.77 GiB of which 6.80 GiB is free. Including non-PyTorch memory, this process has 8.96 GiB memory in use. Of the allocated memory 8.56 GiB is allocated by PyTorch, and 16.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 330, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 108, in forward
    x = F.relu(self.bn1(self.conv1(x)))
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
           ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/functional.py", line 2813, in batch_norm
    return torch.batch_norm(
           ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.81 GiB. GPU 0 has a total capacity of 15.77 GiB of which 6.80 GiB is free. Including non-PyTorch memory, this process has 8.96 GiB memory in use. Of the allocated memory 8.56 GiB is allocated by PyTorch, and 16.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:08:28 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 04c10054d7664abeb25d08ac7c6e9d5d
2025-12-12 19:08:28 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 3/5 (20% sample)
2025-12-12 19:08:28 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 3 directory (clean mode)
2025-12-12 19:08:28 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 19:08:28 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 2c43158cf9b24474876d5af5b879d7b4 (experiment: naive_cnn)
2025-12-12 19:08:28 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 3...
2025-12-12 19:08:30 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 19:08:30 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:08:36 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 144, 256]), Model num_frames: 1000
2025-12-12 19:08:36 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Current batch size: 2
2025-12-12 19:08:36 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 19:08:36 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 19:08:36 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:08:41 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:08:41 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 1: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:08:41 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 1): {'cpu_memory_mb': 8604.46484375, 'cpu_memory_gb': 8.40279769897461, 'cpu_vms_mb': 73888.41796875, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 19:08:41 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Current batch size: 1
2025-12-12 19:08:41 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:08:48 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:08:48 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 2: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:08:48 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 2): {'cpu_memory_mb': 8604.46484375, 'cpu_memory_gb': 8.40279769897461, 'cpu_vms_mb': 73888.41796875, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 19:08:48 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Current batch size: 1
2025-12-12 19:08:48 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:09:03 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:09:03 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 4: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:09:03 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 4): {'cpu_memory_mb': 8604.46484375, 'cpu_memory_gb': 8.40279769897461, 'cpu_vms_mb': 73888.41796875, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 19:09:03 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Final batch size: 1
2025-12-12 19:09:03 [ERROR] [lib.training.pipeline:1640] Error training fold 3: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 112, in forward
    x = F.relu(self.bn3(self.conv3(x)))
                        ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 543, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:09:03 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 2c43158cf9b24474876d5af5b879d7b4
2025-12-12 19:09:04 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 4/5 (20% sample)
2025-12-12 19:09:04 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 4 directory (clean mode)
2025-12-12 19:09:04 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 19:09:04 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: c52b5ce2d0704b6190de15f21204ffaf (experiment: naive_cnn)
2025-12-12 19:09:04 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 4...
2025-12-12 19:09:06 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 19:09:06 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:09:12 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 144, 256]), Model num_frames: 1000
2025-12-12 19:09:12 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Current batch size: 2
2025-12-12 19:09:12 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 19:09:12 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 19:09:12 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:09:18 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 240.19 MiB is free. Including non-PyTorch memory, this process has 15.52 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 17.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:09:18 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 1: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 240.19 MiB is free. Including non-PyTorch memory, this process has 15.52 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 17.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:09:18 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 1): {'cpu_memory_mb': 8604.46484375, 'cpu_memory_gb': 8.40279769897461, 'cpu_vms_mb': 73888.41796875, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.257122304, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.671219712}
2025-12-12 19:09:18 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 240.19 MiB is free. Including non-PyTorch memory, this process has 15.52 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 17.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Current batch size: 1
2025-12-12 19:09:18 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:09:34 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:09:34 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 5: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:09:34 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 5): {'cpu_memory_mb': 8604.46484375, 'cpu_memory_gb': 8.40279769897461, 'cpu_vms_mb': 73888.41796875, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 19:09:34 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Current batch size: 1
2025-12-12 19:09:34 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:10:12 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:10:12 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 9: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:10:12 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 9): {'cpu_memory_mb': 10860.0625, 'cpu_memory_gb': 10.60552978515625, 'cpu_vms_mb': 76144.9765625, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 19:10:12 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Final batch size: 1
2025-12-12 19:10:12 [ERROR] [lib.training.pipeline:1640] Error training fold 4: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 112, in forward
    x = F.relu(self.bn3(self.conv3(x)))
                        ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 543, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:10:12 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: c52b5ce2d0704b6190de15f21204ffaf
2025-12-12 19:10:12 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 5/5 (20% sample)
2025-12-12 19:10:12 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 5 directory (clean mode)
2025-12-12 19:10:12 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 19:10:12 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 43fcf29864464dd2b076f64c61c65c9d (experiment: naive_cnn)
2025-12-12 19:10:12 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 5...
2025-12-12 19:10:14 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 19:10:14 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:10:19 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 144, 256]), Model num_frames: 1000
2025-12-12 19:10:19 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 5, Current batch size: 2
2025-12-12 19:10:19 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 19:10:19 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 19:10:19 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:10:21 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:10:21 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:10:21 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 10860.06640625, 'cpu_memory_gb': 10.605533599853516, 'cpu_vms_mb': 76144.9765625, 'gpu_allocated_gb': 16.238706176, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 19:10:21 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 5, Current batch size: 1
2025-12-12 19:10:21 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:10:24 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 256, 204]), Model num_frames: 1000
2025-12-12 19:10:24 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 1: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:10:25 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 1): {'cpu_memory_mb': 10860.06640625, 'cpu_memory_gb': 10.605533599853516, 'cpu_vms_mb': 76144.9765625, 'gpu_allocated_gb': 15.163918848, 'gpu_reserved_gb': 15.208546304, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 1.719795712}
2025-12-12 19:10:25 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 5, Current batch size: 1
2025-12-12 19:10:25 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:10:27 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 204, 256]), Model num_frames: 1000
2025-12-12 19:10:27 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:10:27 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 10860.06640625, 'cpu_memory_gb': 10.605533599853516, 'cpu_vms_mb': 76144.9765625, 'gpu_allocated_gb': 15.163506176, 'gpu_reserved_gb': 15.208546304, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 1.719795712}
2025-12-12 19:10:27 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 5, Final batch size: 1
2025-12-12 19:10:27 [ERROR] [lib.training.pipeline:1640] Error training fold 5: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 110, in forward
    x = F.relu(self.bn2(self.conv2(x)))
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/functional.py", line 1697, in relu
    result = torch.relu(input)
             ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:10:27 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 43fcf29864464dd2b076f64c61c65c9d
2025-12-12 19:10:27 [INFO] [lib.training.pipeline:818] 
================================================================================
2025-12-12 19:10:27 [INFO] [lib.training.pipeline:819] Grid Search: Hyperparameter combination 7/18
2025-12-12 19:10:27 [INFO] [lib.training.pipeline:820] Parameters: {'learning_rate': 0.0005, 'weight_decay': 1e-05, 'batch_size': 2, 'num_epochs': 20}
2025-12-12 19:10:27 [INFO] [lib.training.pipeline:821] ================================================================================
2025-12-12 19:10:27 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 1/5 (20% sample)
2025-12-12 19:10:27 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 1 directory (clean mode)
2025-12-12 19:10:27 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 19:10:27 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 518bdd916c1b4fe68d9c46bdc4ad5bfa (experiment: naive_cnn)
2025-12-12 19:10:27 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 1...
2025-12-12 19:10:29 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 19:10:29 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:10:33 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 11.72 GiB. GPU 0 has a total capacity of 15.77 GiB of which 2.52 GiB is free. Including non-PyTorch memory, this process has 13.23 GiB memory in use. Of the allocated memory 12.83 GiB is allocated by PyTorch, and 21.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:10:33 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 11.72 GiB. GPU 0 has a total capacity of 15.77 GiB of which 2.52 GiB is free. Including non-PyTorch memory, this process has 13.23 GiB memory in use. Of the allocated memory 12.83 GiB is allocated by PyTorch, and 21.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Current batch size: 2
2025-12-12 19:10:33 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 19:10:33 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 19:10:33 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:10:42 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 256, 204]), Model num_frames: 1000
2025-12-12 19:10:42 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 1: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:10:42 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 1): {'cpu_memory_mb': 10860.0703125, 'cpu_memory_gb': 10.605537414550781, 'cpu_vms_mb': 76144.9765625, 'gpu_allocated_gb': 15.163918848, 'gpu_reserved_gb': 15.208546304, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 1.719795712}
2025-12-12 19:10:42 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Current batch size: 1
2025-12-12 19:10:42 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:10:44 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 672.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 320.19 MiB is free. Including non-PyTorch memory, this process has 15.44 GiB memory in use. Of the allocated memory 15.00 GiB is allocated by PyTorch, and 63.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 174, 256]), Model num_frames: 1000
2025-12-12 19:10:44 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 672.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 320.19 MiB is free. Including non-PyTorch memory, this process has 15.44 GiB memory in use. Of the allocated memory 15.00 GiB is allocated by PyTorch, and 63.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:10:44 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 10860.0703125, 'cpu_memory_gb': 10.605537414550781, 'cpu_vms_mb': 76144.9765625, 'gpu_allocated_gb': 16.106758656, 'gpu_reserved_gb': 16.173236224, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.755105792}
2025-12-12 19:10:44 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 672.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 320.19 MiB is free. Including non-PyTorch memory, this process has 15.44 GiB memory in use. Of the allocated memory 15.00 GiB is allocated by PyTorch, and 63.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Current batch size: 1
2025-12-12 19:10:44 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:10:58 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 204, 256]), Model num_frames: 1000
2025-12-12 19:10:58 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 4: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:10:58 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 4): {'cpu_memory_mb': 10860.0703125, 'cpu_memory_gb': 10.605537414550781, 'cpu_vms_mb': 76144.9765625, 'gpu_allocated_gb': 15.163918848, 'gpu_reserved_gb': 15.208546304, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 1.719795712}
2025-12-12 19:10:58 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Final batch size: 1
2025-12-12 19:10:58 [ERROR] [lib.training.pipeline:1640] Error training fold 1: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 110, in forward
    x = F.relu(self.bn2(self.conv2(x)))
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/functional.py", line 1697, in relu
    result = torch.relu(input)
             ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:10:58 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 518bdd916c1b4fe68d9c46bdc4ad5bfa
2025-12-12 19:10:58 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 2/5 (20% sample)
2025-12-12 19:10:58 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 2 directory (clean mode)
2025-12-12 19:10:58 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 19:10:58 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 5bbefe98aed546db846a226d4b3e9668 (experiment: naive_cnn)
2025-12-12 19:10:58 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 2...
2025-12-12 19:11:00 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 19:11:00 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:11:05 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 11.72 GiB. GPU 0 has a total capacity of 15.77 GiB of which 2.52 GiB is free. Including non-PyTorch memory, this process has 13.23 GiB memory in use. Of the allocated memory 12.83 GiB is allocated by PyTorch, and 21.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:11:05 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 11.72 GiB. GPU 0 has a total capacity of 15.77 GiB of which 2.52 GiB is free. Including non-PyTorch memory, this process has 13.23 GiB memory in use. Of the allocated memory 12.83 GiB is allocated by PyTorch, and 21.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Current batch size: 2
2025-12-12 19:11:05 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 19:11:05 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 19:11:05 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:11:25 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:11:25 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 7: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:11:26 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 7): {'cpu_memory_mb': 9448.38671875, 'cpu_memory_gb': 9.226940155029297, 'cpu_vms_mb': 74733.16796875, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 19:11:26 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Current batch size: 1
2025-12-12 19:11:26 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:11:35 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:11:35 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 3: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:11:35 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 3): {'cpu_memory_mb': 9448.38671875, 'cpu_memory_gb': 9.226940155029297, 'cpu_vms_mb': 74733.16796875, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 19:11:35 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Current batch size: 1
2025-12-12 19:11:35 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:11:41 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 720.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 192.00 KiB is free. Including non-PyTorch memory, this process has 15.75 GiB memory in use. Of the allocated memory 15.33 GiB is allocated by PyTorch, and 41.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 186, 256]), Model num_frames: 1000
2025-12-12 19:11:41 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 2: CUDA out of memory. Tried to allocate 720.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 192.00 KiB is free. Including non-PyTorch memory, this process has 15.75 GiB memory in use. Of the allocated memory 15.33 GiB is allocated by PyTorch, and 41.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:11:41 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 2): {'cpu_memory_mb': 9448.38671875, 'cpu_memory_gb': 9.226940155029297, 'cpu_vms_mb': 74733.16796875, 'gpu_allocated_gb': 16.465570304, 'gpu_reserved_gb': 16.508780544, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.419561472}
2025-12-12 19:11:41 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 720.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 192.00 KiB is free. Including non-PyTorch memory, this process has 15.75 GiB memory in use. Of the allocated memory 15.33 GiB is allocated by PyTorch, and 41.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Final batch size: 1
2025-12-12 19:11:41 [ERROR] [lib.training.pipeline:1640] Error training fold 2: CUDA out of memory. Tried to allocate 720.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 192.00 KiB is free. Including non-PyTorch memory, this process has 15.75 GiB memory in use. Of the allocated memory 15.33 GiB is allocated by PyTorch, and 41.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 112, in forward
    x = F.relu(self.bn3(self.conv3(x)))
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
           ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/functional.py", line 2813, in batch_norm
    return torch.batch_norm(
           ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 720.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 192.00 KiB is free. Including non-PyTorch memory, this process has 15.75 GiB memory in use. Of the allocated memory 15.33 GiB is allocated by PyTorch, and 41.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:11:41 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 5bbefe98aed546db846a226d4b3e9668
2025-12-12 19:11:42 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 3/5 (20% sample)
2025-12-12 19:11:42 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 3 directory (clean mode)
2025-12-12 19:11:42 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 19:11:42 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: f3b1ce72c4f44fcba9a31e4789a94acc (experiment: naive_cnn)
2025-12-12 19:11:42 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 3...
2025-12-12 19:11:43 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 19:11:43 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:11:49 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 15.62 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.87 GiB is free. Including non-PyTorch memory, this process has 1.89 GiB memory in use. Of the allocated memory 1.48 GiB is allocated by PyTorch, and 26.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 256, 256]), Model num_frames: 1000
2025-12-12 19:11:49 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 15.62 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.87 GiB is free. Including non-PyTorch memory, this process has 1.89 GiB memory in use. Of the allocated memory 1.48 GiB is allocated by PyTorch, and 26.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Current batch size: 2
2025-12-12 19:11:49 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 19:11:49 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 19:11:49 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:11:51 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 1.59 GiB. GPU 0 has a total capacity of 15.77 GiB of which 940.19 MiB is free. Including non-PyTorch memory, this process has 14.84 GiB memory in use. Of the allocated memory 14.40 GiB is allocated by PyTorch, and 59.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 208, 256]), Model num_frames: 1000
2025-12-12 19:11:51 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 1.59 GiB. GPU 0 has a total capacity of 15.77 GiB of which 940.19 MiB is free. Including non-PyTorch memory, this process has 14.84 GiB memory in use. Of the allocated memory 14.40 GiB is allocated by PyTorch, and 59.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:11:51 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 8604.63671875, 'cpu_memory_gb': 8.402965545654297, 'cpu_vms_mb': 73889.41796875, 'gpu_allocated_gb': 15.460466176, 'gpu_reserved_gb': 15.523119104, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 1.405222912}
2025-12-12 19:11:51 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 1.59 GiB. GPU 0 has a total capacity of 15.77 GiB of which 940.19 MiB is free. Including non-PyTorch memory, this process has 14.84 GiB memory in use. Of the allocated memory 14.40 GiB is allocated by PyTorch, and 59.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Current batch size: 1
2025-12-12 19:11:51 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:11:54 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:11:54 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:11:54 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 8604.63671875, 'cpu_memory_gb': 8.402965545654297, 'cpu_vms_mb': 73889.41796875, 'gpu_allocated_gb': 16.238706176, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 19:11:54 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Current batch size: 1
2025-12-12 19:11:54 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:11:57 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:11:57 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:11:57 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 8604.63671875, 'cpu_memory_gb': 8.402965545654297, 'cpu_vms_mb': 73889.41796875, 'gpu_allocated_gb': 16.238706176, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 19:11:57 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Final batch size: 1
2025-12-12 19:11:57 [ERROR] [lib.training.pipeline:1640] Error training fold 3: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 112, in forward
    x = F.relu(self.bn3(self.conv3(x)))
                        ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 543, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:11:57 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: f3b1ce72c4f44fcba9a31e4789a94acc
2025-12-12 19:11:57 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 4/5 (20% sample)
2025-12-12 19:11:57 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 4 directory (clean mode)
2025-12-12 19:11:57 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 19:11:57 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: cb883b56c3c94f00b108ca0ed9de3deb (experiment: naive_cnn)
2025-12-12 19:11:57 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 4...
2025-12-12 19:11:59 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 19:11:59 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:12:04 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 144, 256]), Model num_frames: 1000
2025-12-12 19:12:04 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Current batch size: 2
2025-12-12 19:12:04 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 19:12:04 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 19:12:04 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:12:07 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:12:07 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:12:07 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 8604.63671875, 'cpu_memory_gb': 8.402965545654297, 'cpu_vms_mb': 73889.41796875, 'gpu_allocated_gb': 16.238706176, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 19:12:07 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Current batch size: 1
2025-12-12 19:12:07 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:12:30 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:12:30 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 7: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:12:30 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 7): {'cpu_memory_mb': 8604.63671875, 'cpu_memory_gb': 8.402965545654297, 'cpu_vms_mb': 73889.41796875, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 19:12:30 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Current batch size: 1
2025-12-12 19:12:30 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:12:33 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:12:33 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:12:34 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 8604.63671875, 'cpu_memory_gb': 8.402965545654297, 'cpu_vms_mb': 73889.41796875, 'gpu_allocated_gb': 16.238706176, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 19:12:34 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Final batch size: 1
2025-12-12 19:12:34 [ERROR] [lib.training.pipeline:1640] Error training fold 4: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 112, in forward
    x = F.relu(self.bn3(self.conv3(x)))
                        ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 543, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:12:34 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: cb883b56c3c94f00b108ca0ed9de3deb
2025-12-12 19:12:34 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 5/5 (20% sample)
2025-12-12 19:12:34 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 5 directory (clean mode)
2025-12-12 19:12:34 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 19:12:34 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 4977553dc8f54f668d2f5822e4d16b3d (experiment: naive_cnn)
2025-12-12 19:12:34 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 5...
2025-12-12 19:12:36 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 19:12:36 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:12:40 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 144, 256]), Model num_frames: 1000
2025-12-12 19:12:40 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 5, Current batch size: 2
2025-12-12 19:12:40 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 19:12:40 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 19:12:40 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:12:42 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:12:42 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:12:42 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 8604.63671875, 'cpu_memory_gb': 8.402965545654297, 'cpu_vms_mb': 73889.41796875, 'gpu_allocated_gb': 16.238706176, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 19:12:42 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 5, Current batch size: 1
2025-12-12 19:12:42 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:12:58 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 688.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 660.19 MiB is free. Including non-PyTorch memory, this process has 15.11 GiB memory in use. Of the allocated memory 14.67 GiB is allocated by PyTorch, and 56.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 178, 256]), Model num_frames: 1000
2025-12-12 19:12:58 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 2: CUDA out of memory. Tried to allocate 688.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 660.19 MiB is free. Including non-PyTorch memory, this process has 15.11 GiB memory in use. Of the allocated memory 14.67 GiB is allocated by PyTorch, and 56.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:12:58 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 2): {'cpu_memory_mb': 8604.63671875, 'cpu_memory_gb': 8.402965545654297, 'cpu_vms_mb': 73889.41796875, 'gpu_allocated_gb': 15.756962304, 'gpu_reserved_gb': 15.816720384, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 1.111621632}
2025-12-12 19:12:58 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 688.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 660.19 MiB is free. Including non-PyTorch memory, this process has 15.11 GiB memory in use. Of the allocated memory 14.67 GiB is allocated by PyTorch, and 56.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 5, Current batch size: 1
2025-12-12 19:12:58 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:13:01 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:13:01 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:13:01 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 8604.63671875, 'cpu_memory_gb': 8.402965545654297, 'cpu_vms_mb': 73889.41796875, 'gpu_allocated_gb': 16.238706176, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 19:13:01 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 5, Final batch size: 1
2025-12-12 19:13:01 [ERROR] [lib.training.pipeline:1640] Error training fold 5: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 112, in forward
    x = F.relu(self.bn3(self.conv3(x)))
                        ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 543, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:13:01 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 4977553dc8f54f668d2f5822e4d16b3d
2025-12-12 19:13:02 [INFO] [lib.training.pipeline:818] 
================================================================================
2025-12-12 19:13:02 [INFO] [lib.training.pipeline:819] Grid Search: Hyperparameter combination 8/18
2025-12-12 19:13:02 [INFO] [lib.training.pipeline:820] Parameters: {'learning_rate': 0.0005, 'weight_decay': 1e-05, 'batch_size': 2, 'num_epochs': 30}
2025-12-12 19:13:02 [INFO] [lib.training.pipeline:821] ================================================================================
2025-12-12 19:13:02 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 1/5 (20% sample)
2025-12-12 19:13:02 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 1 directory (clean mode)
2025-12-12 19:13:02 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 19:13:02 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: d4baf1c9a69b4d54b7b78666615c219f (experiment: naive_cnn)
2025-12-12 19:13:02 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 1...
2025-12-12 19:13:04 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 19:13:04 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:13:10 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 15.62 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.87 GiB is free. Including non-PyTorch memory, this process has 1.89 GiB memory in use. Of the allocated memory 1.48 GiB is allocated by PyTorch, and 26.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 256, 256]), Model num_frames: 1000
2025-12-12 19:13:10 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 15.62 GiB. GPU 0 has a total capacity of 15.77 GiB of which 13.87 GiB is free. Including non-PyTorch memory, this process has 1.89 GiB memory in use. Of the allocated memory 1.48 GiB is allocated by PyTorch, and 26.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Current batch size: 2
2025-12-12 19:13:10 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 19:13:10 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 19:13:10 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:13:13 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:13:13 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:13:13 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 8604.63671875, 'cpu_memory_gb': 8.402965545654297, 'cpu_vms_mb': 73889.41796875, 'gpu_allocated_gb': 16.238706176, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 19:13:13 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Current batch size: 1
2025-12-12 19:13:13 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:13:18 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:13:18 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 1: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:13:18 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 1): {'cpu_memory_mb': 8604.63671875, 'cpu_memory_gb': 8.402965545654297, 'cpu_vms_mb': 73889.41796875, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 19:13:18 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Current batch size: 1
2025-12-12 19:13:18 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:13:22 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 7.81 GiB. GPU 0 has a total capacity of 15.77 GiB of which 6.80 GiB is free. Including non-PyTorch memory, this process has 8.96 GiB memory in use. Of the allocated memory 8.56 GiB is allocated by PyTorch, and 16.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 256, 256]), Model num_frames: 1000
2025-12-12 19:13:22 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 7.81 GiB. GPU 0 has a total capacity of 15.77 GiB of which 6.80 GiB is free. Including non-PyTorch memory, this process has 8.96 GiB memory in use. Of the allocated memory 8.56 GiB is allocated by PyTorch, and 16.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 1, Final batch size: 1
2025-12-12 19:13:22 [ERROR] [lib.training.pipeline:1640] Error training fold 1: CUDA out of memory. Tried to allocate 7.81 GiB. GPU 0 has a total capacity of 15.77 GiB of which 6.80 GiB is free. Including non-PyTorch memory, this process has 8.96 GiB memory in use. Of the allocated memory 8.56 GiB is allocated by PyTorch, and 16.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 330, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 108, in forward
    x = F.relu(self.bn1(self.conv1(x)))
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
           ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/functional.py", line 2813, in batch_norm
    return torch.batch_norm(
           ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.81 GiB. GPU 0 has a total capacity of 15.77 GiB of which 6.80 GiB is free. Including non-PyTorch memory, this process has 8.96 GiB memory in use. Of the allocated memory 8.56 GiB is allocated by PyTorch, and 16.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:13:22 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: d4baf1c9a69b4d54b7b78666615c219f
2025-12-12 19:13:22 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 2/5 (20% sample)
2025-12-12 19:13:22 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 2 directory (clean mode)
2025-12-12 19:13:22 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 19:13:22 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 680d110e61b14605a88d68b214f2a2cb (experiment: naive_cnn)
2025-12-12 19:13:22 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 2...
2025-12-12 19:13:24 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 19:13:24 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:13:31 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 144, 256]), Model num_frames: 1000
2025-12-12 19:13:31 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Current batch size: 2
2025-12-12 19:13:31 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 19:13:31 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 19:13:31 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:13:33 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:13:33 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:13:34 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 8604.63671875, 'cpu_memory_gb': 8.402965545654297, 'cpu_vms_mb': 73889.41796875, 'gpu_allocated_gb': 16.238706176, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 19:13:34 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Current batch size: 1
2025-12-12 19:13:34 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:13:37 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:13:37 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:13:37 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 8604.63671875, 'cpu_memory_gb': 8.402965545654297, 'cpu_vms_mb': 73889.41796875, 'gpu_allocated_gb': 16.238706176, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 19:13:37 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Current batch size: 1
2025-12-12 19:13:37 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:13:39 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 204, 256]), Model num_frames: 1000
2025-12-12 19:13:39 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:13:40 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 8604.63671875, 'cpu_memory_gb': 8.402965545654297, 'cpu_vms_mb': 73889.41796875, 'gpu_allocated_gb': 15.163506176, 'gpu_reserved_gb': 15.208546304, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 1.719795712}
2025-12-12 19:13:40 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 2, Final batch size: 1
2025-12-12 19:13:40 [ERROR] [lib.training.pipeline:1640] Error training fold 2: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 110, in forward
    x = F.relu(self.bn2(self.conv2(x)))
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/functional.py", line 1697, in relu
    result = torch.relu(input)
             ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 42.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:13:40 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 680d110e61b14605a88d68b214f2a2cb
2025-12-12 19:13:40 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 3/5 (20% sample)
2025-12-12 19:13:40 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 3 directory (clean mode)
2025-12-12 19:13:40 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 19:13:40 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 1af426f6ef6c40a588b962146a4d7d84 (experiment: naive_cnn)
2025-12-12 19:13:40 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 3...
2025-12-12 19:13:42 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 19:13:42 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:13:49 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 144, 256]), Model num_frames: 1000
2025-12-12 19:13:49 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Current batch size: 2
2025-12-12 19:13:49 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 19:13:49 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 19:13:49 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:13:53 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:13:53 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 1: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:13:54 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 1): {'cpu_memory_mb': 8604.63671875, 'cpu_memory_gb': 8.402965545654297, 'cpu_vms_mb': 73889.41796875, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 19:13:54 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Current batch size: 1
2025-12-12 19:13:54 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:13:57 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:13:57 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 0: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:13:57 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 0): {'cpu_memory_mb': 8604.63671875, 'cpu_memory_gb': 8.402965545654297, 'cpu_vms_mb': 73889.41796875, 'gpu_allocated_gb': 16.238706176, 'gpu_reserved_gb': 16.299065344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.629276672}
2025-12-12 19:13:57 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 200.19 MiB is free. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Current batch size: 1
2025-12-12 19:13:57 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:14:26 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 720.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 700.19 MiB is free. Including non-PyTorch memory, this process has 15.07 GiB memory in use. Of the allocated memory 14.63 GiB is allocated by PyTorch, and 59.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 186, 256]), Model num_frames: 1000
2025-12-12 19:14:26 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 9: CUDA out of memory. Tried to allocate 720.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 700.19 MiB is free. Including non-PyTorch memory, this process has 15.07 GiB memory in use. Of the allocated memory 14.63 GiB is allocated by PyTorch, and 59.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:14:26 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 9): {'cpu_memory_mb': 8604.63671875, 'cpu_memory_gb': 8.402965545654297, 'cpu_vms_mb': 73889.41796875, 'gpu_allocated_gb': 15.711758848, 'gpu_reserved_gb': 15.774777344, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 1.153564672}
2025-12-12 19:14:26 [ERROR] [lib.training.pipeline:1261] CUDA OOM or runtime error during training (max retries reached): CUDA out of memory. Tried to allocate 720.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 700.19 MiB is free. Including non-PyTorch memory, this process has 15.07 GiB memory in use. Of the allocated memory 14.63 GiB is allocated by PyTorch, and 59.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 3, Final batch size: 1
2025-12-12 19:14:26 [ERROR] [lib.training.pipeline:1640] Error training fold 3: CUDA out of memory. Tried to allocate 720.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 700.19 MiB is free. Including non-PyTorch memory, this process has 15.07 GiB memory in use. Of the allocated memory 14.63 GiB is allocated by PyTorch, and 59.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/pipeline.py", line 1220, in stage5_train_models
    model = fit(
            ^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 624, in fit
    train_loss = train_one_epoch(
                 ^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/trainer.py", line 342, in train_one_epoch
    logits = model(clips)
             ^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/lib/training/_cnn.py", line 112, in forward
    x = F.relu(self.bn3(self.conv3(x)))
                        ^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 543, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 720.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 700.19 MiB is free. Including non-PyTorch memory, this process has 15.07 GiB memory in use. Of the allocated memory 14.63 GiB is allocated by PyTorch, and 59.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:14:26 [INFO] [lib.mlops.mlflow_tracker:142] MLflow run ended: 1af426f6ef6c40a588b962146a4d7d84
2025-12-12 19:14:26 [INFO] [lib.training.pipeline:850] 
Hyperparameter Search - naive_cnn - Fold 4/5 (20% sample)
2025-12-12 19:14:26 [INFO] [lib.training.pipeline:859] Deleted existing hyperparameter search fold 4 directory (clean mode)
2025-12-12 19:14:26 [INFO] [lib.training.pipeline:954] Training configuration - Batch size: 2, Gradient accumulation steps: 16, Effective batch size: 32
2025-12-12 19:14:26 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 364a0f2bd18442a1930357eb849b72d2 (experiment: naive_cnn)
2025-12-12 19:14:26 [INFO] [lib.training.pipeline:1073] Training PyTorch model naive_cnn on fold 4...
2025-12-12 19:14:28 [INFO] [lib.training.pipeline:1115] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 19:14:28 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:14:33 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([2, 3, 1000, 144, 256]), Model num_frames: 1000
2025-12-12 19:14:33 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 1/4): CUDA out of memory. Tried to allocate 8.79 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 10.03 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 22.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Current batch size: 2
2025-12-12 19:14:33 [WARNING] [lib.training.pipeline:1175] OOM retry 1: Reducing batch size from 2 to 1
2025-12-12 19:14:33 [INFO] [lib.training.pipeline:1209] Retrying with batch_size=1, gradient_accumulation_steps=32, effective_batch_size=32
2025-12-12 19:14:33 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:14:38 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 192, 256]), Model num_frames: 1000
2025-12-12 19:14:38 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 1: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:14:38 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 1): {'cpu_memory_mb': 8604.63671875, 'cpu_memory_gb': 8.402965545654297, 'cpu_vms_mb': 73889.41796875, 'gpu_allocated_gb': 16.239118848, 'gpu_reserved_gb': 16.320036864, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 0.608305152}
2025-12-12 19:14:38 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 2/4): CUDA out of memory. Tried to allocate 750.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 180.19 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 77.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Current batch size: 1
2025-12-12 19:14:38 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 19:15:07 [ERROR] [lib.training._cnn:142] Runtime error in forward pass: CUDA out of memory. Tried to allocate 1.71 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.54 GiB is free. Including non-PyTorch memory, this process has 14.21 GiB memory in use. Of the allocated memory 13.80 GiB is allocated by PyTorch, and 36.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Input shape: torch.Size([1, 3, 1000, 224, 256]), Model num_frames: 1000
2025-12-12 19:15:07 [ERROR] [lib.utils.memory:128] OOM error forward pass batch 6: CUDA out of memory. Tried to allocate 1.71 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.54 GiB is free. Including non-PyTorch memory, this process has 14.21 GiB memory in use. Of the allocated memory 13.80 GiB is allocated by PyTorch, and 36.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-12 19:15:08 [INFO] [lib.utils.memory:89] Memory stats (after OOM forward pass batch 6): {'cpu_memory_mb': 8605.87890625, 'cpu_memory_gb': 8.404178619384766, 'cpu_vms_mb': 73890.41796875, 'gpu_allocated_gb': 14.813709824, 'gpu_reserved_gb': 14.852030464, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 2.076311552}
2025-12-12 19:15:08 [WARNING] [lib.training.pipeline:1246] CUDA OOM during training (attempt 3/4): CUDA out of memory. Tried to allocate 1.71 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.54 GiB is free. Including non-PyTorch memory, this process has 14.21 GiB memory in use. Of the allocated memory 13.80 GiB is allocated by PyTorch, and 36.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Model: naive_cnn, Fold: 4, Current batch size: 1
2025-12-12 19:15:08 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
