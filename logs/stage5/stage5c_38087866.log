2025-12-12 21:18:35 [INFO] [__main__:310] ================================================================================
2025-12-12 21:18:35 [INFO] [__main__:311] STAGE 5: MODEL TRAINING
2025-12-12 21:18:35 [INFO] [__main__:312] ================================================================================
2025-12-12 21:18:35 [INFO] [__main__:313] Project root: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc
2025-12-12 21:18:35 [INFO] [__main__:314] Scaled metadata: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/scaled_videos/scaled_metadata.arrow
2025-12-12 21:18:35 [INFO] [__main__:315] Features Stage 2: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/features_stage2/features_metadata.arrow
2025-12-12 21:18:35 [INFO] [__main__:316] Features Stage 4: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/features_stage4/features_scaled_metadata.arrow
2025-12-12 21:18:35 [INFO] [__main__:317] Model types: ['naive_cnn']
2025-12-12 21:18:35 [INFO] [__main__:318] K-fold splits: 5
2025-12-12 21:18:35 [INFO] [__main__:319] Number of frames: 1000
2025-12-12 21:18:35 [INFO] [__main__:320] Output directory: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/stage5
2025-12-12 21:18:35 [INFO] [__main__:321] Experiment tracking: Enabled
2025-12-12 21:18:35 [INFO] [__main__:324] Delete existing: True
2025-12-12 21:18:35 [INFO] [__main__:325] Resume mode: True
2025-12-12 21:18:35 [INFO] [__main__:326] Log file: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/logs/stage5_training_1765592315.log
2025-12-12 21:18:35 [INFO] [__main__:333] ================================================================================
2025-12-12 21:18:35 [INFO] [__main__:334] Checking prerequisites...
2025-12-12 21:18:35 [INFO] [__main__:335] ================================================================================
2025-12-12 21:18:35 [INFO] [__main__:342] ✓ Scaled metadata file found: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/scaled_videos/scaled_metadata.arrow
2025-12-12 21:18:35 [INFO] [__main__:352] ✓ All model types are valid
2025-12-12 21:18:35 [INFO] [__main__:358] ================================================================================
2025-12-12 21:18:35 [INFO] [__main__:359] Initial memory statistics:
2025-12-12 21:18:35 [INFO] [__main__:360] ================================================================================
2025-12-12 21:18:35 [INFO] [lib.utils.memory:91] Memory stats (Stage 5: before training): {'cpu_memory_mb': 735.734375, 'cpu_memory_gb': 0.7184906005859375, 'cpu_vms_mb': 10217.6015625, 'gpu_allocated_gb': 0.0, 'gpu_reserved_gb': 0.0, 'gpu_total_gb': 16.928342016, 'gpu_free_gb': 16.928342016}
2025-12-12 21:18:35 [INFO] [__main__:364] ================================================================================
2025-12-12 21:18:35 [INFO] [__main__:365] Starting Stage 5: Model Training
2025-12-12 21:18:35 [INFO] [__main__:366] ================================================================================
2025-12-12 21:18:35 [INFO] [__main__:367] Training 1 model(s) with 5-fold cross-validation
2025-12-12 21:18:35 [INFO] [__main__:368] This may take a while depending on dataset size and model complexity...
2025-12-12 21:18:35 [INFO] [__main__:369] Progress will be logged in real-time
2025-12-12 21:18:35 [INFO] [__main__:370] ================================================================================
2025-12-12 21:18:35 [INFO] [__main__:375] Calling Stage 5 training pipeline...
2025-12-12 21:18:35 [INFO] [__main__:376] This may take a while - progress will be logged in real-time
2025-12-12 21:18:35 [INFO] [lib.training.pipeline:496] ================================================================================
2025-12-12 21:18:35 [INFO] [lib.training.pipeline:497] Stage 5: Model Training Pipeline Started
2025-12-12 21:18:35 [INFO] [lib.training.pipeline:498] ================================================================================
2025-12-12 21:18:35 [INFO] [lib.training.pipeline:499] Model types: ['naive_cnn']
2025-12-12 21:18:35 [INFO] [lib.training.pipeline:500] K-fold splits: 5
2025-12-12 21:18:35 [INFO] [lib.training.pipeline:501] Frames per video: 1000
2025-12-12 21:18:35 [INFO] [lib.training.pipeline:502] Output directory: data/stage5
2025-12-12 21:18:35 [INFO] [lib.training.pipeline:503] Initializing pipeline...
2025-12-12 21:18:35 [INFO] [lib.training.pipeline:265] ================================================================================
2025-12-12 21:18:35 [INFO] [lib.training.pipeline:266] STAGE 5 PREREQUISITE VALIDATION
2025-12-12 21:18:35 [INFO] [lib.training.pipeline:267] ================================================================================
2025-12-12 21:18:35 [INFO] [lib.training.pipeline:270] 
[1/3] Checking Stage 3 (scaled videos) - REQUIRED for all models...
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:279] ✓ Stage 3 metadata found: 3278 scaled videos
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:280]   Path: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/scaled_videos/scaled_metadata.arrow
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:283] 
[2/3] Checking Stage 2 (features) - REQUIRED for *_stage2 models...
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:291] ✓ Stage 2 metadata found: 3278 feature rows
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:292]   Path: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/features_stage2/features_metadata.arrow
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:295] 
[3/3] Checking Stage 4 (scaled features) - REQUIRED for *_stage2_stage4 models...
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:303] ✓ Stage 4 metadata found: 3277 feature rows
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:304]   Path: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/fvc/data/features_stage4/features_scaled_metadata.arrow
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:307] 
================================================================================
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:308] MODEL REQUIREMENTS CHECK
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:309] ================================================================================
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:333] ✓ naive_cnn: CAN RUN
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:336] 
================================================================================
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:337] VALIDATION SUMMARY
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:338] ================================================================================
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:339] Stage 3 (scaled videos): ✓ Available
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:340]   Count: 3278 videos
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:341] Stage 2 (features): ✓ Available
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:342]   Count: 3278 feature rows
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:343] Stage 4 (scaled features): ✓ Available
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:344]   Count: 3277 feature rows
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:345] 
Runnable models: 1/1
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:346]   ['naive_cnn']
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:353] ================================================================================
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:589] 
Stage 5: Loading metadata...
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:615] Loaded metadata: 3278 rows
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:625] ✓ Data validation passed: 3278 rows (> 3000 required)
2025-12-12 21:18:36 [WARNING] [lib.utils.guardrails:171] Disk usage high: free=832983.37GB, used=1666856.63GB (66.7%)
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:657] Stage 5: Found 3278 scaled videos
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:705] Enabling adaptive chunked frame loading for naive_cnn: initial_chunk_size=10, num_frames=1000. Chunk size will adapt automatically based on OOM events (AIMD algorithm).
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:738] Overriding num_frames to 500 for naive_cnn (small-chunk model) to prevent OOM. Original num_frames was 1000.
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:771] Stage 5: Deleting existing model results (clean mode)...
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:780] Deleted existing results for naive_cnn
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:783] Stage 5: Deleted 1 existing model directories
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:788] 
================================================================================
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:789] Stage 5: Training model: naive_cnn
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:790] ================================================================================
2025-12-12 21:18:36 [INFO] [lib.training.pipeline:803] Overriding model_config num_frames to 500 for naive_cnn (small-chunk model) to match video_config.
2025-12-12 21:18:37 [INFO] [lib.training.pipeline:823] Grid search: 18 hyperparameter combinations to try
2025-12-12 21:18:37 [INFO] [lib.training.pipeline:831] ================================================================================
2025-12-12 21:18:37 [INFO] [lib.training.pipeline:832] HYPERPARAMETER SEARCH: Using 20% stratified sample for efficiency
2025-12-12 21:18:37 [INFO] [lib.training.pipeline:833] ================================================================================
2025-12-12 21:18:38 [INFO] [lib.training.pipeline:842] Hyperparameter search sample: 655 rows (20.0% of 3278 total)
2025-12-12 21:18:39 [INFO] [lib.training.pipeline:855] Using 5-fold stratified cross-validation on 20% sample for hyperparameter search
2025-12-12 21:18:39 [INFO] [lib.training.pipeline:868] 
================================================================================
2025-12-12 21:18:39 [INFO] [lib.training.pipeline:869] Grid Search: Hyperparameter combination 1/18
2025-12-12 21:18:39 [INFO] [lib.training.pipeline:870] Parameters: {'learning_rate': 0.0001, 'weight_decay': 1e-05, 'batch_size': 1, 'num_epochs': 20}
2025-12-12 21:18:39 [INFO] [lib.training.pipeline:871] ================================================================================
2025-12-12 21:18:39 [INFO] [lib.training.pipeline:900] 
Hyperparameter Search - naive_cnn - Fold 1/5 (20% sample)
2025-12-12 21:18:39 [INFO] [lib.training.pipeline:1004] Training configuration - Batch size: 1, Gradient accumulation steps: 16, Effective batch size: 16
2025-12-12 21:18:40 [INFO] [lib.training.pipeline:1050] Applied PyTorch memory optimizations: expandable_segments, cudnn.benchmark=False
2025-12-12 21:18:45 [INFO] [lib.mlops.mlflow_tracker:89] MLflow run started: 75d3277c8beb4d4fa6165920a1dbe683 (experiment: naive_cnn)
2025-12-12 21:18:45 [INFO] [lib.training.pipeline:1143] Training PyTorch model naive_cnn on fold 1...
2025-12-12 21:19:10 [INFO] [lib.training.pipeline:1217] Model forward pass test successful. Output shape: torch.Size([1, 2])
2025-12-12 21:19:10 [INFO] [lib.training.trainer:260] Warmup will be handled in training loop (2 epochs)
2025-12-12 21:22:40 [INFO] [lib.training.trainer:452] Epoch 1, Batch 10 (loader has 523 batches), Loss: 0.6982
2025-12-12 21:25:54 [INFO] [lib.training.trainer:452] Epoch 1, Batch 20 (loader has 523 batches), Loss: 0.6880
2025-12-12 21:29:04 [INFO] [lib.training.trainer:452] Epoch 1, Batch 30 (loader has 523 batches), Loss: 0.6880
2025-12-12 21:32:24 [INFO] [lib.training.trainer:452] Epoch 1, Batch 40 (loader has 523 batches), Loss: 0.6992
2025-12-12 21:35:50 [INFO] [lib.training.trainer:452] Epoch 1, Batch 50 (loader has 523 batches), Loss: 0.6865
2025-12-12 21:39:03 [INFO] [lib.training.trainer:452] Epoch 1, Batch 60 (loader has 523 batches), Loss: 0.6865
2025-12-12 21:42:23 [INFO] [lib.training.trainer:452] Epoch 1, Batch 70 (loader has 523 batches), Loss: 0.6865
2025-12-12 21:45:38 [INFO] [lib.training.trainer:452] Epoch 1, Batch 80 (loader has 523 batches), Loss: 0.6865
2025-12-12 21:48:56 [INFO] [lib.training.trainer:452] Epoch 1, Batch 90 (loader has 523 batches), Loss: 0.7007
2025-12-12 21:52:08 [INFO] [lib.training.trainer:452] Epoch 1, Batch 100 (loader has 523 batches), Loss: 0.7017
2025-12-12 21:55:21 [INFO] [lib.training.trainer:452] Epoch 1, Batch 110 (loader has 523 batches), Loss: 0.7021
2025-12-12 21:58:33 [INFO] [lib.training.trainer:452] Epoch 1, Batch 120 (loader has 523 batches), Loss: 0.7012
2025-12-12 22:01:50 [INFO] [lib.training.trainer:452] Epoch 1, Batch 130 (loader has 523 batches), Loss: 0.7026
2025-12-12 22:05:09 [INFO] [lib.training.trainer:452] Epoch 1, Batch 140 (loader has 523 batches), Loss: 0.7026
2025-12-12 22:08:22 [INFO] [lib.training.trainer:452] Epoch 1, Batch 150 (loader has 523 batches), Loss: 0.6826
2025-12-12 22:11:38 [INFO] [lib.training.trainer:452] Epoch 1, Batch 160 (loader has 523 batches), Loss: 0.7026
2025-12-12 22:14:54 [INFO] [lib.training.trainer:452] Epoch 1, Batch 170 (loader has 523 batches), Loss: 0.7036
2025-12-12 22:18:04 [INFO] [lib.training.trainer:452] Epoch 1, Batch 180 (loader has 523 batches), Loss: 0.6826
2025-12-12 22:21:17 [INFO] [lib.training.trainer:452] Epoch 1, Batch 190 (loader has 523 batches), Loss: 0.6826
2025-12-12 22:24:33 [INFO] [lib.training.trainer:452] Epoch 1, Batch 200 (loader has 523 batches), Loss: 0.7036
2025-12-12 22:27:45 [INFO] [lib.training.trainer:452] Epoch 1, Batch 210 (loader has 523 batches), Loss: 0.7036
2025-12-12 22:30:59 [INFO] [lib.training.trainer:452] Epoch 1, Batch 220 (loader has 523 batches), Loss: 0.6831
2025-12-12 22:34:25 [INFO] [lib.training.trainer:452] Epoch 1, Batch 230 (loader has 523 batches), Loss: 0.6831
2025-12-12 22:37:39 [INFO] [lib.training.trainer:452] Epoch 1, Batch 240 (loader has 523 batches), Loss: 0.6831
2025-12-12 22:41:01 [INFO] [lib.training.trainer:452] Epoch 1, Batch 250 (loader has 523 batches), Loss: 0.7031
2025-12-12 22:44:13 [INFO] [lib.training.trainer:452] Epoch 1, Batch 260 (loader has 523 batches), Loss: 0.6831
2025-12-12 22:47:28 [INFO] [lib.training.trainer:452] Epoch 1, Batch 270 (loader has 523 batches), Loss: 0.7017
2025-12-12 22:50:40 [INFO] [lib.training.trainer:452] Epoch 1, Batch 280 (loader has 523 batches), Loss: 0.7026
2025-12-12 22:54:10 [INFO] [lib.training.trainer:452] Epoch 1, Batch 290 (loader has 523 batches), Loss: 0.6846
2025-12-12 22:57:28 [INFO] [lib.training.trainer:452] Epoch 1, Batch 300 (loader has 523 batches), Loss: 0.6846
2025-12-12 23:00:37 [INFO] [lib.training.trainer:452] Epoch 1, Batch 310 (loader has 523 batches), Loss: 0.6841
2025-12-12 23:03:54 [INFO] [lib.training.trainer:452] Epoch 1, Batch 320 (loader has 523 batches), Loss: 0.7026
2025-12-12 23:07:14 [INFO] [lib.training.trainer:452] Epoch 1, Batch 330 (loader has 523 batches), Loss: 0.6846
2025-12-12 23:10:27 [INFO] [lib.training.trainer:452] Epoch 1, Batch 340 (loader has 523 batches), Loss: 0.6846
2025-12-12 23:13:41 [INFO] [lib.training.trainer:452] Epoch 1, Batch 350 (loader has 523 batches), Loss: 0.6841
2025-12-12 23:17:06 [INFO] [lib.training.trainer:452] Epoch 1, Batch 360 (loader has 523 batches), Loss: 0.6851
2025-12-12 23:20:20 [INFO] [lib.training.trainer:452] Epoch 1, Batch 370 (loader has 523 batches), Loss: 0.7021
2025-12-12 23:23:42 [INFO] [lib.training.trainer:452] Epoch 1, Batch 380 (loader has 523 batches), Loss: 0.7012
2025-12-12 23:27:15 [INFO] [lib.training.trainer:452] Epoch 1, Batch 390 (loader has 523 batches), Loss: 0.7021
2025-12-12 23:30:25 [INFO] [lib.training.trainer:452] Epoch 1, Batch 400 (loader has 523 batches), Loss: 0.7021
2025-12-12 23:33:42 [INFO] [lib.training.trainer:452] Epoch 1, Batch 410 (loader has 523 batches), Loss: 0.6855
2025-12-12 23:36:57 [INFO] [lib.training.trainer:452] Epoch 1, Batch 420 (loader has 523 batches), Loss: 0.7021
2025-12-12 23:40:11 [INFO] [lib.training.trainer:452] Epoch 1, Batch 430 (loader has 523 batches), Loss: 0.6846
2025-12-12 23:43:36 [INFO] [lib.training.trainer:452] Epoch 1, Batch 440 (loader has 523 batches), Loss: 0.7021
2025-12-12 23:46:51 [INFO] [lib.training.trainer:452] Epoch 1, Batch 450 (loader has 523 batches), Loss: 0.6846
2025-12-12 23:50:07 [INFO] [lib.training.trainer:452] Epoch 1, Batch 460 (loader has 523 batches), Loss: 0.6997
2025-12-12 23:53:22 [INFO] [lib.training.trainer:452] Epoch 1, Batch 470 (loader has 523 batches), Loss: 0.6855
2025-12-12 23:56:42 [INFO] [lib.training.trainer:452] Epoch 1, Batch 480 (loader has 523 batches), Loss: 0.6851
2025-12-12 23:59:52 [INFO] [lib.training.trainer:452] Epoch 1, Batch 490 (loader has 523 batches), Loss: 0.7002
2025-12-13 00:03:07 [INFO] [lib.training.trainer:452] Epoch 1, Batch 500 (loader has 523 batches), Loss: 0.6851
2025-12-13 00:06:26 [INFO] [lib.training.trainer:452] Epoch 1, Batch 510 (loader has 523 batches), Loss: 0.6851
2025-12-13 00:09:42 [INFO] [lib.training.trainer:452] Epoch 1, Batch 520 (loader has 523 batches), Loss: 0.6860
2025-12-13 00:10:41 [INFO] [lib.training.trainer:697] Epoch 1/20, Train Loss: 0.6934, LR: 5.50e-05
2025-12-13 00:53:31 [INFO] [lib.training.trainer:711] Epoch 1, Val Loss: 0.6928, Val Acc: 0.4924, Val F1: 0.0000, Val Precision: 0.0000, Val Recall: 0.0000
2025-12-13 00:53:31 [INFO] [lib.training.trainer:727] New best validation accuracy: 0.4924
2025-12-13 00:57:01 [INFO] [lib.training.trainer:452] Epoch 2, Batch 10 (loader has 523 batches), Loss: 0.6851
2025-12-13 01:00:17 [INFO] [lib.training.trainer:452] Epoch 2, Batch 20 (loader has 523 batches), Loss: 0.6851
2025-12-13 01:03:31 [INFO] [lib.training.trainer:452] Epoch 2, Batch 30 (loader has 523 batches), Loss: 0.6860
2025-12-13 01:06:42 [INFO] [lib.training.trainer:452] Epoch 2, Batch 40 (loader has 523 batches), Loss: 0.7012
2025-12-13 01:10:04 [INFO] [lib.training.trainer:452] Epoch 2, Batch 50 (loader has 523 batches), Loss: 0.6860
2025-12-13 01:13:16 [INFO] [lib.training.trainer:452] Epoch 2, Batch 60 (loader has 523 batches), Loss: 0.6860
2025-12-13 01:16:35 [INFO] [lib.training.trainer:452] Epoch 2, Batch 70 (loader has 523 batches), Loss: 0.7002
2025-12-13 01:19:48 [INFO] [lib.training.trainer:452] Epoch 2, Batch 80 (loader has 523 batches), Loss: 0.6855
2025-12-13 01:23:18 [INFO] [lib.training.trainer:452] Epoch 2, Batch 90 (loader has 523 batches), Loss: 0.7012
2025-12-13 01:26:31 [INFO] [lib.training.trainer:452] Epoch 2, Batch 100 (loader has 523 batches), Loss: 0.7002
2025-12-13 01:29:50 [INFO] [lib.training.trainer:452] Epoch 2, Batch 110 (loader has 523 batches), Loss: 0.7002
2025-12-13 01:33:02 [INFO] [lib.training.trainer:452] Epoch 2, Batch 120 (loader has 523 batches), Loss: 0.6855
2025-12-13 01:36:19 [INFO] [lib.training.trainer:452] Epoch 2, Batch 130 (loader has 523 batches), Loss: 0.7007
2025-12-13 01:39:30 [INFO] [lib.training.trainer:452] Epoch 2, Batch 140 (loader has 523 batches), Loss: 0.7012
2025-12-13 01:42:44 [INFO] [lib.training.trainer:452] Epoch 2, Batch 150 (loader has 523 batches), Loss: 0.7007
2025-12-13 01:45:56 [INFO] [lib.training.trainer:452] Epoch 2, Batch 160 (loader has 523 batches), Loss: 0.6855
2025-12-13 01:49:07 [INFO] [lib.training.trainer:452] Epoch 2, Batch 170 (loader has 523 batches), Loss: 0.6851
2025-12-13 01:52:44 [INFO] [lib.training.trainer:452] Epoch 2, Batch 180 (loader has 523 batches), Loss: 0.6846
2025-12-13 01:55:59 [INFO] [lib.training.trainer:452] Epoch 2, Batch 190 (loader has 523 batches), Loss: 0.7012
2025-12-13 01:59:15 [INFO] [lib.training.trainer:452] Epoch 2, Batch 200 (loader has 523 batches), Loss: 0.6846
