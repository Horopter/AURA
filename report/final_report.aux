\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{rossler2019faceforensics}
\citation{guera2018deepfake}
\citation{feichtenhofer2019slowfast}
\citation{feichtenhofer2020x3d}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Project Goals}{2}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}\protected@file@percent }
\@writefile{brf}{\backcite{rossler2019faceforensics}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{guera2018deepfake}{{2}{2}{section.2}}}
\citation{ciftci2020fakecatcher}
\citation{gragnaniello2021gan}
\citation{carlini2020evading}
\@writefile{brf}{\backcite{feichtenhofer2019slowfast}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{feichtenhofer2020x3d}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{ciftci2020fakecatcher}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{gragnaniello2021gan}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{carlini2020evading}{{3}{2}{section.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methods}{3}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}System Architecture}{3}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Model Architectures}{4}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Feature Engineering}{5}{subsection.3.3}\protected@file@percent }
\citation{papadopoulou2018corpus}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Datasets}{6}{subsection.3.4}\protected@file@percent }
\newlabel{sec:datasets}{{3.4}{6}{Datasets}{subsection.3.4}{}}
\@writefile{brf}{\backcite{papadopoulou2018corpus}{{6}{3.4}{subsection.3.4}}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Evaluation and Results}{6}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Evaluation Protocols}{6}{subsection.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Model performance on fake video classification. Metrics reported as mean $\pm $ std across 5-fold CV. $^*$ViT-GRU shows signs of overfitting (see Section~\ref {sec:overfitting}).}}{7}{table.1}\protected@file@percent }
\newlabel{tab:results}{{1}{7}{Model performance on fake video classification. Metrics reported as mean $\pm $ std across 5-fold CV. $^*$ViT-GRU shows signs of overfitting (see Section~\ref {sec:overfitting})}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Key Findings}{7}{subsection.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces ROC and Precision-Recall curves comparing baseline logistic regression (top) with XGBoost on handcrafted features (bottom). XGBoost achieves AUC $= 0.997$ and AP $= 0.997$, demonstrating superior class separation. However, such near-perfect metrics warrant careful validation to ensure generalization.}}{8}{figure.1}\protected@file@percent }
\newlabel{fig:roc_pr_baseline}{{1}{8}{ROC and Precision-Recall curves comparing baseline logistic regression (top) with XGBoost on handcrafted features (bottom). XGBoost achieves AUC $= 0.997$ and AP $= 0.997$, demonstrating superior class separation. However, such near-perfect metrics warrant careful validation to ensure generalization}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Cross-Validation Stability Analysis}{8}{subsection.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Cross-validation fold comparison for (a) Logistic Regression and (b) SVM. Traditional ML models show higher variance ($\sigma _{\text  {F1}} \approx 0.036$), indicating greater sensitivity to data distribution variations.}}{9}{figure.2}\protected@file@percent }
\newlabel{fig:cv_comparison_traditional}{{2}{9}{Cross-validation fold comparison for (a) Logistic Regression and (b) SVM. Traditional ML models show higher variance ($\sigma _{\text {F1}} \approx 0.036$), indicating greater sensitivity to data distribution variations}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Cross-validation fold comparison for XGBoost ensemble models: (a) XGBoost-I3D and (b) XGBoost-R(2+1)D. Ensemble models show excellent stability ($\sigma _{\text  {F1}} \approx 0.010$--$0.015$), combining deep feature extraction with robust classification.}}{9}{figure.3}\protected@file@percent }
\newlabel{fig:cv_ensemble}{{3}{9}{Cross-validation fold comparison for XGBoost ensemble models: (a) XGBoost-I3D and (b) XGBoost-R(2+1)D. Ensemble models show excellent stability ($\sigma _{\text {F1}} \approx 0.010$--$0.015$), combining deep feature extraction with robust classification}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Ensemble Model Analysis}{9}{subsection.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Cross-validation performance for XGBoost-ViT-GRU ensemble. ViT-GRU shows suspiciously consistent near-perfect performance across all folds (F1 $= 0.996 \pm 0.002$), indicating potential overfitting.}}{10}{figure.4}\protected@file@percent }
\newlabel{fig:cv_vit}{{4}{10}{Cross-validation performance for XGBoost-ViT-GRU ensemble. ViT-GRU shows suspiciously consistent near-perfect performance across all folds (F1 $= 0.996 \pm 0.002$), indicating potential overfitting}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Hyperparameter Sensitivity}{10}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Comprehensive Model Comparison}{10}{subsection.4.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Hyperparameter search results showing the impact of regularization parameters on model performance: (a) Logistic Regression and (b) SVM. Optimal configurations use moderate regularization ($C = 0.01$--$0.1$) with elastic net penalties, achieving F1 scores around 0.75 for logistic regression. The plots reveal clear sensitivity to hyperparameter choices.}}{11}{figure.5}\protected@file@percent }
\newlabel{fig:hyperparameter}{{5}{11}{Hyperparameter search results showing the impact of regularization parameters on model performance: (a) Logistic Regression and (b) SVM. Optimal configurations use moderate regularization ($C = 0.01$--$0.1$) with elastic net penalties, achieving F1 scores around 0.75 for logistic regression. The plots reveal clear sensitivity to hyperparameter choices}{figure.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Cross-validation stability comparison across model families. Lower variance indicates better generalization. Performance-variance trade-off analysis guides production deployment decisions.}}{11}{table.2}\protected@file@percent }
\newlabel{tab:cv_stability}{{2}{11}{Cross-validation stability comparison across model families. Lower variance indicates better generalization. Performance-variance trade-off analysis guides production deployment decisions}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Performance vs. Generalization Trade-off}{12}{subsection.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8}Synthesized Insights from All Visualizations}{12}{subsection.4.8}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Comprehensive comparison synthesizing insights from all figures. Performance-variance trade-off guides deployment decisions.}}{14}{table.3}\protected@file@percent }
\newlabel{tab:synthesis}{{3}{14}{Comprehensive comparison synthesizing insights from all figures. Performance-variance trade-off guides deployment decisions}{table.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Platform transformation robustness analysis. Models trained with comprehensive augmentation show resilience to common platform transformations.}}{15}{table.4}\protected@file@percent }
\newlabel{tab:robustness}{{4}{15}{Platform transformation robustness analysis. Models trained with comprehensive augmentation show resilience to common platform transformations}{table.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9}Ablation Study}{15}{subsection.4.9}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Ablation study: Impact of feature engineering and model architecture choices.}}{16}{table.5}\protected@file@percent }
\newlabel{tab:ablation}{{5}{16}{Ablation study: Impact of feature engineering and model architecture choices}{table.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Feature retention and regularization indicators across models. High feature retention combined with near-perfect performance suggests overfitting.}}{16}{table.6}\protected@file@percent }
\newlabel{tab:overfitting}{{6}{16}{Feature retention and regularization indicators across models. High feature retention combined with near-perfect performance suggests overfitting}{table.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10}Overfitting Analysis}{16}{subsection.4.10}\protected@file@percent }
\newlabel{sec:overfitting}{{4.10}{16}{Overfitting Analysis}{subsection.4.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Statistical significance of performance improvements. $p < 0.001$ indicates highly significant improvement.}}{17}{table.7}\protected@file@percent }
\newlabel{tab:statistical}{{7}{17}{Statistical significance of performance improvements. $p < 0.001$ indicates highly significant improvement}{table.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11}Statistical Analysis and Model Comparison}{17}{subsection.4.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12}Computational Requirements}{18}{subsection.4.12}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion and Conclusion}{19}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Quantitative Performance Summary}{19}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Reflection}{20}{section.6}\protected@file@percent }
\citation{papadopoulou2018corpus}
\bibstyle{ieee_fullname}
\bibcite{papadopoulou2018corpus}{1}
\bibcite{rossler2019faceforensics}{2}
\bibcite{feichtenhofer2019slowfast}{3}
\bibcite{feichtenhofer2020x3d}{4}
\bibcite{ciftci2020fakecatcher}{5}
\bibcite{guera2018deepfake}{6}
\bibcite{gragnaniello2021gan}{7}
\bibcite{carlini2020evading}{8}
\@writefile{brf}{\backcite{papadopoulou2018corpus}{{22}{6}{section*.1}}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Pipeline Architecture Details}{23}{appendix.A}\protected@file@percent }
\newlabel{app:pipeline}{{A}{23}{Pipeline Architecture Details}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Stage 1: Video Ingestion and Preprocessing}{23}{subsection.A.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Stage 2: Multi-Modal Augmentation}{23}{subsection.A.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Stage 3: Feature Extraction}{23}{subsection.A.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Stage 4: Classification}{24}{subsection.A.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.5}Stage 5: Post-Processing}{24}{subsection.A.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}Feature Engineering Details}{24}{appendix.B}\protected@file@percent }
\newlabel{app:features}{{B}{24}{Feature Engineering Details}{appendix.B}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Handcrafted Features (Stage 2)}{24}{subsection.B.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Scaled Features (Stage 4)}{25}{subsection.B.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}Deep Learning Features}{25}{subsection.B.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {C}Training Strategy and Hyperparameters}{26}{appendix.C}\protected@file@percent }
\newlabel{app:training}{{C}{26}{Training Strategy and Hyperparameters}{appendix.C}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Cross-Validation Protocol}{26}{subsection.C.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}Hyperparameter Search}{26}{subsection.C.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.3}Regularization Strategies}{27}{subsection.C.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.4}Optimization Details}{27}{subsection.C.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.5}Data Augmentation During Training}{28}{subsection.C.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {D}Memory Optimizations}{28}{appendix.D}\protected@file@percent }
\newlabel{app:memory}{{D}{28}{Memory Optimizations}{appendix.D}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.1}Frame-by-Frame Video Decoding}{28}{subsection.D.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.2}Adaptive Chunked Frame Loading}{28}{subsection.D.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.3}OOM Error Handling}{28}{subsection.D.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.4}Resource Allocation}{29}{subsection.D.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.5}Training Time Measurements}{29}{subsection.D.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.6}Memory Usage by Model Type}{29}{subsection.D.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {E}Caching Mechanisms}{30}{appendix.E}\protected@file@percent }
\newlabel{app:caching}{{E}{30}{Caching Mechanisms}{appendix.E}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E.1}Frame Caching}{30}{subsection.E.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {E.2}Video Metadata Caching}{30}{subsection.E.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {E.3}Cache Management}{31}{subsection.E.3}\protected@file@percent }
\gdef \@abspage@last{31}
