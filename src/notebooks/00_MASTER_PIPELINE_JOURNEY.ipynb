{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FVC Deepfake Detection: Complete Production Pipeline\n",
    "\n",
    "**From Raw ZIP Archives to Production-Ready ML Models**\n",
    "\n",
    "This comprehensive notebook demonstrates a production-grade machine learning pipeline for deepfake video detection, showcasing:\n",
    "- **Data Engineering**: Extraction, validation, and preprocessing\n",
    "- **Feature Engineering**: Handcrafted features with domain expertise\n",
    "- **Model Architecture**: 23 diverse models from baselines to state-of-the-art\n",
    "- **MLOps Infrastructure**: MLflow, Airflow, DuckDB integration\n",
    "- **Production Practices**: 5-fold CV, hyperparameter optimization, experiment tracking\n",
    "\n",
    "**Target Audience**: ML Engineers, Research Scientists, Hiring Managers\n",
    "**Level**: Production-Grade, Research-Quality Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Infrastructure & Requirements](#1-infrastructure--requirements)\n",
    "2. [Data Extraction & Exploration](#2-data-extraction--exploration)\n",
    "3. [Stage 1: Video Augmentation Strategy](#3-stage-1-video-augmentation-strategy)\n",
    "4. [Stage 2: Handcrafted Feature Engineering](#4-stage-2-handcrafted-feature-engineering)\n",
    "5. [Stage 3: Video Scaling & Normalization](#5-stage-3-video-scaling--normalization)\n",
    "6. [Stage 4: Scaled Feature Extraction](#6-stage-4-scaled-feature-extraction)\n",
    "7. [Stage 5: Model Training Architecture](#7-stage-5-model-training-architecture)\n",
    "8. [MLOps: Experiment Tracking with MLflow](#8-mlops-experiment-tracking-with-mlflow)\n",
    "9. [Analytics: DuckDB for Fast Queries](#9-analytics-duckdb-for-fast-queries)\n",
    "10. [Orchestration: Apache Airflow DAGs](#10-orchestration-apache-airflow-dags)\n",
    "11. [Model Evaluation & Results](#11-model-evaluation--results)\n",
    "12. [Production Deployment Considerations](#12-production-deployment-considerations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Infrastructure & Requirements\n",
    "\n",
    "### Technology Stack\n",
    "\n",
    "**Deep Learning Framework**:\n",
    "- PyTorch 2.0+ with CUDA 11.8+ support\n",
    "- torchvision 0.15+ for video models (X3D, SlowFast, R(2+1)D, I3D)\n",
    "- timm 0.9+ for Vision Transformers (ViT, TimeSformer, ViViT)\n",
    "- transformers 4.30+ for HuggingFace model integration\n",
    "\n",
    "**Data Processing Stack**:\n",
    "- **Polars 0.19+**: Columnar DataFrame library (10-100x faster than pandas)\n",
    "- **PyArrow 14+**: In-memory columnar format (Arrow) and file format (Parquet)\n",
    "- **DuckDB 0.9+**: In-process analytical SQL database for fast queries\n",
    "- **Pandera 0.18+**: DataFrame schema validation\n",
    "\n",
    "**MLOps & Orchestration**:\n",
    "- **MLflow 2.8+**: Experiment tracking, model registry, artifact management\n",
    "- **Apache Airflow 2.7+**: Workflow orchestration, dependency management, scheduling\n",
    "- **Custom MLOps**: ExperimentTracker, CheckpointManager, RunConfig\n",
    "\n",
    "**Video Processing**:\n",
    "- **PyAV 10.0+**: Pythonic FFmpeg bindings for efficient video I/O\n",
    "- **OpenCV 4.8+**: Computer vision operations (feature extraction, transforms)\n",
    "- **FFmpeg/ffprobe**: Codec analysis, metadata extraction\n",
    "- **PyTorchVideo 0.1.5+**: Video model library (I3D, X3D, SlowFast)\n",
    "\n",
    "**Feature Engineering**:\n",
    "- **NumPy 1.24+**: Signal processing, DCT transforms\n",
    "- **scikit-image**: Image analysis, filters\n",
    "- **scipy 1.11+**: Statistical functions\n",
    "\n",
    "**Machine Learning**:\n",
    "- **scikit-learn 1.3+**: Baseline models (Logistic Regression, SVM)\n",
    "- **XGBoost 2.0+**: Gradient boosting with pretrained feature extractors\n",
    "- **joblib 1.3+**: Model serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML, Video, Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"üìÅ Project root: {project_root}\")\n",
    "print(f\"üêç Python version: {sys.version.split()[0]}\")\n",
    "print(f\"\\n‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify infrastructure stack\n",
    "import torch\n",
    "import torchvision\n",
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "\n",
    "infrastructure_status = {}\n",
    "\n",
    "# Deep Learning\n",
    "infrastructure_status['PyTorch'] = f\"{torch.__version__}\"\n",
    "infrastructure_status['torchvision'] = f\"{torchvision.__version__}\"\n",
    "if torch.cuda.is_available():\n",
    "    infrastructure_status['CUDA'] = f\"{torch.version.cuda} ({torch.cuda.get_device_name(0)})\"\n",
    "else:\n",
    "    infrastructure_status['CUDA'] = \"Not available (CPU mode)\"\n",
    "\n",
    "# Data Processing\n",
    "infrastructure_status['Polars'] = pl.__version__\n",
    "infrastructure_status['PyArrow'] = pa.__version__\n",
    "\n",
    "# MLOps\n",
    "try:\n",
    "    import mlflow\n",
    "    infrastructure_status['MLflow'] = mlflow.__version__\n",
    "except ImportError:\n",
    "    infrastructure_status['MLflow'] = \"Not installed\"\n",
    "\n",
    "try:\n",
    "    import duckdb\n",
    "    infrastructure_status['DuckDB'] = duckdb.__version__\n",
    "except ImportError:\n",
    "    infrastructure_status['DuckDB'] = \"Not installed\"\n",
    "\n",
    "# Display status\n",
    "status_df = pd.DataFrame(list(infrastructure_status.items()), columns=['Component', 'Version'])\n",
    "display(status_df.style.set_properties(**{'text-align': 'left'}).set_table_styles([\n",
    "    {'selector': 'th', 'props': [('background-color', '#4CAF50'), ('color', 'white'), ('font-weight', 'bold')]}\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Extraction & Exploration\n",
    "\n",
    "### Initial Data Structure\n",
    "\n",
    "The FVC (Fake Video Challenge) dataset comes as password-protected ZIP archives:\n",
    "- `FVC1.zip`, `FVC2.zip`, `FVC3.zip`: Video files (MP4 format)\n",
    "- `Metadata.zip`: CSV metadata files with labels (real/fake)\n",
    "\n",
    "### Data Extraction Process\n",
    "\n",
    "**Location**: `src/setup_fvc_dataset.py`\n",
    "\n",
    "**Steps**:\n",
    "1. Extract videos from ZIP archives to `data/videos/`\n",
    "2. Copy metadata CSV files to `archive/`\n",
    "3. Build comprehensive video index with:\n",
    "   - File paths and sizes\n",
    "   - Video metadata (duration, fps, resolution, codec)\n",
    "   - Labels (real/fake)\n",
    "   - Data integrity checks\n",
    "4. Generate metadata: `data/videos/video_index.parquet`\n",
    "\n",
    "### Data Exploration Rationale\n",
    "\n",
    "**Why Explore Before Processing?**\n",
    "- **Class Distribution**: Check for imbalance (affects loss functions, sampling)\n",
    "- **Video Statistics**: Duration, resolution, codec diversity (affects preprocessing)\n",
    "- **Data Quality**: Corrupted files, missing metadata (affects pipeline robustness)\n",
    "- **Storage Requirements**: Estimate disk space for augmented/scaled videos\n",
    "- **Processing Time**: Estimate pipeline duration based on video counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for archive files and extracted data\n",
    "archive_dir = project_root / \"archive\"\n",
    "data_dir = project_root / \"data\"\n",
    "videos_dir = data_dir / \"videos\"\n",
    "\n",
    "print(\"üì¶ Archive Directory:\")\n",
    "if archive_dir.exists():\n",
    "    zip_files = list(archive_dir.glob(\"*.zip\"))\n",
    "    csv_files = list(archive_dir.glob(\"*.csv\"))\n",
    "    \n",
    "    print(f\"  ‚úì Found {len(zip_files)} ZIP archives\")\n",
    "    for f in zip_files:\n",
    "        size_gb = f.stat().st_size / (1024**3)\n",
    "        print(f\"    - {f.name}: {size_gb:.2f} GB\")\n",
    "    \n",
    "    print(f\"\\n  ‚úì Found {len(csv_files)} CSV metadata files\")\n",
    "    for f in csv_files:\n",
    "        print(f\"    - {f.name}\")\n",
    "else:\n",
    "    print(\"  ‚ö† Archive directory not found\")\n",
    "\n",
    "print(\"\\nüìÅ Data Directory:\")\n",
    "if videos_dir.exists():\n",
    "    video_files = list(videos_dir.glob(\"*.mp4\"))\n",
    "    index_file = videos_dir / \"video_index.parquet\"\n",
    "    \n",
    "    print(f\"  ‚úì Found {len(video_files)} video files\")\n",
    "    print(f\"  ‚úì Index file exists: {index_file.exists()}\")\n",
    "    \n",
    "    if index_file.exists():\n",
    "        from lib.utils.paths import load_metadata_flexible\n",
    "        index_df = load_metadata_flexible(str(index_file))\n",
    "        if index_df is not None:\n",
    "            print(f\"\\n  üìä Video Index Statistics:\")\n",
    "            print(f\"    - Total videos: {index_df.height}\")\n",
    "            if 'label' in index_df.columns:\n",
    "                label_counts = index_df['label'].value_counts()\n",
    "                print(f\"    - Class distribution:\")\n",
    "                for label, count in label_counts.items():\n",
    "                    print(f\"      {label}: {count} ({100*count/index_df.height:.1f}%)\")\n",
    "else:\n",
    "    print(\"  ‚ö† Videos directory not found - run setup_fvc_dataset.py first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore metadata\n",
    "if videos_dir.exists() and (videos_dir / \"video_index.parquet\").exists():\n",
    "    from lib.utils.paths import load_metadata_flexible\n",
    "    \n",
    "    index_df = load_metadata_flexible(str(videos_dir / \"video_index.parquet\"))\n",
    "    \n",
    "    if index_df is not None and index_df.height > 0:\n",
    "        # Convert to pandas for visualization\n",
    "        index_pd = index_df.to_pandas()\n",
    "        \n",
    "        # Class distribution\n",
    "        if 'label' in index_pd.columns:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "            \n",
    "            # Bar plot\n",
    "            label_counts = index_pd['label'].value_counts()\n",
    "            axes[0].bar(label_counts.index, label_counts.values, color=['#4CAF50', '#f44336'])\n",
    "            axes[0].set_title('Class Distribution', fontsize=14, fontweight='bold')\n",
    "            axes[0].set_xlabel('Label')\n",
    "            axes[0].set_ylabel('Count')\n",
    "            axes[0].grid(axis='y', alpha=0.3)\n",
    "            \n",
    "            # Pie chart\n",
    "            axes[1].pie(label_counts.values, labels=label_counts.index, autopct='%1.1f%%', \n",
    "                       colors=['#4CAF50', '#f44336'], startangle=90)\n",
    "            axes[1].set_title('Class Proportion', fontsize=14, fontweight='bold')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Video statistics\n",
    "            if 'duration' in index_pd.columns:\n",
    "                fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "                \n",
    "                # Duration distribution\n",
    "                axes[0].hist(index_pd['duration'], bins=30, color='#2196F3', edgecolor='black', alpha=0.7)\n",
    "                axes[0].set_title('Video Duration Distribution', fontsize=14, fontweight='bold')\n",
    "                axes[0].set_xlabel('Duration (seconds)')\n",
    "                axes[0].set_ylabel('Frequency')\n",
    "                axes[0].grid(axis='y', alpha=0.3)\n",
    "                \n",
    "                # FPS distribution\n",
    "                if 'fps' in index_pd.columns:\n",
    "                    axes[1].hist(index_pd['fps'], bins=30, color='#FF9800', edgecolor='black', alpha=0.7)\n",
    "                    axes[1].set_title('Frame Rate Distribution', fontsize=14, fontweight='bold')\n",
    "                    axes[1].set_xlabel('FPS')\n",
    "                    axes[1].set_ylabel('Frequency')\n",
    "                    axes[1].grid(axis='y', alpha=0.3)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        \n",
    "        print(\"\\nüìà Summary Statistics:\")\n",
    "        display(index_pd.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stage 1: Video Augmentation Strategy\n",
    "\n",
    "### Why Augmentation?\n",
    "\n",
    "**Problem**: Limited dataset size (typically 200-500 videos)\n",
    "- **Overfitting Risk**: Small datasets lead to poor generalization\n",
    "- **Limited Diversity**: Real-world videos have infinite variations\n",
    "- **Class Imbalance**: May need more samples of minority class\n",
    "\n",
    "**Solution**: Data augmentation to increase dataset diversity\n",
    "- **10x Dataset Expansion**: Generate 10 augmented versions per video\n",
    "- **Spatial Diversity**: Rotation, flip, color jitter, noise, blur\n",
    "- **Temporal Diversity**: Frame dropping, duplication, reversal\n",
    "\n",
    "### Augmentation Types & Rationale\n",
    "\n",
    "**Spatial Augmentations** (applied per-frame):\n",
    "\n",
    "1. **Rotation (¬±10¬∞)**: Simulates camera angle variation, handles tilted videos\n",
    "2. **Horizontal Flip**: Doubles dataset, preserves temporal structure (no semantic change for faces)\n",
    "3. **Brightness/Contrast/Saturation Jitter**: Handles lighting variations, different cameras\n",
    "4. **Gaussian Noise**: Adds robustness to compression artifacts, low-quality captures\n",
    "5. **Gaussian Blur**: Simulates motion blur, out-of-focus captures\n",
    "6. **Affine Transformations**: Translation, scale, shear (handles camera movement)\n",
    "7. **Elastic Transform**: Simulates non-rigid deformations (handles perspective changes)\n",
    "8. **Cutout (Random Erasing)**: Occlusion robustness, prevents overfitting to specific regions\n",
    "\n",
    "**Temporal Augmentations** (applied to sequence):\n",
    "\n",
    "1. **Frame Dropping (up to 25%)**: Handles variable frame rates, temporal compression\n",
    "2. **Frame Duplication**: Slow motion effect, temporal interpolation\n",
    "3. **Temporal Reversal**: Time-reversed videos (doubles temporal diversity)\n",
    "\n",
    "### Implementation: Pre-Generated vs On-the-Fly\n",
    "\n",
    "**Why Pre-Generated Augmentations?**\n",
    "\n",
    "**Advantages**:\n",
    "- ‚úÖ **Reproducibility**: Same augmentations across runs (deterministic seeds)\n",
    "- ‚úÖ **Speed**: No augmentation overhead during training (10-100x faster)\n",
    "- ‚úÖ **Caching**: Can store on disk, share across experiments\n",
    "- ‚úÖ **Memory Efficiency**: Frame-by-frame decoding (50x memory reduction vs loading full videos)\n",
    "- ‚úÖ **Debugging**: Can inspect augmented videos before training\n",
    "\n",
    "**Trade-offs**:\n",
    "- ‚ö†Ô∏è **Disk Space**: 10x dataset size (mitigated by scaling to 256px max dimension)\n",
    "- ‚ö†Ô∏è **Initial Processing Time**: One-time cost (parallelizable)\n",
    "\n",
    "**Location**: `lib/augmentation/pipeline.py`, `lib/augmentation/transforms.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for augmented videos\n",
    "augmented_dir = data_dir / \"augmented_videos\"\n",
    "augmented_metadata = augmented_dir / \"augmented_metadata.parquet\"\n",
    "\n",
    "if augmented_metadata.exists():\n",
    "    from lib.utils.paths import load_metadata_flexible\n",
    "    \n",
    "    aug_df = load_metadata_flexible(str(augmented_metadata))\n",
    "    \n",
    "    if aug_df is not None:\n",
    "        print(f\"‚úÖ Stage 1 Complete: {aug_df.height} augmented videos\")\n",
    "        \n",
    "        # Show augmentation statistics\n",
    "        if 'augmentation_type' in aug_df.columns:\n",
    "            aug_counts = aug_df['augmentation_type'].value_counts()\n",
    "            print(f\"\\nüìä Augmentation Type Distribution:\")\n",
    "            for aug_type, count in aug_counts.items():\n",
    "                print(f\"  - {aug_type}: {count}\")\n",
    "        \n",
    "        # Sample augmented video\n",
    "        sample_video = aug_df.filter(pl.col('label') == 'real').head(1)\n",
    "        if sample_video.height > 0:\n",
    "            video_path = sample_video['video_path'][0]\n",
    "            print(f\"\\nüé¨ Sample Augmented Video: {Path(video_path).name}\")\n",
    "            # Note: Video display requires actual video file\n",
    "            # Video(video_path, width=400)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Stage 1 not completed - augmented videos not found\")\n",
    "    print(\"\\nüí° To run Stage 1:\")\n",
    "    print(\"```python\")\n",
    "    print(\"from lib.augmentation.pipeline import stage1_augment_videos\")\n",
    "    print(\"\")\n",
    "    print(\"stage1_augment_videos(\")\n",
    "    print(\"    project_root='.',\")\n",
    "    print(\"    num_augmentations=10,\")\n",
    "    print(\"    output_dir='data/augmented_videos'\")\n",
    "    print(\")\")\n",
    "    print(\"```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stage 2: Handcrafted Feature Engineering\n",
    "\n",
    "### Why Handcrafted Features?\n",
    "\n",
    "**Domain Knowledge**: Deepfake videos exhibit specific artifacts that can be detected:\n",
    "- **Compression Artifacts**: Block boundaries, DCT patterns\n",
    "- **Face Swap Artifacts**: Inconsistencies at boundaries\n",
    "- **Temporal Inconsistencies**: Frame-to-frame variations\n",
    "- **Codec Cues**: Compression parameters differ between real and fake\n",
    "\n",
    "**Advantages**:\n",
    "- **Interpretability**: Features have clear meaning\n",
    "- **Efficiency**: Fast extraction, small feature vectors (~50 features)\n",
    "- **Baseline Models**: Enable simple models (Logistic Regression, SVM)\n",
    "- **Complementary**: Can be combined with deep learning features\n",
    "\n",
    "### Feature Types & Extraction Methods\n",
    "\n",
    "**1. Noise Residual Energy** (3 features)\n",
    "- **Method**: High-pass filter to extract noise patterns\n",
    "- **Rationale**: Deepfakes often have different noise characteristics\n",
    "- **Features**: Total energy, mean energy, std energy\n",
    "\n",
    "**2. DCT Band Statistics** (5 features)\n",
    "- **Method**: Discrete Cosine Transform on 8x8 blocks (JPEG-like)\n",
    "- **Rationale**: Compression artifacts differ between real and fake\n",
    "- **Features**: DC coefficient mean/std, AC coefficient mean/std/energy\n",
    "\n",
    "**3. Blur/Sharpness Metrics** (3 features)\n",
    "- **Method**: Laplacian variance (sharpness), gradient mean (edge strength)\n",
    "- **Rationale**: Deepfakes may have different sharpness characteristics\n",
    "- **Features**: Laplacian variance, gradient mean, gradient std\n",
    "\n",
    "**4. Block Boundary Inconsistency** (1 feature)\n",
    "- **Method**: Detect inconsistencies at 8x8 block boundaries\n",
    "- **Rationale**: Face swap boundaries create artifacts\n",
    "- **Features**: Boundary inconsistency score\n",
    "\n",
    "**5. Codec Cues** (3 features)\n",
    "- **Method**: FFprobe analysis of video codec parameters\n",
    "- **Rationale**: Real and fake videos may use different codecs/parameters\n",
    "- **Features**: Codec type, bitrate, GOP size\n",
    "\n",
    "**Total**: ~15 features per video (aggregated across frames)\n",
    "\n",
    "**Location**: `lib/features/handcrafted.py`, `lib/features/pipeline.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Stage 2 features\n",
    "features_dir = data_dir / \"features_stage2\"\n",
    "features_metadata = features_dir / \"features_metadata.parquet\"\n",
    "\n",
    "if features_metadata.exists():\n",
    "    from lib.utils.paths import load_metadata_flexible\n",
    "    \n",
    "    features_df = load_metadata_flexible(str(features_metadata))\n",
    "    \n",
    "    if features_df is not None:\n",
    "        print(f\"‚úÖ Stage 2 Complete: {features_df.height} feature vectors\")\n",
    "        \n",
    "        # Load a sample feature vector\n",
    "        sample_row = features_df.head(1)\n",
    "        if 'feature_path' in sample_row.columns:\n",
    "            feature_path = sample_row['feature_path'][0]\n",
    "            try:\n",
    "                features = np.load(feature_path)\n",
    "                if isinstance(features, dict):\n",
    "                    print(f\"\\nüìä Sample Feature Vector ({len(features)} features):\")\n",
    "                    for key, value in list(features.items())[:10]:\n",
    "                        if isinstance(value, (int, float)):\n",
    "                            print(f\"  - {key}: {value:.6f}\")\n",
    "                        else:\n",
    "                            print(f\"  - {key}: {type(value).__name__}\")\n",
    "                else:\n",
    "                    print(f\"\\nüìä Feature vector shape: {features.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not load features: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Stage 2 not completed - features not found\")\n",
    "    print(\"\\nüí° To run Stage 2:\")\n",
    "    print(\"```python\")\n",
    "    print(\"from lib.features.pipeline import stage2_extract_features\")\n",
    "    print(\"\")\n",
    "    print(\"stage2_extract_features(\")\n",
    "    print(\"    project_root='.',\")\n",
    "    print(\"    augmented_metadata_path='data/augmented_videos/augmented_metadata.parquet',\")\n",
    "    print(\"    output_dir='data/features_stage2'\")\n",
    "    print(\")\")\n",
    "    print(\"```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stage 3: Video Scaling & Normalization\n",
    "\n",
    "### Why Scale Videos?\n",
    "\n",
    "**Problem**: Videos have diverse resolutions (e.g., 1920x1080, 640x480, 1280x720)\n",
    "- **Memory Constraints**: Full-resolution videos require 10-100GB GPU memory\n",
    "- **Model Input Requirements**: Most models expect fixed-size inputs (e.g., 256x256)\n",
    "- **Training Speed**: Smaller videos train 10-100x faster\n",
    "\n",
    "**Solution**: Scale all videos to target max dimension while preserving aspect ratio\n",
    "\n",
    "### Scaling Strategy\n",
    "\n",
    "**Target Resolution**: 256x256 (max dimension = 256px)\n",
    "- **Downscaling**: Large videos (e.g., 1920x1080 ‚Üí 256x144) reduce memory\n",
    "- **Upscaling**: Small videos (e.g., 320x240 ‚Üí 256x192) ensure minimum quality\n",
    "- **Aspect Ratio Preservation**: Letterboxing maintains original proportions\n",
    "\n",
    "### Scaling Methods\n",
    "\n",
    "**1. Letterbox Resize** (Default)\n",
    "- **Method**: Bilinear interpolation with letterboxing (black bars)\n",
    "- **Pros**: Fast, simple, preserves aspect ratio\n",
    "- **Cons**: Black bars waste pixels\n",
    "- **Use Case**: Production default, fastest option\n",
    "\n",
    "**2. Autoencoder Upscaling** (Optional)\n",
    "- **Method**: Pretrained HuggingFace VAE for high-quality upscaling\n",
    "- **Pros**: Better quality for upscaled videos\n",
    "- **Cons**: Slower, requires GPU\n",
    "- **Use Case**: Research, quality-critical applications\n",
    "\n",
    "### Normalization\n",
    "\n",
    "**Pixel Normalization**:\n",
    "- **Method**: Normalize to [0, 1] or ImageNet statistics\n",
    "- **Rationale**: Consistent input distribution improves training stability\n",
    "- **Implementation**: Applied during DataLoader transforms\n",
    "\n",
    "**Location**: `lib/scaling/pipeline.py`, `lib/scaling/methods.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for scaled videos\n",
    "scaled_dir = data_dir / \"scaled_videos\"\n",
    "scaled_metadata = scaled_dir / \"scaled_metadata.parquet\"\n",
    "\n",
    "if scaled_metadata.exists():\n",
    "    from lib.utils.paths import load_metadata_flexible\n",
    "    \n",
    "    scaled_df = load_metadata_flexible(str(scaled_metadata))\n",
    "    \n",
    "    if scaled_df is not None:\n",
    "        print(f\"‚úÖ Stage 3 Complete: {scaled_df.height} scaled videos\")\n",
    "        \n",
    "        # Show resolution statistics\n",
    "        if 'scaled_width' in scaled_df.columns and 'scaled_height' in scaled_df.columns:\n",
    "            scaled_pd = scaled_df.select(['scaled_width', 'scaled_height']).to_pandas()\n",
    "            \n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "            \n",
    "            # Width distribution\n",
    "            axes[0].hist(scaled_pd['scaled_width'], bins=30, color='#2196F3', edgecolor='black', alpha=0.7)\n",
    "            axes[0].set_title('Scaled Width Distribution', fontsize=14, fontweight='bold')\n",
    "            axes[0].set_xlabel('Width (pixels)')\n",
    "            axes[0].set_ylabel('Frequency')\n",
    "            axes[0].grid(axis='y', alpha=0.3)\n",
    "            axes[0].axvline(256, color='red', linestyle='--', label='Target: 256px')\n",
    "            axes[0].legend()\n",
    "            \n",
    "            # Height distribution\n",
    "            axes[1].hist(scaled_pd['scaled_height'], bins=30, color='#FF9800', edgecolor='black', alpha=0.7)\n",
    "            axes[1].set_title('Scaled Height Distribution', fontsize=14, fontweight='bold')\n",
    "            axes[1].set_xlabel('Height (pixels)')\n",
    "            axes[1].set_ylabel('Frequency')\n",
    "            axes[1].grid(axis='y', alpha=0.3)\n",
    "            axes[1].axvline(256, color='red', linestyle='--', label='Target: 256px')\n",
    "            axes[1].legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Stage 3 not completed - scaled videos not found\")\n",
    "    print(\"\\nüí° To run Stage 3:\")\n",
    "    print(\"```python\")\n",
    "    print(\"from lib.scaling.pipeline import stage3_scale_videos\")\n",
    "    print(\"\")\n",
    "    print(\"stage3_scale_videos(\")\n",
    "    print(\"    project_root='.',\")\n",
    "    print(\"    augmented_metadata_path='data/augmented_videos/augmented_metadata.parquet',\")\n",
    "    print(\"    output_dir='data/scaled_videos',\")\n",
    "    print(\"    target_size=256\")\n",
    "    print(\")\")\n",
    "    print(\"```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stage 4: Scaled Feature Extraction\n",
    "\n",
    "### Why Extract Features from Scaled Videos?\n",
    "\n",
    "**Complementary Information**:\n",
    "- **Scale-Invariant Features**: Some artifacts are visible at different scales\n",
    "- **Normalized Statistics**: Features extracted from normalized resolutions\n",
    "- **Model Input Alignment**: Features match the scale used by video models\n",
    "\n",
    "**Same Feature Types as Stage 2**:\n",
    "- Noise residual energy\n",
    "- DCT statistics\n",
    "- Blur/sharpness metrics\n",
    "- Block boundary inconsistency\n",
    "- Codec cues\n",
    "\n",
    "**Total Features**: ~15 features from scaled videos + ~15 from original = ~30 total handcrafted features\n",
    "\n",
    "**Location**: `lib/features/scaled.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Stage 4 features\n",
    "features4_dir = data_dir / \"features_stage4\"\n",
    "features4_metadata = features4_dir / \"features_metadata.parquet\"\n",
    "\n",
    "if features4_metadata.exists():\n",
    "    from lib.utils.paths import load_metadata_flexible\n",
    "    \n",
    "    features4_df = load_metadata_flexible(str(features4_metadata))\n",
    "    \n",
    "    if features4_df is not None:\n",
    "        print(f\"‚úÖ Stage 4 Complete: {features4_df.height} scaled feature vectors\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Stage 4 not completed - scaled features not found\")\n",
    "    print(\"\\nüí° To run Stage 4:\")\n",
    "    print(\"```python\")\n",
    "    print(\"from lib.features.scaled import stage4_extract_scaled_features\")\n",
    "    print(\"\")\n",
    "    print(\"stage4_extract_scaled_features(\")\n",
    "    print(\"    project_root='.',\")\n",
    "    print(\"    scaled_metadata_path='data/scaled_videos/scaled_metadata.parquet',\")\n",
    "    print(\"    output_dir='data/features_stage4'\")\n",
    "    print(\")\")\n",
    "    print(\"```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stage 5: Model Training Architecture\n",
    "\n",
    "### Model Portfolio (23 Models)\n",
    "\n",
    "**Baseline Models** (Feature-Based):\n",
    "- **5a**: Logistic Regression (handcrafted features)\n",
    "- **5b**: SVM (handcrafted features)\n",
    "\n",
    "**CNN Models** (Direct Video Processing):\n",
    "- **5c**: Naive CNN (3D convolutions, 1000 frames)\n",
    "- **5d**: Pretrained Inception (R3D-18 backbone + Inception head)\n",
    "- **5e**: Variable AR CNN (handles variable aspect ratios)\n",
    "\n",
    "**XGBoost + Pretrained Feature Extractors**:\n",
    "- **5f**: XGBoost + Pretrained Inception features\n",
    "- **5g**: XGBoost + I3D features\n",
    "- **5h**: XGBoost + R(2+1)D features\n",
    "- **5i**: XGBoost + ViT-GRU features\n",
    "- **5j**: XGBoost + ViT-Transformer features\n",
    "\n",
    "**Vision Transformer Models**:\n",
    "- **5k**: ViT-GRU (ViT per frame + GRU temporal)\n",
    "- **5l**: ViT-Transformer (ViT per frame + Transformer temporal)\n",
    "- **5m**: TimeSformer (divided space-time attention)\n",
    "- **5n**: ViViT (tubelet embedding)\n",
    "\n",
    "**3D CNN Models**:\n",
    "- **5o**: I3D (Inflated 3D ConvNet)\n",
    "- **5p**: R(2+1)D (Factorized 3D convolutions)\n",
    "- **5q**: X3D (Efficient video models)\n",
    "\n",
    "**SlowFast Variants**:\n",
    "- **5r**: SlowFast (dual pathway: slow + fast)\n",
    "- **5s**: SlowFast with Attention\n",
    "- **5t**: Multi-Scale SlowFast\n",
    "\n",
    "**Two-Stream Models**:\n",
    "- **5u**: Two-Stream (RGB + Optical Flow)\n",
    "\n",
    "### Training Strategy\n",
    "\n",
    "**5-Fold Stratified Cross-Validation**:\n",
    "- **Stratification**: Ensures balanced class distribution in each fold\n",
    "- **Group-Aware Splitting**: Prevents data leakage (same video ID in train/val)\n",
    "- **Reproducibility**: Fixed random seeds (42)\n",
    "\n",
    "**Hyperparameter Optimization**:\n",
    "- **Grid Search**: Exhaustive search over hyperparameter space\n",
    "- **Sample-Based**: Grid search on 10-20% sample for efficiency\n",
    "- **Best Params**: Applied to full dataset training\n",
    "- **Single Combination**: Models 5c-5u use single hyperparameter set (efficiency)\n",
    "\n",
    "**Regularization Techniques**:\n",
    "- **L2 Regularization**: Weight decay (1e-4 to 1e-3)\n",
    "- **Dropout**: 0.3-0.5 for fully connected layers\n",
    "- **Batch Normalization**: Stabilizes training, enables higher learning rates\n",
    "- **Gradient Clipping**: Prevents exploding gradients (max_norm=1.0)\n",
    "\n",
    "**Optimization**:\n",
    "- **Optimizer**: Adam with learning rate 1e-4 to 5e-4\n",
    "- **Scheduler**: Cosine annealing with warmup (2 epochs)\n",
    "- **Mixed Precision**: AMP for 2x speedup, 50% memory reduction\n",
    "- **Gradient Accumulation**: Effective batch size = batch_size √ó accumulation_steps\n",
    "\n",
    "**Activation Functions**:\n",
    "- **ReLU**: Standard for CNNs\n",
    "- **GELU**: For Transformers (smoother gradients)\n",
    "- **Sigmoid**: Final output (binary classification)\n",
    "\n",
    "**Location**: `lib/training/pipeline.py`, `lib/training/trainer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for trained models\n",
    "stage5_dir = data_dir / \"stage5\"\n",
    "\n",
    "if stage5_dir.exists():\n",
    "    model_dirs = [d for d in stage5_dir.iterdir() if d.is_dir()]\n",
    "    \n",
    "    print(f\"‚úÖ Found {len(model_dirs)} trained models:\")\n",
    "    \n",
    "    model_status = []\n",
    "    for model_dir in sorted(model_dirs):\n",
    "        model_name = model_dir.name\n",
    "        \n",
    "        # Check for model files\n",
    "        model_files = list(model_dir.glob(\"**/*.pt\")) + list(model_dir.glob(\"**/*.joblib\"))\n",
    "        metrics_files = list(model_dir.glob(\"**/metrics.json\"))\n",
    "        \n",
    "        status = {\n",
    "            'Model': model_name,\n",
    "            'Checkpoints': len(model_files),\n",
    "            'Metrics': 'Yes' if metrics_files else 'No'\n",
    "        }\n",
    "        model_status.append(status)\n",
    "    \n",
    "    status_df = pd.DataFrame(model_status)\n",
    "    display(status_df.style.set_properties(**{'text-align': 'left'}).set_table_styles([\n",
    "        {'selector': 'th', 'props': [('background-color', '#2196F3'), ('color', 'white'), ('font-weight', 'bold')]}\n",
    "    ]))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Stage 5 not completed - no trained models found\")\n",
    "    print(\"\\nüí° To train models:\")\n",
    "    print(\"```python\")\n",
    "    print(\"from lib.training.pipeline import stage5_train_models\")\n",
    "    print(\"\")\n",
    "    print(\"results = stage5_train_models(\")\n",
    "    print(\"    project_root='.',\")\n",
    "    print(\"    scaled_metadata_path='data/scaled_videos/scaled_metadata.parquet',\")\n",
    "    print(\"    features_stage2_path='data/features_stage2/features_metadata.parquet',\")\n",
    "    print(\"    features_stage4_path='data/features_stage4/features_metadata.parquet',\")\n",
    "    print(\"    model_types=['logistic_regression', 'svm', 'i3d', 'x3d', 'slowfast'],\")\n",
    "    print(\"    n_splits=5,\")\n",
    "    print(\"    num_frames=1000,\")\n",
    "    print(\"    output_dir='data/stage5',\")\n",
    "    print(\"    use_tracking=True,\")\n",
    "    print(\"    use_mlflow=True\")\n",
    "    print(\")\")\n",
    "    print(\"```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. MLOps: Experiment Tracking with MLflow\n",
    "\n",
    "### MLflow Integration\n",
    "\n",
    "**What MLflow Tracks**:\n",
    "- **Hyperparameters**: Learning rate, batch size, weight decay, etc.\n",
    "- **Metrics**: Train/val/test loss, accuracy, F1, precision, recall\n",
    "- **Artifacts**: Model checkpoints, configs, plots, logs\n",
    "- **Metadata**: Run ID, experiment name, timestamps, tags\n",
    "\n",
    "**Benefits**:\n",
    "- **Reproducibility**: Track exact hyperparameters for each run\n",
    "- **Comparison**: Compare models across experiments\n",
    "- **Model Registry**: Version and manage production models\n",
    "- **UI**: Web interface for browsing experiments\n",
    "\n",
    "**Location**: `lib/mlops/mlflow_tracker.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MLflow\n",
    "try:\n",
    "    import mlflow\n",
    "    \n",
    "    # Set tracking URI (default: local file store)\n",
    "    mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "    \n",
    "    # List experiments\n",
    "    experiments = mlflow.search_experiments()\n",
    "    \n",
    "    print(f\"‚úÖ MLflow Connected: {len(experiments)} experiments found\")\n",
    "    \n",
    "    if len(experiments) > 0:\n",
    "        print(\"\\nüìä Recent Experiments:\")\n",
    "        for exp in experiments[:5]:\n",
    "            print(f\"  - {exp.name} (ID: {exp.experiment_id})\")\n",
    "        \n",
    "        # Get runs from first experiment\n",
    "        exp = experiments[0]\n",
    "        runs = mlflow.search_runs(experiment_ids=[exp.experiment_id], max_results=10)\n",
    "        \n",
    "        if len(runs) > 0:\n",
    "            print(f\"\\nüìà Recent Runs (showing top 10):\")\n",
    "            display(runs[['run_id', 'status', 'start_time', 'metrics.test_f1', 'params.model_type']].head(10))\n",
    "    \n",
    "    print(\"\\nüí° To start MLflow UI:\")\n",
    "    print(\"```bash\")\n",
    "    print(\"mlflow ui --port 5000\")\n",
    "    print(\"# Open http://localhost:5000\")\n",
    "    print(\"```\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è MLflow not installed. Install with: pip install mlflow\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error connecting to MLflow: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analytics: DuckDB for Fast Queries\n",
    "\n",
    "### Why DuckDB?\n",
    "\n",
    "**Performance**:\n",
    "- **10-100x Faster**: Than pandas for analytical queries\n",
    "- **SQL Interface**: Familiar SQL syntax for complex queries\n",
    "- **Columnar Processing**: Optimized for analytical workloads\n",
    "- **Zero Configuration**: In-process database, no server setup\n",
    "\n",
    "**Use Cases**:\n",
    "- Query training results across models\n",
    "- Aggregate metrics by fold, model type, hyperparameters\n",
    "- Join metadata with results\n",
    "- Fast filtering and grouping\n",
    "\n",
    "**Location**: `lib/utils/duckdb_analytics.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DuckDB Analytics Example\n",
    "try:\n",
    "    from lib.utils.duckdb_analytics import DuckDBAnalytics\n",
    "    \n",
    "    analytics = DuckDBAnalytics()\n",
    "    \n",
    "    # Register metadata tables\n",
    "    if scaled_metadata.exists():\n",
    "        analytics.register_parquet('videos', str(scaled_metadata))\n",
    "        print(\"‚úÖ Registered 'videos' table\")\n",
    "    \n",
    "    if features_metadata.exists():\n",
    "        analytics.register_parquet('features', str(features_metadata))\n",
    "        print(\"‚úÖ Registered 'features' table\")\n",
    "    \n",
    "    # Example query: Class distribution\n",
    "    if scaled_metadata.exists():\n",
    "        result = analytics.query(\"\"\"\n",
    "            SELECT \n",
    "                label,\n",
    "                COUNT(*) as count,\n",
    "                ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) as percentage\n",
    "            FROM videos\n",
    "            GROUP BY label\n",
    "            ORDER BY count DESC\n",
    "        \"\"\")\n",
    "        \n",
    "        if result is not None and len(result) > 0:\n",
    "            print(\"\\nüìä Class Distribution (DuckDB Query):\")\n",
    "            display(result)\n",
    "    \n",
    "    print(\"\\nüí° Example DuckDB Queries:\")\n",
    "    print(\"```python\")\n",
    "    print(\"# Query training results\")\n",
    "    print(\"analytics.register_parquet('results', 'data/stage5/*/metrics.json')\")\n",
    "    print(\"result = analytics.query('\"\"\")\n",
    "    print(\"    SELECT model_type, AVG(test_f1) as avg_f1, STDDEV(test_f1) as std_f1\")\n",
    "    print(\"    FROM results\")\n",
    "    print(\"    GROUP BY model_type\")\n",
    "    print(\"    ORDER BY avg_f1 DESC\")\n",
    "    print(\"\"\"\")\")\n",
    "    print(\"```\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è DuckDB not installed. Install with: pip install duckdb\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error using DuckDB: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Orchestration: Apache Airflow DAGs\n",
    "\n",
    "### Pipeline Orchestration\n",
    "\n",
    "**Apache Airflow DAG**: `airflow/dags/fvc_pipeline_dag.py`\n",
    "\n",
    "**Pipeline Stages as Tasks**:\n",
    "1. **Stage 1 Task**: Video augmentation (parallelizable)\n",
    "2. **Stage 2 Task**: Feature extraction (depends on Stage 1)\n",
    "3. **Stage 3 Task**: Video scaling (depends on Stage 1, parallel with Stage 2)\n",
    "4. **Stage 4 Task**: Scaled feature extraction (depends on Stage 3)\n",
    "5. **Stage 5 Task**: Model training (depends on Stages 2, 3, 4)\n",
    "\n",
    "**Dependency Graph**:\n",
    "```\n",
    "Stage 1\n",
    "  ‚îú‚îÄ> Stage 2\n",
    "  ‚îî‚îÄ> Stage 3 ‚îÄ> Stage 4\n",
    "  ‚îî‚îÄ> Stage 5 (depends on 2, 3, 4)\n",
    "```\n",
    "\n",
    "**Benefits**:\n",
    "- **Dependency Management**: Automatic task ordering\n",
    "- **Retry Logic**: Automatic retries on failure (1 retry, 5min delay)\n",
    "- **Monitoring**: Web UI for pipeline status\n",
    "- **Scheduling**: Cron-based scheduling support\n",
    "- **Parallelization**: Parallel stage execution where possible\n",
    "- **Checkpointing**: Resume from failures\n",
    "\n",
    "**Location**: `airflow/dags/fvc_pipeline_dag.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Airflow DAG visualization\n",
    "airflow_dag_path = project_root / \"airflow\" / \"dags\" / \"fvc_pipeline_dag.py\"\n",
    "\n",
    "if airflow_dag_path.exists():\n",
    "    print(\"‚úÖ Airflow DAG found:\")\n",
    "    print(f\"   Location: {airflow_dag_path}\")\n",
    "    \n",
    "    # Read and display DAG structure\n",
    "    with open(airflow_dag_path, 'r') as f:\n",
    "        dag_code = f.read()\n",
    "    \n",
    "    print(\"\\nüìã DAG Tasks:\")\n",
    "    print(\"   1. stage1_augmentation\")\n",
    "    print(\"   2. stage2_features (depends on stage1)\")\n",
    "    print(\"   3. stage3_scaling (depends on stage1)\")\n",
    "    print(\"   4. stage4_scaled_features (depends on stage3)\")\n",
    "    print(\"   5. stage5_training (depends on stage2, stage3, stage4)\")\n",
    "    \n",
    "    print(\"\\nüí° To use Airflow:\")\n",
    "    print(\"```bash\")\n",
    "    print(\"# Start Airflow webserver\")\n",
    "    print(\"airflow webserver --port 8080\")\n",
    "    print(\"\")\n",
    "    print(\"# Start Airflow scheduler\")\n",
    "    print(\"airflow scheduler\")\n",
    "    print(\"\")\n",
    "    print(\"# Trigger DAG\")\n",
    "    print(\"airflow dags trigger fvc_pipeline\")\n",
    "    print(\"```\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Airflow DAG not found\")\n",
    "    print(f\"   Expected: {airflow_dag_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Evaluation & Results\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "**Classification Metrics** (per fold, then averaged):\n",
    "- **Accuracy**: Overall correctness\n",
    "- **F1 Score**: Harmonic mean of precision and recall (primary metric)\n",
    "- **Precision**: True positives / (True positives + False positives)\n",
    "- **Recall**: True positives / (True positives + False negatives)\n",
    "- **AUC-ROC**: Area under ROC curve\n",
    "- **Confusion Matrix**: Per-class error analysis\n",
    "\n",
    "**Cross-Validation Statistics**:\n",
    "- **Mean**: Average across 5 folds\n",
    "- **Std**: Standard deviation (measures consistency)\n",
    "- **Min/Max**: Best and worst fold performance\n",
    "\n",
    "### Results Visualization\n",
    "\n",
    "**Location**: `src/dashboard_results.py` (Streamlit dashboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize results\n",
    "if stage5_dir.exists():\n",
    "    import json\n",
    "    \n",
    "    results_summary = []\n",
    "    \n",
    "    for model_dir in sorted(stage5_dir.iterdir()):\n",
    "        if not model_dir.is_dir():\n",
    "            continue\n",
    "        \n",
    "        model_name = model_dir.name\n",
    "        \n",
    "        # Find metrics file\n",
    "        metrics_files = list(model_dir.glob(\"**/metrics.json\"))\n",
    "        \n",
    "        if metrics_files:\n",
    "            metrics_file = metrics_files[0]\n",
    "            \n",
    "            try:\n",
    "                with open(metrics_file, 'r') as f:\n",
    "                    metrics = json.load(f)\n",
    "                \n",
    "                # Extract summary metrics\n",
    "                if 'mean_test_f1' in metrics:\n",
    "                    results_summary.append({\n",
    "                        'Model': model_name,\n",
    "                        'Mean F1': metrics.get('mean_test_f1', 0),\n",
    "                        'Std F1': metrics.get('std_test_f1', 0),\n",
    "                        'Mean Accuracy': metrics.get('mean_test_acc', 0),\n",
    "                        'Mean Precision': metrics.get('mean_test_precision', 0),\n",
    "                        'Mean Recall': metrics.get('mean_test_recall', 0)\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not load metrics for {model_name}: {e}\")\n",
    "    \n",
    "    if results_summary:\n",
    "        results_df = pd.DataFrame(results_summary)\n",
    "        results_df = results_df.sort_values('Mean F1', ascending=False)\n",
    "        \n",
    "        print(\"üìä Model Performance Summary:\")\n",
    "        display(results_df.style.format({\n",
    "            'Mean F1': '{:.4f}',\n",
    "            'Std F1': '{:.4f}',\n",
    "            'Mean Accuracy': '{:.4f}',\n",
    "            'Mean Precision': '{:.4f}',\n",
    "            'Mean Recall': '{:.4f}'\n",
    "        }).background_gradient(subset=['Mean F1'], cmap='RdYlGn').set_table_styles([\n",
    "            {'selector': 'th', 'props': [('background-color', '#2196F3'), ('color', 'white'), ('font-weight', 'bold')]}\n",
    "        ]))\n",
    "        \n",
    "        # Visualization\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # F1 Score comparison\n",
    "        axes[0].barh(results_df['Model'], results_df['Mean F1'], \n",
    "                    xerr=results_df['Std F1'], capsize=5, color='#4CAF50')\n",
    "        axes[0].set_xlabel('F1 Score', fontsize=12)\n",
    "        axes[0].set_title('Model Performance (F1 Score)', fontsize=14, fontweight='bold')\n",
    "        axes[0].grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Accuracy comparison\n",
    "        axes[1].barh(results_df['Model'], results_df['Mean Accuracy'], color='#2196F3')\n",
    "        axes[1].set_xlabel('Accuracy', fontsize=12)\n",
    "        axes[1].set_title('Model Performance (Accuracy)', fontsize=14, fontweight='bold')\n",
    "        axes[1].grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No metrics found in trained models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Production Deployment Considerations\n",
    "\n",
    "### Model Serving\n",
    "\n",
    "**Options**:\n",
    "- **MLflow Model Serving**: Built-in serving for PyTorch models\n",
    "- **TorchServe**: PyTorch's production serving framework\n",
    "- **FastAPI**: Custom REST API with model loading\n",
    "- **ONNX Export**: Cross-platform deployment\n",
    "\n",
    "### Monitoring\n",
    "\n",
    "**MLflow Model Registry**:\n",
    "- Version control for models\n",
    "- Staging ‚Üí Production promotion\n",
    "- A/B testing support\n",
    "\n",
    "**Custom Monitoring**:\n",
    "- Prediction logging\n",
    "- Performance metrics tracking\n",
    "- Drift detection\n",
    "\n",
    "### Scalability\n",
    "\n",
    "**Batch Processing**:\n",
    "- Process videos in batches\n",
    "- Use GPU clusters for inference\n",
    "- Parallelize across multiple GPUs\n",
    "\n",
    "**Real-Time Processing**:\n",
    "- Frame-by-frame processing\n",
    "- Streaming inference\n",
    "- Low-latency requirements\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This pipeline demonstrates a **production-grade ML system** with:\n",
    "- ‚úÖ **Comprehensive Data Processing**: 5-stage pipeline from raw videos to trained models\n",
    "- ‚úÖ **23 Diverse Models**: From baselines to state-of-the-art architectures\n",
    "- ‚úÖ **MLOps Infrastructure**: MLflow, Airflow, DuckDB integration\n",
    "- ‚úÖ **Best Practices**: 5-fold CV, hyperparameter optimization, experiment tracking\n",
    "- ‚úÖ **Production-Ready**: Error handling, checkpointing, reproducibility\n",
    "\n",
    "**Next Steps**:\n",
    "1. Review individual model notebooks (5a-5u) for detailed architecture\n",
    "2. Explore MLflow UI for experiment comparison\n",
    "3. Use DuckDB for custom analytics queries\n",
    "4. Deploy best model to production using MLflow Model Registry"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
