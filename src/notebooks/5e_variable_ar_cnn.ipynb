{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5e: Variable Aspect Ratio CNN\n",
    "\n",
    "This notebook demonstrates the Variable Aspect Ratio CNN for deepfake video detection.\n",
    "\n",
    "## Model Overview\n",
    "\n",
    "Custom 3D CNN with Inception blocks that supports variable aspect ratios via adaptive pooling.\n",
    "\n",
    "## Training Instructions\n",
    "\n",
    "To train this model, run:\n",
    "\n",
    "```bash\n",
    "sbatch src/scripts/slurm_stage5e.sh\n",
    "```\n",
    "\n",
    "Or use Python:\n",
    "\n",
    "```python\n",
    "from lib.training.pipeline import stage5_train_models\n",
    "\n",
    "results = stage5_train_models(\n",
    "    project_root=\".\",\n",
    "    scaled_metadata_path=\"data/stage3/scaled_metadata.parquet\",\n",
    "    features_stage2_path=\"data/stage2/features_metadata.parquet\",\n",
    "    features_stage4_path=None,\n",
    "    model_types=[\"variable_ar_cnn\"],\n",
    "    n_splits=5,\n",
    "    num_frames=1000,\n",
    "    output_dir=\"data/stage5\",\n",
    "    use_tracking=True,\n",
    "    use_mlflow=True\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Video, display, HTML\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from lib.training.model_factory import create_model\n",
    "from lib.mlops.config import RunConfig\n",
    "from lib.utils.paths import load_metadata_flexible\n",
    "from lib.training.metrics_utils import compute_classification_metrics\n",
    "\n",
    "# Configuration\n",
    "MODEL_TYPE = \"variable_ar_cnn\"\n",
    "MODEL_DIR = project_root / \"data\" / \"stage5\" / MODEL_TYPE\n",
    "SCALED_METADATA_PATH = project_root / \"data\" / \"stage3\" / \"scaled_metadata.parquet\"\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Model directory: {MODEL_DIR}\")\n",
    "print(f\"Model directory exists: {MODEL_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Saved Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_saved_models(model_dir: Path):\n",
    "    \"\"\"Check for saved model files.\"\"\"\n",
    "    if not model_dir.exists():\n",
    "        print(f\"❌ Model directory does not exist: {model_dir}\")\n",
    "        return False, []\n",
    "    \n",
    "    fold_dirs = sorted([d for d in model_dir.iterdir() if d.is_dir() and d.name.startswith(\"fold_\")])\n",
    "    \n",
    "    if not fold_dirs:\n",
    "        print(f\"❌ No fold directories found in {model_dir}\")\n",
    "        return False, []\n",
    "    \n",
    "    print(f\"✓ Found {len(fold_dirs)} fold(s)\")\n",
    "    \n",
    "    models_found = []\n",
    "    for fold_dir in fold_dirs:\n",
    "        model_file = fold_dir / \"model.pt\"\n",
    "        if model_file.exists():\n",
    "            models_found.append((fold_dir.name, model_file))\n",
    "            print(f\"  ✓ {fold_dir.name}: Found model.pt\")\n",
    "        else:\n",
    "            print(f\"  ❌ {fold_dir.name}: No model.pt found\")\n",
    "    \n",
    "    return len(models_found) > 0, models_found\n",
    "\n",
    "models_available, model_files = check_saved_models(MODEL_DIR)\n",
    "\n",
    "if not models_available:\n",
    "    print(\"\\n⚠️  No trained models found. Please train the model first using the instructions above.\")\n",
    "    print(f\"Expected location: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_available:\n",
    "    fold_name, model_path = model_files[0]\n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "    \n    # Create model instance
    config = RunConfig(
        run_id="demo",
        experiment_name="demo",
        model_type=MODEL_TYPE,
        num_frames=500
    )
    model = create_model(MODEL_TYPE, config)
    
    # Load weights
    checkpoint = torch.load(model_path, map_location='cpu')
    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint)
    model.eval()
    print(f"✓ Model architecture: {type(model).__name__}")
    "    \n",
    "    print(f\"✓ Model loaded successfully from {fold_name}\")\n",
    "    print(f\"Model type: {type(model)}\")\n",
    "    \n",
    "    # Load metadata\n",
    "    scaled_df = load_metadata_flexible(str(SCALED_METADATA_PATH))\n",
    "    \n",
    "    if scaled_df is not None:\n",
    "        print(f\"\\n✓ Loaded {scaled_df.height} videos from scaled metadata\")\n",
    "        sample_videos = scaled_df.head(5).to_pandas()\n",
    "        print(f\"\\nSample videos for demonstration:\")\n",
    "        print(sample_videos[[\"video_path\", \"label\"]].to_string())\n",
    "    else:\n",
    "        print(\"⚠️  Could not load metadata files\")\n",
    "else:\n",
    "    print(\"⚠️  Skipping model loading - no trained models found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Sample Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_available and 'sample_videos' in locals():\n",
    "    fig, axes = plt.subplots(1, min(3, len(sample_videos)), figsize=(15, 5))\n",
    "    if len(sample_videos) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (ax, row) in enumerate(zip(axes, sample_videos.iterrows())):\n",
    "        video_path = project_root / row[1][\"video_path\"]\n",
    "        label = row[1][\"label\"]\n",
    "        \n",
    "        try:\n",
    "            import cv2\n",
    "            cap = cv2.VideoCapture(str(video_path))\n",
    "            if cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if ret:\n",
    "                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    ax.imshow(frame_rgb)\n",
    "                    ax.set_title(f\"{Path(video_path).name}\\nLabel: {label}\", fontsize=10)\n",
    "                cap.release()\n",
    "        except Exception as e:\n",
    "            ax.text(0.5, 0.5, f\"Video: {Path(video_path).name}\\nLabel: {label}\", \n",
    "                    ha='center', va='center', fontsize=12, transform=ax.transAxes)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nNote: To play videos in the notebook, use:\")\n",
    "    print(\"display(Video('path/to/video.mp4', embed=True, width=640, height=480))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_available:\n",
    "    fold_dir = model_files[0][0]\n",
    "    metrics_file = MODEL_DIR / fold_dir / \"metrics.json\"\n",
    "    \n",
    "    if metrics_file.exists():\n",
    "        with open(metrics_file, 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "        \n",
    "        print(\"Model Performance Metrics:\")\n",
    "        print(\"=\" * 50)\n",
    "        for key, value in metrics.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                print(f\"{key}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"{key}: {value}\")\n",
    "        \n",
    "        if 'accuracy' in metrics or 'f1_score' in metrics:\n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "            metric_names = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "            metric_values = [metrics.get(m, 0) for m in metric_names]\n",
    "            \n",
    "            bars = ax.bar(metric_names, metric_values, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "            ax.set_ylabel('Score')\n",
    "            ax.set_title('Variable Aspect Ratio CNN Model Performance')\n",
    "            ax.set_ylim(0, 1)\n",
    "            \n",
    "            for bar, val in zip(bars, metric_values):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{val:.3f}', ha='center', va='bottom')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"⚠️  Metrics file not found.\")\n",
    "        print(f\"Expected: {metrics_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture Summary\n",
    "\n",
    "**Variable Aspect Ratio CNN**\n",
    "\n",
    "Custom 3D CNN with Inception-style blocks. Uses 3D convolutions with adaptive average pooling to handle videos of different aspect ratios and lengths.\n",
    "\n",
    "**Implementation:**\n",
    "- Model code: `lib/training/variable_ar_cnn.py`\n",
    "\n",
    "**Advantages:**\n",
    "- See model-specific advantages in the implementation\n",
    "\n",
    "**Use Cases:**\n",
    "- Deepfake video detection\n",
    "- Video authenticity verification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}