{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5alpha: sklearn LogisticRegression\n",
    "\n",
    "This notebook demonstrates the sklearn LogisticRegression model for deepfake video detection.\n",
    "\n",
    "## Model Overview\n",
    "\n",
    "sklearn LogisticRegression with L1/L2/ElasticNet regularization. Uses handcrafted features from Stage 2/4. This is a standalone implementation separate from the pipeline's logistic regression.\n",
    "\n",
    "## Training Instructions\n",
    "\n",
    "To train this model, run:\n",
    "\n",
    "```bash\n",
    "sbatch src/scripts/slurm_stage5alpha.sh\n",
    "```\n",
    "\n",
    "Or use Python:\n",
    "\n",
    "```python\n",
    "from src.scripts.train_sklearn_logreg import train_sklearn_logreg\n",
    "\n",
    "results = train_sklearn_logreg(\n",
    "    project_root=\".\",\n",
    "    scaled_metadata_path=\"data/stage3/scaled_metadata.parquet\",\n",
    "    features_stage2_path=\"data/stage2/features_metadata.parquet\",\n",
    "    features_stage4_path=None,\n",
    "    output_dir=\"data/stage5/sklearn_logreg\",\n",
    "    n_splits=5,\n",
    "    delete_existing=False\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Video, display, HTML\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from lib.utils.paths import load_metadata_flexible\n",
    "from lib.training.metrics_utils import compute_classification_metrics\n",
    "\n",
    "# Configuration\n",
    "MODEL_TYPE = \"sklearn_logreg\"\n",
    "MODEL_DIR = project_root / \"data\" / \"stage5\" / \"sklearn_logreg\"\n",
    "SCALED_METADATA_PATH = project_root / \"data\" / \"stage3\" / \"scaled_metadata.parquet\"\n",
    "FEATURES_STAGE2_PATH = project_root / \"data\" / \"stage2\" / \"features_metadata.parquet\"\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Model directory: {MODEL_DIR}\")\n",
    "print(f\"Model directory exists: {MODEL_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Saved Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_saved_models(model_dir: Path):\n",
    "    \"\"\"Check for saved sklearn model files.\"\"\"\n",
    "    if not model_dir.exists():\n",
    "        print(f\"❌ Model directory does not exist: {model_dir}\")\n",
    "        return False, None\n",
    "    \n",
    "    # sklearn_logreg saves model directly in output_dir, not in fold subdirectories\n",
    "    model_file = model_dir / \"model.joblib\"\n",
    "    scaler_file = model_dir / \"scaler.joblib\"\n",
    "    metrics_file = model_dir / \"metrics.json\"\n",
    "    \n",
    "    if model_file.exists():\n",
    "        print(f\"✓ Found model.joblib\")\n",
    "        if scaler_file.exists():\n",
    "            print(f\"✓ Found scaler.joblib\")\n",
    "        if metrics_file.exists():\n",
    "            print(f\"✓ Found metrics.json\")\n",
    "        return True, model_file\n",
    "    else:\n",
    "        print(f\"❌ No model.joblib found in {model_dir}\")\n",
    "        return False, None\n",
    "\n",
    "models_available, model_file = check_saved_models(MODEL_DIR)\n",
    "\n",
    "if not models_available:\n",
    "    print(\"\\n⚠️  No trained models found. Please train the model first using the instructions above.\")\n",
    "    print(f\"Expected location: {MODEL_DIR / 'model.joblib'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_available:\n",
    "    print(f\"Loading model from: {model_file}\")\n",
    "    \n",
    "    model = joblib.load(model_file)\n",
    "    print(f\"✓ Model loaded successfully\")\n",
    "    print(f\"Model type: {type(model)}\")\n",
    "    \n",
    "    # Load scaler if available\n",
    "    scaler = None\n",
    "    scaler_file = MODEL_DIR / \"scaler.joblib\"\n",
    "    if scaler_file.exists():\n",
    "        scaler = joblib.load(scaler_file)\n",
    "        print(f\"✓ Scaler loaded\")\n",
    "    \n",
    "    # Load metadata\n",
    "    scaled_df = load_metadata_flexible(str(SCALED_METADATA_PATH))\n",
    "    features_df = load_metadata_flexible(str(FEATURES_STAGE2_PATH))\n",
    "    \n",
    "    if scaled_df is not None and features_df is not None:\n",
    "        print(f\"\\n✓ Loaded {scaled_df.height} videos from scaled metadata\")\n",
    "        print(f\"✓ Loaded {features_df.height} feature rows from Stage 2\")\n",
    "        \n",
    "        # Get sample videos\n",
    "        sample_videos = scaled_df.head(5).to_pandas()\n",
    "        print(f\"\\nSample videos for demonstration:\")\n",
    "        print(sample_videos[[\"video_path\", \"label\"]].to_string())\n",
    "    else:\n",
    "        print(\"⚠️  Could not load metadata files\")\n",
    "else:\n",
    "    print(\"⚠️  Skipping model loading - no trained models found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Sample Videos and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_available and 'model' in locals() and 'sample_videos' in locals():\n",
    "    # Create a simple visualization\n",
    "    fig, axes = plt.subplots(1, min(3, len(sample_videos)), figsize=(15, 5))\n",
    "    if len(sample_videos) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (ax, row) in enumerate(zip(axes, sample_videos.iterrows())):\n",
    "        video_path = project_root / row[1][\"video_path\"]\n",
    "        label = row[1][\"label\"]\n",
    "        \n",
    "        # Try to load and display video thumbnail\n",
    "        try:\n",
    "            import cv2\n",
    "            cap = cv2.VideoCapture(str(video_path))\n",
    "            if cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if ret:\n",
    "                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    ax.imshow(frame_rgb)\n",
    "                    ax.set_title(f\"{Path(video_path).name}\\nLabel: {label}\", fontsize=10)\n",
    "                cap.release()\n",
    "        except Exception as e:\n",
    "            ax.text(0.5, 0.5, f\"Video: {Path(video_path).name}\\nLabel: {label}\", \n",
    "                    ha='center', va='center', fontsize=12, transform=ax.transAxes)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nNote: To play videos in the notebook, use:\")\n",
    "    print(\"display(Video('path/to/video.mp4', embed=True, width=640, height=480))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_available:\n",
    "    # Try to load metrics from model directory\n",
    "    metrics_file = MODEL_DIR / \"metrics.json\"\n",
    "    \n",
    "    if metrics_file.exists():\n",
    "        with open(metrics_file, 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "        \n",
    "        print(\"Model Performance Metrics:\")\n",
    "        print(\"=\" * 50)\n",
    "        for key, value in metrics.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                print(f\"{key}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"{key}: {value}\")\n",
    "        \n",
    "        # Create visualization if metrics available\n",
    "        if 'test_f1' in metrics or 'test_accuracy' in metrics:\n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "            metric_names = ['test_accuracy', 'test_precision', 'test_recall', 'test_f1']\n",
    "            metric_values = [metrics.get(m, 0) for m in metric_names]\n",
    "            \n",
    "            bars = ax.bar(metric_names, metric_values, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "            ax.set_ylabel('Score')\n",
    "            ax.set_title('sklearn LogisticRegression Model Performance')\n",
    "            ax.set_ylim(0, 1)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for bar, val in zip(bars, metric_values):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{val:.3f}', ha='center', va='bottom')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"⚠️  Metrics file not found. Model may not have been fully trained.\")\n",
    "        print(f\"Expected: {metrics_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture Summary\n",
    "\n",
    "**sklearn LogisticRegression** is a linear classifier that:\n",
    "- Uses handcrafted features from Stage 2 (noise residual, DCT statistics, blur/sharpness, codec cues)\n",
    "- Supports L1, L2, and ElasticNet regularization\n",
    "- Uses StandardScaler for feature normalization\n",
    "- Outputs probability scores for binary classification (real vs fake)\n",
    "- Trained with grid search on hyperparameters (C, penalty, solver)\n",
    "\n",
    "**Advantages:**\n",
    "- Simple and interpretable\n",
    "- Fast training and inference\n",
    "- Multiple regularization options (L1/L2/ElasticNet)\n",
    "- Good baseline for comparison\n",
    "\n",
    "**Limitations:**\n",
    "- Linear decision boundary (may not capture complex patterns)\n",
    "- Relies on quality of handcrafted features\n",
    "- No temporal modeling\n",
    "\n",
    "**Difference from 5a:** This is a standalone sklearn implementation with more regularization options, while 5a uses the pipeline's LogisticRegressionBaseline class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
